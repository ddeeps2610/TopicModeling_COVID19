{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling for COVID19\n",
    "COVID 19 has been the biggest pandemic people have seen in the recent times. It almost feels like one of those apocalyptic movies in real life. People hiding in their homes trying to save themselves from the infection. Some brave souls trying to find a better destination to survive this pandemic. With so much happening around the world, I have one question. **What are people around the world thinking about COVID 19?**\n",
    "\n",
    "In this notebook, we will try to answer the above question using Topic Modeling. Let's explore the various topics people are talking about Corona Virus Disease 2019(COVID-19) in Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/deepakawari/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Twitter data collection library\n",
    "import tweepy as tw\n",
    "\n",
    "# Data processing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Loading Gensim and nltk libraries\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "#Other stop_words: gensim.parsing.preprocessing.STOPWORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure notebook display to show data from pandas dataframe more clearly.\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.width',100)\n",
    "pd.set_option('display.max_colwidth',800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gather the textual data for Topic Modeling\n",
    "To begin with the topic modeling we need the textual data. The textual data for what people are talking about COVID 19 can be pulled from many places such as social media, news articles, web scraping etc. In this notebook, we'll download the data from Twitter. Tweepy is an amazing library to pull data from twitter using your Twitter Developer Account. \n",
    "\n",
    "## Extracting tweets from Twitter API:\n",
    "To load the data from Twitter using Tweepy API, you'll have to create Developer account with Twitter. Then download the credentials to authenticate using Tweepy API. **Please do not share these credentials with anybody else.**\n",
    "* Here is the link to [apply for twitter developer access](https://developer.twitter.com/en/apply-for-access)\n",
    "* You can follow the below code to use Tweepy API to authenticate and load the data. Here is the [Tweepy Documentation for reference](http://docs.tweepy.org/en/latest/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LoadFromTwitter - \n",
    "    If true, pull the latest set of tweets from Twitter using the Tweepy library.\n",
    "    If false, load the data from the datafile '../data/tweets.csv' if it exists, \n",
    "    otherwise load the tweets from Twitter using the Tweepy library.\n",
    "    Set the LoadFromTwitter to True if you want to override loading the tweets afresh from twitter.\n",
    "'''\n",
    "LoadFromTwitter = False\n",
    "\n",
    "fileName = '../data/tweets.csv'\n",
    "tweetsDF = None\n",
    "\n",
    "# Load the data\n",
    "if os.path.exists(fileName) and not LoadFromTwitter:\n",
    "    tweetsDF = pd.read_csv(fileName)\n",
    "else:\n",
    "    from TwitterDevSecrets import getTwitterDevCreds\n",
    "    consumer_key, consumer_secret, access_token, access_secret = getTwitterDevCreds()\n",
    "\n",
    "    auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "    # Set the wait_on_rate_limit and wait_on_rate_limit_notify to True\n",
    "    # wait_on_rate_limit – \n",
    "    #    Whether or not to automatically wait for rate limits to replenish\n",
    "    # wait_on_rate_limit_notify – \n",
    "    #    Whether or not to print a notification when Tweepy is waiting \n",
    "    #    for rate limits to replenish\n",
    "    api = tw.API(\n",
    "        auth, \n",
    "        wait_on_rate_limit=True, \n",
    "        wait_on_rate_limit_notify=True)\n",
    "\n",
    "    # Define the search term and the date_since date as variables\n",
    "    search_words = \"#covid OR #covid19 OR #COVID OR #COVID19 OR #ncov OR #corona OR #coronaviru\"\n",
    "    date_since = \"2020-05-16\"\n",
    "    \n",
    "    # Read the tweets\n",
    "    tweets = tw.Cursor(api.search, \n",
    "                   q=search_words,\n",
    "                   lang=\"en\",\n",
    "                   since=date_since)\n",
    "\n",
    "    # extract the data in pandas dataframe\n",
    "    # Other parameters: tweet.user.screen_name, retweet_counts, favorite_counts\n",
    "    tweetsDF = pd.DataFrame()\n",
    "    for tweet in tweets.items(10000):\n",
    "        id = tweet.id\n",
    "        text = tweet.text\n",
    "        loc = tweet.user.location\n",
    "        tweetsDF = tweetsDF.append({'Id':id, 'Text':text, 'Location':loc},ignore_index=True)\n",
    "    \n",
    "    tweetsDF['index'] = tweetsDF.index\n",
    "    \n",
    "    # Save the new set of tweets in the file.\n",
    "    tweetsDF.to_csv(fileName,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how doest he textual data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @Carol_D_Johnson: Thank you nurses for helping to keep us healthy  ❤ #COVID19 \\n#StayHomeSaveLives \\n#coronavirus https://t.co/HGv0HfuTgt'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF.Text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing\n",
    "As you can see from the above text, a tweet contains a lot of textual data which probably doesn't contain any useful informaiton for Topic Modeling. So, these tweets needs to be processed to extract only useful textual data for further analysis. We will perform the following data processing steps:\n",
    "\n",
    "* Tweet Preprocessing:\n",
    "> * Remove the leading **RT** - RT indicates that the user is re-posting someone else's tweet. We can remove this token.\n",
    "> * Remove the references to other accounts. The other accounts are usually referenced with '@' symbol.\n",
    "> * Remove urls mentioned in the tweets.\n",
    "\n",
    "* Generic text preprocessing:\n",
    "> * **Tokenization**: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "> * Remove words that have fewer than 3 characters.\n",
    "> * Remove all **stopwords**. [Stop words](https://en.wikipedia.org/wiki/Stop_words) usually do not contain any usual information. As such these words are generally removed from the text in the preprocessing stage. \n",
    "> * **Lemmatize** the words: words in third person are changed to first person and verbs in past and future tenses are changed into present.  \n",
    "> Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words. \n",
    ">> **WordnetLemmatizer**: uses lookup table from nltk wordnet corpus to lookup the lemma to return a valid language lemma.\n",
    "> * **Stem** the Words: words are reduced to their root form.  \n",
    "> Stemming is the process of reducing inflection in words to their root forms such as \n",
    "mapping a group of words to the same stem even if the stem itself is not a valid word \n",
    "in the Language.\n",
    ">> **PorterStemmer**: is known for simplicity and ease. The algorithm does not follow linguistics rather a set of rules that are applied in phases (step by step) to generate stems. This is the reason why PorterStemmer does not often generate stems that are actual English words.  \n",
    ">> **SnowballStemmer**: One can generate their own set of rules for any language. Python nltk introduced SnowballStemmers that are used to create non-English Stemmers!  \n",
    ">> **LancasterStemmer**: is simple, but heavy stemming due to iterations and over-stemming may occur. Over-stemming causes the stems to be not linguistic, or they may have no meaning.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data preprocessing for all tweets.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def tweet_cleanup(text):\n",
    "    # Remove the leading RT from the tweet\n",
    "    text = text.replace('RT','')\n",
    "    # Remove the references to the account names starting with '@'\n",
    "    text = re.sub(r'(@[a-zA-Z]*)','',text)\n",
    "    # Remove the urls in the tweet.\n",
    "    text = re.sub(r'((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)','',text)\n",
    "    \n",
    "    return text\n",
    "  \n",
    "# Clean up the tweets and then Tokenize and lemmatize\n",
    "def preprocess(text, stop_words=stop_words):\n",
    "    result=[]\n",
    "    text = tweet_cleanup(text)\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in stop_words and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: \n",
      "['RT', '@ALPublicHealth:', 'State', 'Health', 'Officer', 'Dr.', 'Scott', 'Harris', 'has', 'issued', 'a', 'stay', 'at', 'home', 'order', 'and', 'strict', 'quarantine', 'requirements.', 'Read', 'our', 'full…']\n",
      "\n",
      "\n",
      "Preprocessed tweet: \n",
      "['state', 'health', 'offic', 'scott', 'harri', 'issu', 'stay', 'home', 'order', 'strict', 'quarantin', 'requir', 'read', 'full']\n"
     ]
    }
   ],
   "source": [
    "# Test the preprocessing step on a sample tweet\n",
    "tweet_num = 0\n",
    "sampleTweet = tweetsDF[tweetsDF['index'] == tweet_num].Text.iloc[0]\n",
    "\n",
    "print(\"Original tweet: \")\n",
    "words = []\n",
    "for word in sampleTweet.split():\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nPreprocessed tweet: \")\n",
    "print(preprocess(sampleTweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [state, health, offic, scott, harri, issu, stay, home, order, strict, quarantin, requir, read, full]\n",
       "1                                                      [thank, nurs, help, keep, healthi, covid, coronavirus]\n",
       "2                                                                        [togetherapart, slow, spread, covid]\n",
       "3                    [smoker, greater, risk, contract, coronavirus, elli, cannon, say, equal, risk, contract]\n",
       "4                                      [video, model, scan, show, extent, covid, damag, lung, tissu, stayhom]\n",
       "5                      [leader, hous, parti, caucus, arizona, andi, bigg, think, spread, covid, much, possib]\n",
       "6                                                                                     [covid, test, administ]\n",
       "7           [keep, think, master, public, health, write, doctor, dissert, global, effort, tackl, aid, pandem]\n",
       "8    [ceylonblacktea, rich, theaflavin, help, increas, human, immun, covid, srilankatea, industri, successfu]\n",
       "9                                     [peopl, first, group, hospit, sick, place, infirmari, convert, convent]\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess all tweets and generate a new processed tweet text dataset.\n",
    "processed_tweets = tweetsDF['Text'].map(preprocess)\n",
    "processed_tweets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text representation\n",
    "Computers don't understand natural language texts. Text is a mere sequence of letters for computers. While its still difficult for computer to understand what the sequence of letters mean, language is way more complicated than that. For an example, let us consider an idiom \"Kicked the bucket\". You know where I am going right? When I first heard that phrase as a kid I thought it meant someone was actually kicking a bucket. That's fun! But, when I realized that it meant someone died, it was no more fun! So, natural language is hard and computers don't understand it. \n",
    "\n",
    "Computers love numbers. At the core, computers perform their operations on numbers. So, it'd be good to represent the natural language text with numbers for computer algorithms to process easily. In the below section, we'll explore two different models for text representation namely Bag of words and TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1: Bag of words on the dataset\n",
    "Create a dictionary of words present in the preprocessed_tweets dataset. Gensim offers a great api for the same. This dictionary assigns a numerical id to each word so that you can work on the number representations of the word. This makes the data processing very easy than working on strings. \n",
    "\n",
    "Then create a corpus of Bag of words where words are represented by their numerical ids along with the frequency of occurence of that word in the tweet for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_tweets)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 full\n",
      "1 harri\n",
      "2 health\n",
      "3 home\n",
      "4 issu\n",
      "5 offic\n"
     ]
    }
   ],
   "source": [
    "# Check the id to word mapping from the dictionary created above\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the text corpus is very huge and sparse, we should try to minimize the amount of text being used for modeling. For this reason, let us remove very rare and very common words. Gensim dictionary object provides a good api to perform this operation.\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "\n",
    "Then convert it into bag of word corpus with very rare and very common wordsd filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_tweets]\n",
    "\n",
    "# Test the Bag of Words representation of the tweet --> (token_id, token_count)\n",
    "bow_corpus[tweet_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"health\") appears 1 time.\n",
      "Word 1 (\"home\") appears 1 time.\n",
      "Word 2 (\"order\") appears 1 time.\n",
      "Word 3 (\"state\") appears 1 time.\n",
      "Word 4 (\"stay\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Preview BOW for our sample preprocessed tweet\n",
    "bow_tweet_0 = bow_corpus[tweet_num]\n",
    "\n",
    "for i in range(len(bow_tweet_0)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_tweet_0[i][0], \n",
    "                                                     dictionary[bow_tweet_0[i][0]], \n",
    "                                                     bow_tweet_0[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2: TF-IDF on the data set\n",
    "TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. Summing the Tf-idf of all possible terms and documents recovers the mutual information between documents and term taking into account all the specificities of their joint distribution.\n",
    "\n",
    "TF (Term Frequency) - number of times a term occurs in a document.  \n",
    "IDF (Inverse Document Frequency) diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.40009295170061265),\n",
       " (1, 0.44472924895798494),\n",
       " (2, 0.45253501051552114),\n",
       " (3, 0.47433139975516847),\n",
       " (4, 0.4608289406979316)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tf-idf model object using models.TfidfModel\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# Apply transformation to the entire corpus\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "\n",
    "# Test the tf-idf representation of the sample tweet. Each word is represented by (token_id, tf-idf score).\n",
    "tfidf_corpus[tweet_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"health\") TF-IDF score: 0.40009295170061265.\n",
      "Word 1 (\"home\") TF-IDF score: 0.44472924895798494.\n",
      "Word 2 (\"order\") TF-IDF score: 0.45253501051552114.\n",
      "Word 3 (\"state\") TF-IDF score: 0.47433139975516847.\n",
      "Word 4 (\"stay\") TF-IDF score: 0.4608289406979316.\n"
     ]
    }
   ],
   "source": [
    "# Preview TF-IDF for our sample preprocessed tweet\n",
    "tfidf_tweet_0 = tfidf_corpus[tweet_num]\n",
    "\n",
    "for i in range(len(tfidf_tweet_0)):\n",
    "    print(\"Word {} (\\\"{}\\\") TF-IDF score: {}.\".format(tfidf_tweet_0[i][0], \n",
    "                                                     dictionary[tfidf_tweet_0[i][0]], \n",
    "                                                     tfidf_tweet_0[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Topic modeling using LDA\n",
    "Topic modeling is a statistical model to discover the abstract topics in a collection of documents. Probabilistic Latent Semantic Analysis(PLSA) is one of the ealiest models for topic modeling. Latent Dirichlet Allocation(LDA) is the most common topic model algorithm in use today which is a generalization of PLSA. \n",
    "\n",
    "LDA introduces sparse Dirichlet prior distributions over document-topic and topic-word distributions. This algorithm tries to model the intuition that each document has different abstract topics and that each topic is generalized by a small number of words.\n",
    "\n",
    "In this section we'll be building the topic models using LDA for both text representations developed above. \n",
    "\n",
    "\n",
    "## 4.1: LDA using Bag of Words\n",
    "In this section, we'll train the LDA model to generate topic clusters using Bag of Words (BOW) corpus. The LDA algorithm requires a few inputs to build the clusters. The main parameter it requires is the number of clusters we want the model to group the tweets into. But how do we identify the number of topics? Let's start with some intuitive value like 5 or 10. We'll then tune this hyper parameter in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the lda model using gensim.models.LdaMulticore on Bag of word corpus\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=5, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.050*\"mask\" + 0.047*\"help\" + 0.041*\"send\" + 0.039*\"time\" + 0.039*\"today\" + 0.038*\"thank\" + 0.037*\"need\" + 0.037*\"say\" + 0.037*\"coronavirus\" + 0.032*\"right\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.089*\"coronavirus\" + 0.035*\"everyon\" + 0.034*\"help\" + 0.034*\"affect\" + 0.031*\"make\" + 0.031*\"pleas\" + 0.027*\"protect\" + 0.027*\"poor\" + 0.024*\"face\" + 0.024*\"discrimin\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.053*\"work\" + 0.052*\"home\" + 0.049*\"caus\" + 0.048*\"trut\" + 0.048*\"father\" + 0.048*\"natur\" + 0.048*\"racism\" + 0.048*\"speak\" + 0.047*\"poverti\" + 0.047*\"murder\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.062*\"like\" + 0.042*\"public\" + 0.038*\"never\" + 0.033*\"amaz\" + 0.031*\"live\" + 0.030*\"peopl\" + 0.029*\"often\" + 0.029*\"work\" + 0.028*\"gullibl\" + 0.028*\"narrat\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.077*\"case\" + 0.060*\"test\" + 0.049*\"peopl\" + 0.043*\"health\" + 0.028*\"human\" + 0.028*\"keep\" + 0.024*\"april\" + 0.024*\"death\" + 0.023*\"state\" + 0.023*\"make\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the words occuring in that topic and its relative weight\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2: LDA using TF-IDF\n",
    "TF-IDF intends to reflect on the importance of each word in the tweet amongst other tweets. Thus it tries to create a better model instead of using mere Term Frequency as in Bag of words model. However, for TF-IDF to work it needs to have a good size of text in each document. However, tweet is usually very small in size. Thus, most of the times each word ends up being mentioned only once. Thus, TF-IDF doesn't work better for short texts. However, let's train the model and evaluate the performance and see how does it perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train lda model using corpus_tfidf\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(tfidf_corpus, \n",
    "                                             num_topics=5, \n",
    "                                             id2word = dictionary, \n",
    "                                             passes = 2, \n",
    "                                             workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.057*\"test\" + 0.051*\"make\" + 0.045*\"pandem\" + 0.040*\"die\" + 0.033*\"help\" + 0.033*\"ask\" + 0.031*\"hospit\" + 0.031*\"confirm\" + 0.029*\"patient\" + 0.027*\"equip\"\n",
      "\n",
      "\n",
      "Topic: 1 Word: 0.051*\"case\" + 0.033*\"caus\" + 0.032*\"natur\" + 0.031*\"father\" + 0.031*\"speak\" + 0.031*\"poverti\" + 0.031*\"trut\" + 0.031*\"racism\" + 0.031*\"murder\" + 0.031*\"report\"\n",
      "\n",
      "\n",
      "Topic: 2 Word: 0.040*\"health\" + 0.039*\"order\" + 0.034*\"thank\" + 0.033*\"medic\" + 0.033*\"time\" + 0.029*\"stay\" + 0.029*\"coronavirus\" + 0.028*\"take\" + 0.027*\"state\" + 0.026*\"keep\"\n",
      "\n",
      "\n",
      "Topic: 3 Word: 0.085*\"coronavirus\" + 0.079*\"fight\" + 0.048*\"china\" + 0.035*\"india\" + 0.032*\"support\" + 0.031*\"enough\" + 0.030*\"work\" + 0.027*\"protect\" + 0.024*\"live\" + 0.023*\"make\"\n",
      "\n",
      "\n",
      "Topic: 4 Word: 0.073*\"peopl\" + 0.049*\"spread\" + 0.044*\"mask\" + 0.040*\"need\" + 0.037*\"think\" + 0.036*\"trump\" + 0.033*\"say\" + 0.032*\"pleas\" + 0.031*\"home\" + 0.028*\"week\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the words occuring in that topic and its relative weight\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model evaluation\n",
    "In the previous section, we created the topic models using LDA. But how do we evaluate if the clustering is good at all? Topic cluster is a statistical tool that imposes probability distribution over words. There are so many ways to evaluate the models. Some of the ways to evaluate the models are mentioned below.\n",
    "* By visualization, we can generate an intuition behind the quality of the topics by visualizing. \n",
    "> * We can try visualizing the clusters in intertopic distance mapping via multi-dimensional scaling. In this 2 dimensional map, if the clusters are overlapping then the cluster segregation wasn't good. On the otherhand, if the clusters were separated clearly, that'd be a good model. We'll explore this more when we visualize the clusters using PyLDAVis in the further sections.\n",
    "> * We could also try running the sample examples through the model and see how well the sample examples get classified into topics and how well they match the clusters. \n",
    "\n",
    "* Using performance evaluation metrics\n",
    "We could also try using different clustering evaluation metrics. There are two types of metrics intrinsic and extrinsic metrics. \n",
    "> * Perplexity is a good intrinsic performance evaluation metric. It measures how well the probability distribution predicts a sample. A low perplexity indicates the probablity distribution is good at predicting the sample.\n",
    "> * However, coherence measures have shown to be better metrics for topic cluster evaluations. So, in this section, we use coherence measures to quantify the quality of the clusters and the probability distribution of the words. \n",
    "\n",
    "\n",
    "## 5.1. Evaluation by human validation\n",
    "In this section, we'll cluster a sample tweet and see how well does this clustering matches the overall topics generated above.\n",
    "\n",
    "### 5.1.1: Human validation for Bag of words model\n",
    "Classify a sample tweet into the topics and then evaluate if the general topic matches with the tweet better than other topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our test tweet is: 0: ['health', 'home', 'order', 'state', 'stay']\n",
      "\n",
      "Score: 0.5977810025215149\t \n",
      "Topic: 2 : 0.044*\"health\" + 0.034*\"case\" + 0.034*\"keep\" + 0.033*\"pandem\" + 0.026*\"time\" + 0.025*\"death\" + 0.024*\"care\" + 0.023*\"human\" + 0.023*\"like\" + 0.021*\"trump\"\n",
      "\n",
      "Score: 0.34373247623443604\t \n",
      "Topic: 0 : 0.043*\"work\" + 0.038*\"natur\" + 0.035*\"caus\" + 0.034*\"father\" + 0.034*\"murder\" + 0.033*\"speak\" + 0.033*\"trut\" + 0.033*\"poverti\" + 0.033*\"racism\" + 0.030*\"mask\"\n",
      "\n",
      "Score: 0.05848647654056549\t \n",
      "Topic: 1 : 0.070*\"coronavirus\" + 0.065*\"peopl\" + 0.050*\"test\" + 0.041*\"help\" + 0.029*\"affect\" + 0.025*\"everyon\" + 0.021*\"hospit\" + 0.020*\"fight\" + 0.020*\"poor\" + 0.019*\"either\"\n"
     ]
    }
   ],
   "source": [
    "# Our test tweet is \n",
    "print('Our test tweet is: {}: {}'.format(tweet_num, [dictionary[word[0]] for word in bow_corpus[tweet_num]]))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_corpus[tweet_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {} : {}\".format(score, index, lda_model.print_topic(index, 10))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample tweet is classified to Topic 0 with 88% score. Topic 0 in Bag of word model was centered around self quarantining. The sample tweet matches with this topic. We could try evaluating more tweets manually. Seems like the BOW based LDA model worked well.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Human validation for TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our test tweet is: 0: ['health', 'home', 'order', 'state', 'stay']\n",
      "\n",
      "Score: 0.6827797293663025\t \n",
      "Topic: 2 : 0.044*\"health\" + 0.034*\"case\" + 0.034*\"keep\" + 0.033*\"pandem\" + 0.026*\"time\" + 0.025*\"death\" + 0.024*\"care\" + 0.023*\"human\" + 0.023*\"like\" + 0.021*\"trump\"\n",
      "\n",
      "Score: 0.1886352002620697\t \n",
      "Topic: 1 : 0.070*\"coronavirus\" + 0.065*\"peopl\" + 0.050*\"test\" + 0.041*\"help\" + 0.029*\"affect\" + 0.025*\"everyon\" + 0.021*\"hospit\" + 0.020*\"fight\" + 0.020*\"poor\" + 0.019*\"either\"\n",
      "\n",
      "Score: 0.128585085272789\t \n",
      "Topic: 0 : 0.043*\"work\" + 0.038*\"natur\" + 0.035*\"caus\" + 0.034*\"father\" + 0.034*\"murder\" + 0.033*\"speak\" + 0.033*\"trut\" + 0.033*\"poverti\" + 0.033*\"racism\" + 0.030*\"mask\"\n"
     ]
    }
   ],
   "source": [
    "# Our test tweet is \n",
    "print('Our test tweet is: {}: {}'.format(tweet_num, [dictionary[word[0]] for word in tfidf_corpus[tweet_num]]))\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[tfidf_corpus[tweet_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {} : {}\".format(score, index, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the sample tweet is split between topics 0 with 49% score and topic 1 with 38% score. As we saw in the section 4.2.1, Topic 0 was centered around President Trump's announcement around COVID 19 and Topic 1 around Quarantine and fight the spread of COVID-19. \n",
    "\n",
    "The sample tweet matches well with Topic 1. However, the confidence of this clustering is low compared to 88% confidence we saw for Bag of Words model. \n",
    "\n",
    "The TF-IDF modeling didn't have good confidence in the classification. This was expected as TF-IDF doesn't work good for short text documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Coherence measures\n",
    "The topic coherence measures scores a single topic by computing the semantic similarity between the top words in that topic. We can then average the scores of each topic to get the overall coherence measure for the model.\n",
    "\n",
    "There are different coherence measures. In the below evaluation we'll be using the C_v measure. C_v measure is based on a sliding window, a one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosinus similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score: 0.4304417242985787\n",
      "Per Topic: [0.4493676045869341, 0.33174836101253, 0.510209207296272]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score for BOW based LDA model\n",
    "cv_model_bow = CoherenceModel(model=lda_model, texts=processed_tweets, dictionary=dictionary, coherence='c_v')\n",
    "cv_bow_overall = cv_model_bow.get_coherence()\n",
    "cv_bow_pertopic = cv_model_bow.get_coherence_per_topic()\n",
    "print('\\nCoherence Score: {0}\\nPer Topic: {1}'.format(cv_bow_overall , cv_bow_pertopic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score: 0.5277570239510211\n",
      "Per Topic: [0.5811252711244042, 0.46330507155149236, 0.5388407291771666]\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score for TF-IDF based LDA model\n",
    "cv_model_tfidf = CoherenceModel(model=lda_model_tfidf, texts=processed_tweets, dictionary=lda_model_tfidf.id2word, coherence='c_v')\n",
    "cv_tfidf_overall = cv_model_tfidf.get_coherence()\n",
    "cv_tfidf_pertopic = cv_model_tfidf.get_coherence_per_topic()\n",
    "print('\\nCoherence Score: {0}\\nPer Topic: {1}'.format(cv_tfidf_overall, cv_tfidf_pertopic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the coherence measures, it seems that the tf-idf model seemed to have gathered the topics based on better semantic similarity between the words. This was contradictory to what we saw in the previous section on human validation of the topics. However, the previous measure was done on a few sample tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Hyper parameter tuning \n",
    "Now that we have established the modeling and performance evaluation methods, let's try to tune the Number of Topics hyper parameter. We'll define a range for the number of topics. Then we'll train a model for each value for the hyper parameter. We'll also compute the coherence score and jot it down. At the end we'll plot the scores to see what's the best for the Number of Topics hyper parameter.\n",
    "\n",
    "We'll repeat the same for both BOW and TF-IDF models and compare the scores to choose the best model.\n",
    "\n",
    "The LDA model has more hyper-parameters namely alpha and beta values. We could tune the alpha and beta hyperparameters also in the similar fashion. \n",
    "\n",
    "For parameter tuning, sklearn exposes a GridSearchCV api that can be configured easily with ranges for different hyper parameters. For this, we can use the LDA_Transform model exposed by gensim. However, that approach uses the default log_likelihood score for tuning the hyper parameters. Since we decided to use coherence scores, we'll tune the hyper parameters in a loop instead of using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(corpus, dictionary, num_topics):\n",
    "    lda_model = gensim.models.LdaMulticore(corpus, \n",
    "                                       num_topics=num_topics, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=2)\n",
    "        \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_tweets, dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    return lda_model, coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 1/18 [00:01<00:25,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 2/18 [00:02<00:23,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 3/18 [00:04<00:21,  1.43s/it]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 4/18 [00:05<00:18,  1.36s/it]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 5/18 [00:06<00:16,  1.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 6/18 [00:07<00:15,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 7/18 [00:09<00:14,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 8/18 [00:10<00:12,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 9/18 [00:11<00:11,  1.30s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 10/18 [00:13<00:10,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 11/18 [00:14<00:09,  1.30s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 12/18 [00:15<00:07,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 13/18 [00:16<00:06,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 14/18 [00:18<00:05,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 15/18 [00:19<00:03,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 16/18 [00:20<00:02,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 17/18 [00:21<00:01,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 18/18 [00:22<00:00,  1.27s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Models\n",
    "model_sets = [bow_corpus, tfidf_corpus]\n",
    "model_title = ['BOW', 'TF-IDF']\n",
    "\n",
    "model_results = pd.DataFrame(columns=['Model','Num_Topics','Coherence'])\n",
    "\n",
    "pbar = tqdm.tqdm(total=len(model_sets)*len(topics_range))\n",
    "    \n",
    "if 1 == 1:\n",
    "  # iterate through validation corpuses\n",
    "  for i in range(len(corpus_sets)):\n",
    "    # iterate through number of topics\n",
    "    for k in topics_range:\n",
    "        # get the coherence score for the given parameters\n",
    "        model, cv = compute_coherence_values(corpus=model_sets[i], dictionary=dictionary, num_topics=k)\n",
    "        \n",
    "        # Save the model results\n",
    "        results = {'Model':model_title[i]\n",
    "                   ,'Num_Topics':k\n",
    "                   ,'Coherence':cv}\n",
    "        \n",
    "        model_results = model_results.append(results , ignore_index=True)\n",
    "        pbar.update(1)\n",
    "\n",
    "  model_results.to_csv('lda_tuning_results.csv', index=False)\n",
    "  pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverlabel": {
          "namelength": 0
         },
         "hovertemplate": "Model=BOW<br>Num_Topics=%{x}<br>Coherence=%{y}",
         "legendgroup": "BOW",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "BOW",
         "showlegend": true,
         "type": "scatter",
         "x": [
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
         ],
         "xaxis": "x",
         "y": [
          0.45075026430197257,
          0.465890600796209,
          0.4527859902306416,
          0.3970279139433873,
          0.41476066892654223,
          0.42239459304722216,
          0.43805771408883615,
          0.4575843336348264,
          0.4301761090700917
         ],
         "yaxis": "y"
        },
        {
         "hoverlabel": {
          "namelength": 0
         },
         "hovertemplate": "Model=TF-IDF<br>Num_Topics=%{x}<br>Coherence=%{y}",
         "legendgroup": "TF-IDF",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "TF-IDF",
         "showlegend": true,
         "type": "scatter",
         "x": [
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
         ],
         "xaxis": "x",
         "y": [
          0.4795015856177397,
          0.49358155212438176,
          0.5146147554608785,
          0.49615503684388046,
          0.5020765962507201,
          0.4460433012413719,
          0.4449069098342686,
          0.4599870730913197,
          0.47777041983050583
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Model"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Num_Topics"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Coherence"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"5e1f693a-a019-4df7-998e-bd84c55e7129\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"5e1f693a-a019-4df7-998e-bd84c55e7129\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '5e1f693a-a019-4df7-998e-bd84c55e7129',\n",
       "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Model=BOW<br>Num_Topics=%{x}<br>Coherence=%{y}\", \"legendgroup\": \"BOW\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"BOW\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [2, 3, 4, 5, 6, 7, 8, 9, 10], \"xaxis\": \"x\", \"y\": [0.45075026430197257, 0.465890600796209, 0.4527859902306416, 0.3970279139433873, 0.41476066892654223, 0.42239459304722216, 0.43805771408883615, 0.4575843336348264, 0.4301761090700917], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Model=TF-IDF<br>Num_Topics=%{x}<br>Coherence=%{y}\", \"legendgroup\": \"TF-IDF\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"TF-IDF\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [2, 3, 4, 5, 6, 7, 8, 9, 10], \"xaxis\": \"x\", \"y\": [0.4795015856177397, 0.49358155212438176, 0.5146147554608785, 0.49615503684388046, 0.5020765962507201, 0.4460433012413719, 0.4449069098342686, 0.4599870730913197, 0.47777041983050583], \"yaxis\": \"y\"}],\n",
       "                        {\"legend\": {\"title\": {\"text\": \"Model\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Num_Topics\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Coherence\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5e1f693a-a019-4df7-998e-bd84c55e7129');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = px.line(model_results, x='Num_Topics', y='Coherence',color='Model')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above models, seems like both BOW and TF-IDF models with 2 topics performs the best. So, let's train the model with the tuned hyper parameters and use that for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_bow = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=3, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=2)\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(tfidf_corpus, \n",
    "                                       num_topics=4, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Visualization using PyLDAVis\n",
    "In this section, we'll visualize the topics generated by the above LDA model using pyLDAvis library. pyLDAvis provides an amazing interactive visualization tool to see how different clusters are generated. It produces the intertopic distance map and shows top relevant terms for each topic amongst other features. \n",
    "\n",
    "As mentioned above, we'll visualize the intertopic distance map to see if there is a good segregation of clusters. If there is an overlap between multiple clusters, we'd reduce the number of topics and run the LDA model with reduced number of clusters and visualize again. Once we are satisfied with the cluster segregation in the intertopic distance map, we can start looking into the terms to see what each topic represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning:\n",
      "\n",
      "Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el6057352073265448213170028\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el6057352073265448213170028_data = {\"mdsDat\": {\"x\": [-0.11317561795361519, 0.21911634929791474, -0.10594073134429968], \"y\": [0.12055897200450591, 0.0026833148518561134, -0.123242286856362], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [35.43104934692383, 32.33424377441406, 32.23470687866211]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [56.0, 42.0, 72.0, 40.0, 40.0, 53.0, 38.0, 38.0, 38.0, 39.0, 71.0, 51.0, 45.0, 31.0, 42.0, 24.0, 42.0, 26.0, 41.0, 28.0, 22.0, 25.0, 25.0, 48.0, 37.0, 25.0, 20.0, 19.0, 29.0, 17.0, 23.49262809753418, 21.398134231567383, 49.77590560913086, 24.524337768554688, 17.2181396484375, 16.257692337036133, 28.759944915771484, 23.13719940185547, 16.788557052612305, 13.433550834655762, 14.150691032409668, 35.53792190551758, 34.73421096801758, 22.859636306762695, 32.86962127685547, 17.71466636657715, 12.146368980407715, 17.261377334594727, 19.83136749267578, 21.696056365966797, 11.131490707397461, 16.5485897064209, 11.359245300292969, 14.688076972961426, 33.231815338134766, 21.195634841918945, 10.605098724365234, 13.315720558166504, 15.935676574707031, 12.124200820922852, 22.230758666992188, 19.892223358154297, 23.575551986694336, 17.296142578125, 16.58230972290039, 13.911274909973145, 37.36268615722656, 37.32383346557617, 37.31831741333008, 39.03676223754883, 41.5081787109375, 39.7269172668457, 37.84862518310547, 13.831755638122559, 13.80993366241455, 13.809591293334961, 13.798210144042969, 52.50623321533203, 14.299687385559082, 41.252891540527344, 17.292667388916016, 14.976826667785645, 60.56104278564453, 19.823591232299805, 12.25583553314209, 11.586928367614746, 15.823037147521973, 11.530588150024414, 12.25837230682373, 14.373879432678223, 8.821338653564453, 10.235734939575195, 46.67116165161133, 17.55014419555664, 7.830440521240234, 12.271860122680664, 12.235337257385254, 9.765713691711426, 16.0470027923584, 18.66058349609375, 14.079081535339355, 14.066035270690918, 15.775547981262207, 13.996199607849121, 45.184024810791016, 22.32345962524414, 16.25017738342285, 17.7230281829834, 24.383378982543945, 21.71030616760254, 12.508400917053223, 55.459259033203125, 22.409683227539062, 13.353559494018555, 23.382102966308594, 14.461569786071777, 16.39163589477539, 11.211126327514648, 25.36322021484375, 14.294690132141113, 18.10454559326172, 20.57326316833496, 14.824333190917969, 13.472905158996582, 12.649775505065918, 24.093639373779297, 22.823476791381836, 22.311689376831055, 24.49744415283203, 28.847787857055664, 15.386725425720215, 13.523921966552734, 13.743764877319336], \"Term\": [\"work\", \"speak\", \"case\", \"caus\", \"father\", \"health\", \"poverti\", \"racism\", \"trut\", \"murder\", \"peopl\", \"test\", \"natur\", \"time\", \"fight\", \"face\", \"make\", \"say\", \"mask\", \"state\", \"doctor\", \"think\", \"govern\", \"help\", \"like\", \"human\", \"posit\", \"confirm\", \"virus\", \"rich\", \"face\", \"doctor\", \"health\", \"say\", \"global\", \"cover\", \"time\", \"think\", \"even\", \"recommend\", \"import\", \"fight\", \"make\", \"care\", \"mask\", \"china\", \"life\", \"equip\", \"protect\", \"patient\", \"support\", \"thank\", \"enough\", \"worker\", \"pandem\", \"trump\", \"look\", \"call\", \"live\", \"countri\", \"need\", \"today\", \"help\", \"stay\", \"coronavirus\", \"peopl\", \"poverti\", \"racism\", \"trut\", \"father\", \"speak\", \"caus\", \"murder\", \"gullibl\", \"narrat\", \"propaganda\", \"ceas\", \"work\", \"often\", \"natur\", \"confirm\", \"amaz\", \"case\", \"pleas\", \"die\", \"week\", \"report\", \"medic\", \"break\", \"never\", \"great\", \"total\", \"coronavirus\", \"public\", \"mani\", \"spread\", \"take\", \"order\", \"rich\", \"posit\", \"cast\", \"religion\", \"isol\", \"basi\", \"test\", \"govern\", \"either\", \"lockdown\", \"state\", \"human\", \"governor\", \"peopl\", \"virus\", \"ask\", \"everyon\", \"discrimin\", \"poor\", \"corona\", \"like\", \"nation\", \"affect\", \"order\", \"want\", \"send\", \"nurs\", \"keep\", \"home\", \"death\", \"help\", \"coronavirus\", \"hospit\", \"april\", \"today\"], \"Total\": [56.0, 42.0, 72.0, 40.0, 40.0, 53.0, 38.0, 38.0, 38.0, 39.0, 71.0, 51.0, 45.0, 31.0, 42.0, 24.0, 42.0, 26.0, 41.0, 28.0, 22.0, 25.0, 25.0, 48.0, 37.0, 25.0, 20.0, 19.0, 29.0, 17.0, 24.673864364624023, 22.71882438659668, 53.306251525878906, 26.674428939819336, 18.737321853637695, 17.720443725585938, 31.586307525634766, 25.559335708618164, 18.67628288269043, 15.781865119934082, 16.707414627075195, 42.15256118774414, 42.15792465209961, 28.548999786376953, 41.514469146728516, 22.716934204101562, 15.741085052490234, 22.490934371948242, 27.117551803588867, 30.07137680053711, 15.48598575592041, 23.38433837890625, 16.549549102783203, 21.42721176147461, 49.22578811645508, 31.56816864013672, 15.834339141845703, 20.22405242919922, 25.55109977722168, 19.48162269592285, 36.271697998046875, 34.66954040527344, 48.6665153503418, 33.41484832763672, 92.10125732421875, 71.58795928955078, 38.2340202331543, 38.23719787597656, 38.23800277709961, 40.06468200683594, 42.81618881225586, 40.994712829589844, 39.18035888671875, 14.602858543395996, 14.605000495910645, 14.604955673217773, 14.606108665466309, 56.70410919189453, 15.55077075958252, 45.83469772338867, 19.253305435180664, 17.485837936401367, 72.82120513916016, 26.95355796813965, 17.740962982177734, 17.78456687927246, 25.3367919921875, 18.812116622924805, 20.698896408081055, 24.447595596313477, 16.06222915649414, 18.954238891601562, 92.10125732421875, 35.09347915649414, 16.14351463317871, 25.70978546142578, 35.644386291503906, 32.97428894042969, 17.84357452392578, 20.818819046020508, 15.847042083740234, 15.845261573791504, 17.822797775268555, 15.839362144470215, 51.757415771484375, 25.79023551940918, 18.80436134338379, 20.820402145385742, 28.835830688476562, 25.838712692260742, 15.884511947631836, 71.58795928955078, 29.829565048217773, 17.819656372070312, 31.45118522644043, 19.801116943359375, 22.740755081176758, 15.64018440246582, 37.791744232177734, 21.672588348388672, 28.451156616210938, 32.97428894042969, 23.85157585144043, 21.858028411865234, 20.702363967895508, 39.544403076171875, 37.61965560913086, 37.35497283935547, 48.6665153503418, 92.10125732421875, 28.45654296875, 23.106903076171875, 34.66954040527344], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9884999990463257, 0.9776999950408936, 0.9690999984741211, 0.953499972820282, 0.953000009059906, 0.9513999819755554, 0.9437999725341797, 0.9380000233650208, 0.9309999942779541, 0.8765000104904175, 0.8715000152587891, 0.8669000267982483, 0.8439000248908997, 0.8152999877929688, 0.804099977016449, 0.7889000177383423, 0.7782999873161316, 0.7728999853134155, 0.7246999740600586, 0.7110999822616577, 0.7074000239372253, 0.6917999982833862, 0.661300003528595, 0.6600000262260437, 0.6446999907493591, 0.63919997215271, 0.6366999745368958, 0.619700014591217, 0.565500020980835, 0.5633000135421753, 0.5479999780654907, 0.4819999933242798, 0.31279999017715454, 0.3790999948978424, -0.6769999861717224, -0.600600004196167, 1.1059999465942383, 1.1049000024795532, 1.104699969291687, 1.1030999422073364, 1.0980000495910645, 1.097599983215332, 1.094499945640564, 1.0748000144958496, 1.073099970817566, 1.0729999542236328, 1.072100043296814, 1.0520999431610107, 1.045199990272522, 1.0236999988555908, 1.0216000080108643, 0.9742000102996826, 0.9447000026702881, 0.8217999935150146, 0.7591999769210815, 0.7006000280380249, 0.65829998254776, 0.6395000219345093, 0.6051999926567078, 0.5978999733924866, 0.529699981212616, 0.5128999948501587, 0.44929999113082886, 0.4361000061035156, 0.40549999475479126, 0.3894999921321869, 0.05979999899864197, -0.08780000358819962, 1.0260000228881836, 1.0226999521255493, 1.013800024986267, 1.0130000114440918, 1.01010000705719, 1.0083999633789062, 0.9962999820709229, 0.9878000020980835, 0.9861000180244446, 0.9710999727249146, 0.9643999934196472, 0.9580000042915344, 0.8931999802589417, 0.876800000667572, 0.8460999727249146, 0.8435999751091003, 0.8356999754905701, 0.8179000020027161, 0.8047000169754028, 0.7991999983787537, 0.733299970626831, 0.7160000205039978, 0.6801000237464905, 0.6603999733924866, 0.656499981880188, 0.6481999754905701, 0.6395000219345093, 0.6366999745368958, 0.6323999762535095, 0.6168000102043152, 0.4456999897956848, -0.028699999675154686, 0.517300009727478, 0.5964999794960022, 0.2069000005722046], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.8036999702453613, -3.8970999717712402, -3.0529000759124756, -3.7607998847961426, -4.114500045776367, -4.171899795532227, -3.6013998985290527, -3.819000005722046, -4.139699935913086, -4.36269998550415, -4.310699939727783, -3.3898000717163086, -3.4126999378204346, -3.8310000896453857, -3.467900037765503, -4.085999965667725, -4.463399887084961, -4.1118998527526855, -3.9732000827789307, -3.8833000659942627, -4.550600051879883, -4.154099941253662, -4.530399799346924, -4.273399829864502, -3.456899881362915, -3.906599998474121, -4.599100112915039, -4.371500015258789, -4.19189977645874, -4.465199947357178, -3.8589000701904297, -3.970099925994873, -3.8001999855041504, -4.109899997711182, -4.152100086212158, -4.327700138092041, -3.248300075531006, -3.249300003051758, -3.249500036239624, -3.2044999599456787, -3.1431000232696533, -3.1868999004364014, -3.2353999614715576, -4.242000102996826, -4.243599891662598, -4.243599891662598, -4.2444000244140625, -2.9079999923706055, -4.208700180053711, -3.149199962615967, -4.018700122833252, -4.162499904632568, -2.7653000354766846, -3.8821001052856445, -4.36299991607666, -4.419099807739258, -4.107500076293945, -4.423999786376953, -4.36269998550415, -4.203499794006348, -4.691800117492676, -4.543099880218506, -3.0257999897003174, -4.003900051116943, -4.8109002113342285, -4.361599922180176, -4.36460018157959, -4.590099811553955, -4.090400218963623, -3.939500093460083, -4.221199989318848, -4.222099781036377, -4.107399940490723, -4.227099895477295, -3.0550999641418457, -3.760200023651123, -4.0777997970581055, -3.990999937057495, -3.671999931335449, -3.788100004196167, -4.3394999504089355, -2.8501999378204346, -3.7564001083374023, -4.274099826812744, -3.713900089263916, -4.194399833679199, -4.0690999031066895, -4.448999881744385, -3.6326000690460205, -4.205999851226807, -3.9697000980377197, -3.841900110244751, -4.169600009918213, -4.265200138092041, -4.328199863433838, -3.6839001178741455, -3.738100051879883, -3.7607998847961426, -3.66729998588562, -3.5037999153137207, -4.132400035858154, -4.26140022277832, -4.245299816131592]}, \"token.table\": {\"Topic\": [1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 2, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 2, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \"Freq\": [0.21088773012161255, 0.17573978006839752, 0.6326631903648376, 0.8578370809555054, 0.11437827348709106, 0.043277110904455185, 0.38949400186538696, 0.605879545211792, 0.16835340857505798, 0.05611780658364296, 0.729531466960907, 0.0631338581442833, 0.8838739991188049, 0.2898705303668976, 0.5797410607337952, 0.0966235101222992, 0.6427989602088928, 0.2966764569282532, 0.04944607615470886, 0.8056324124336243, 0.07005499303340912, 0.14010998606681824, 0.09612584859132767, 0.8376681208610535, 0.06866131722927094, 0.06310325860977173, 0.8834456205368042, 0.024393389001488686, 0.975735604763031, 0.024393389001488686, 0.9585030674934387, 0.7923604249954224, 0.044020023196935654, 0.17608009278774261, 0.8829652667045593, 0.10387826710939407, 0.06393786519765854, 0.19181358814239502, 0.7033165097236633, 0.18457946181297302, 0.5103079080581665, 0.31487083435058594, 0.6159651279449463, 0.20532169938087463, 0.20532169938087463, 0.902911901473999, 0.05643199384212494, 0.26770198345184326, 0.13385099172592163, 0.5889443159103394, 0.11273345351219177, 0.6764007210731506, 0.16910018026828766, 0.20200881361961365, 0.05050220340490341, 0.7070308327674866, 0.9243435859680176, 0.044016361236572266, 0.05317915230989456, 0.10635830461978912, 0.850866436958313, 0.6646707057952881, 0.18127381801605225, 0.12084921449422836, 0.7558600902557373, 0.13338707387447357, 0.08892472088336945, 0.9102453589439392, 0.05354384705424309, 0.09538590908050537, 0.15897652506828308, 0.7312920093536377, 0.9321604371070862, 0.040528714656829834, 0.9734259247779846, 0.024959638714790344, 0.8540406227111816, 0.1186167523264885, 0.04744670167565346, 0.9072801470756531, 0.053369421511888504, 0.053369421511888504, 0.07754872739315033, 0.07754872739315033, 0.8530360460281372, 0.18886321783065796, 0.06295440346002579, 0.8184072375297546, 0.2490314394235611, 0.5603207349777222, 0.18677358329296112, 0.9587163925170898, 0.9379763007164001, 0.018759526312351227, 0.037519052624702454, 0.4931522309780121, 0.020548008382320404, 0.4931522309780121, 0.3455640375614166, 0.05316369608044624, 0.6113825440406799, 0.31627172231674194, 0.14056521654129028, 0.527119517326355, 0.116104856133461, 0.0387016199529171, 0.8514356017112732, 0.8379513025283813, 0.05985366553068161, 0.05985366553068161, 0.11221582442522049, 0.8977265954017639, 0.3034563362598419, 0.07586408406496048, 0.6069126725196838, 0.762336254119873, 0.06352802366018295, 0.1270560473203659, 0.31752967834472656, 0.026460805907845497, 0.6615201234817505, 0.6261961460113525, 0.1174117773771286, 0.27396079897880554, 0.09605962038040161, 0.048029810190200806, 0.8645366430282593, 0.6946927309036255, 0.31576940417289734, 0.8302116394042969, 0.11860166490077972, 0.07116099447011948, 0.37166628241539, 0.49555504322052, 0.12388876080513, 0.794903576374054, 0.02408798784017563, 0.1686159074306488, 0.10631445795297623, 0.6378867626190186, 0.26578614115715027, 0.02552299201488495, 0.9698737263679504, 0.9585757851600647, 0.23070617020130157, 0.13842371106147766, 0.6459773182868958, 0.021817531436681747, 0.8945188522338867, 0.08727012574672699, 0.6065335273742676, 0.13784852623939514, 0.24812734127044678, 0.4090381860733032, 0.5726534724235535, 0.289821982383728, 0.09660732746124268, 0.6279476284980774, 0.06430549174547195, 0.9002768993377686, 0.09097997844219208, 0.30326658487319946, 0.6368598341941833, 0.6703803539276123, 0.08125822246074677, 0.2437746673822403, 0.7315927147865295, 0.23277950286865234, 0.06650842726230621, 0.19556361436843872, 0.02793765999376774, 0.7682856321334839, 0.14840340614318848, 0.7420170903205872, 0.11130256205797195, 0.17589566111564636, 0.08794783055782318, 0.7035826444625854, 0.0480334646999836, 0.0960669293999672, 0.9126358032226562, 0.9677245616912842, 0.9585787653923035, 0.7375296950340271, 0.2581354081630707, 0.036876484751701355, 0.3419438600540161, 0.5129157900810242, 0.1424766182899475, 0.9676441550254822, 0.8237302899360657, 0.06336386501789093, 0.12672773003578186, 0.0631103515625, 0.883544921875, 0.23680977523326874, 0.6314927339553833, 0.11840488761663437, 0.05604258179664612, 0.8966813087463379, 0.937227189540863, 0.037489086389541626, 0.07497817277908325, 0.365998238325119, 0.5947471261024475, 0.9809373617172241, 0.023355651646852493, 0.15558278560638428, 0.46674835681915283, 0.3500612676143646, 0.10403723269701004, 0.03467907756567001, 0.8322978615760803, 0.5087558627128601, 0.11970726400613785, 0.35912179946899414, 0.7103196382522583, 0.2582980692386627, 0.30860400199890137, 0.33665889501571655, 0.3647138178348541, 0.09660451114177704, 0.038641806691884995, 0.8694406151771545, 0.7269822955131531, 0.1710546612739563, 0.12829099595546722, 0.8998668789863586, 0.07824929803609848, 0.03912464901804924, 0.9181193709373474, 0.031659286469221115, 0.06331857293844223, 0.5768752694129944, 0.02884376235306263, 0.4038126766681671, 0.05275864526629448, 0.5275864601135254, 0.42206916213035583, 0.6652270555496216, 0.0633549615740776, 0.2534198462963104, 0.967623770236969, 0.23466651141643524, 0.033523786813020706, 0.7375233173370361, 0.37733355164527893, 0.628889262676239, 0.22491410374641418, 0.6747423410415649, 0.11245705187320709, 0.0176354069262743, 0.9346765279769897, 0.05290621891617775, 0.7000443935394287, 0.18667851388454437, 0.14000888168811798], \"Term\": [\"affect\", \"affect\", \"affect\", \"amaz\", \"amaz\", \"april\", \"april\", \"april\", \"ask\", \"ask\", \"ask\", \"basi\", \"basi\", \"break\", \"break\", \"break\", \"call\", \"call\", \"call\", \"care\", \"care\", \"care\", \"case\", \"case\", \"case\", \"cast\", \"cast\", \"caus\", \"caus\", \"caus\", \"ceas\", \"china\", \"china\", \"china\", \"confirm\", \"confirm\", \"corona\", \"corona\", \"corona\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"countri\", \"countri\", \"countri\", \"cover\", \"cover\", \"death\", \"death\", \"death\", \"die\", \"die\", \"die\", \"discrimin\", \"discrimin\", \"discrimin\", \"doctor\", \"doctor\", \"either\", \"either\", \"either\", \"enough\", \"enough\", \"enough\", \"equip\", \"equip\", \"equip\", \"even\", \"even\", \"everyon\", \"everyon\", \"everyon\", \"face\", \"face\", \"father\", \"father\", \"fight\", \"fight\", \"fight\", \"global\", \"global\", \"global\", \"govern\", \"govern\", \"govern\", \"governor\", \"governor\", \"governor\", \"great\", \"great\", \"great\", \"gullibl\", \"health\", \"health\", \"health\", \"help\", \"help\", \"help\", \"home\", \"home\", \"home\", \"hospit\", \"hospit\", \"hospit\", \"human\", \"human\", \"human\", \"import\", \"import\", \"import\", \"isol\", \"isol\", \"keep\", \"keep\", \"keep\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"live\", \"live\", \"live\", \"lockdown\", \"lockdown\", \"lockdown\", \"look\", \"look\", \"make\", \"make\", \"make\", \"mani\", \"mani\", \"mani\", \"mask\", \"mask\", \"mask\", \"medic\", \"medic\", \"medic\", \"murder\", \"murder\", \"narrat\", \"nation\", \"nation\", \"nation\", \"natur\", \"natur\", \"natur\", \"need\", \"need\", \"need\", \"never\", \"never\", \"nurs\", \"nurs\", \"nurs\", \"often\", \"often\", \"order\", \"order\", \"order\", \"pandem\", \"pandem\", \"pandem\", \"patient\", \"patient\", \"patient\", \"peopl\", \"peopl\", \"peopl\", \"pleas\", \"pleas\", \"pleas\", \"poor\", \"poor\", \"poor\", \"posit\", \"posit\", \"posit\", \"poverti\", \"propaganda\", \"protect\", \"protect\", \"protect\", \"public\", \"public\", \"public\", \"racism\", \"recommend\", \"recommend\", \"recommend\", \"religion\", \"religion\", \"report\", \"report\", \"report\", \"rich\", \"rich\", \"say\", \"say\", \"say\", \"send\", \"send\", \"speak\", \"speak\", \"spread\", \"spread\", \"spread\", \"state\", \"state\", \"state\", \"stay\", \"stay\", \"stay\", \"support\", \"support\", \"take\", \"take\", \"take\", \"test\", \"test\", \"test\", \"thank\", \"thank\", \"thank\", \"think\", \"think\", \"think\", \"time\", \"time\", \"time\", \"today\", \"today\", \"today\", \"total\", \"total\", \"total\", \"trump\", \"trump\", \"trump\", \"trut\", \"virus\", \"virus\", \"virus\", \"want\", \"want\", \"week\", \"week\", \"week\", \"work\", \"work\", \"work\", \"worker\", \"worker\", \"worker\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 3, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el6057352073265448213170028\", ldavis_el6057352073265448213170028_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el6057352073265448213170028\", ldavis_el6057352073265448213170028_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el6057352073265448213170028\", ldavis_el6057352073265448213170028_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.113176  0.120559       1        1  35.431049\n",
       "2      0.219116  0.002683       2        1  32.334244\n",
       "1     -0.105941 -0.123242       3        1  32.234707, topic_info=    Category       Freq         Term      Total  loglift  logprob\n",
       "42   Default  56.000000         work  56.000000  30.0000  30.0000\n",
       "40   Default  42.000000        speak  42.000000  29.0000  29.0000\n",
       "53   Default  72.000000         case  72.000000  28.0000  28.0000\n",
       "34   Default  40.000000         caus  40.000000  27.0000  27.0000\n",
       "35   Default  40.000000       father  40.000000  26.0000  26.0000\n",
       "0    Default  53.000000       health  53.000000  25.0000  25.0000\n",
       "38   Default  38.000000      poverti  38.000000  24.0000  24.0000\n",
       "39   Default  38.000000       racism  38.000000  23.0000  23.0000\n",
       "41   Default  38.000000         trut  38.000000  22.0000  22.0000\n",
       "36   Default  39.000000       murder  39.000000  21.0000  21.0000\n",
       "22   Default  71.000000        peopl  71.000000  20.0000  20.0000\n",
       "13   Default  51.000000         test  51.000000  19.0000  19.0000\n",
       "37   Default  45.000000        natur  45.000000  18.0000  18.0000\n",
       "50   Default  31.000000         time  31.000000  17.0000  17.0000\n",
       "70   Default  42.000000        fight  42.000000  16.0000  16.0000\n",
       "101  Default  24.000000         face  24.000000  15.0000  15.0000\n",
       "72   Default  42.000000         make  42.000000  14.0000  14.0000\n",
       "11   Default  26.000000          say  26.000000  13.0000  13.0000\n",
       "24   Default  41.000000         mask  41.000000  12.0000  12.0000\n",
       "3    Default  28.000000        state  28.000000  11.0000  11.0000\n",
       "14   Default  22.000000       doctor  22.000000  10.0000  10.0000\n",
       "12   Default  25.000000        think  25.000000   9.0000   9.0000\n",
       "43   Default  25.000000       govern  25.000000   8.0000   8.0000\n",
       "6    Default  48.000000         help  48.000000   7.0000   7.0000\n",
       "29   Default  37.000000         like  37.000000   6.0000   6.0000\n",
       "18   Default  25.000000        human  25.000000   5.0000   5.0000\n",
       "104  Default  20.000000        posit  20.000000   4.0000   4.0000\n",
       "100  Default  19.000000      confirm  19.000000   3.0000   3.0000\n",
       "73   Default  29.000000        virus  29.000000   2.0000   2.0000\n",
       "19   Default  17.000000         rich  17.000000   1.0000   1.0000\n",
       "101   Topic1  23.492628         face  24.673864   0.9885  -3.8037\n",
       "14    Topic1  21.398134       doctor  22.718824   0.9777  -3.8971\n",
       "0     Topic1  49.775906       health  53.306252   0.9691  -3.0529\n",
       "11    Topic1  24.524338          say  26.674429   0.9535  -3.7608\n",
       "15    Topic1  17.218140       global  18.737322   0.9530  -4.1145\n",
       "107   Topic1  16.257692        cover  17.720444   0.9514  -4.1719\n",
       "50    Topic1  28.759945         time  31.586308   0.9438  -3.6014\n",
       "12    Topic1  23.137199        think  25.559336   0.9380  -3.8190\n",
       "44    Topic1  16.788557         even  18.676283   0.9310  -4.1397\n",
       "98    Topic1  13.433551    recommend  15.781865   0.8765  -4.3627\n",
       "51    Topic1  14.150691       import  16.707415   0.8715  -4.3107\n",
       "70    Topic1  35.537922        fight  42.152561   0.8669  -3.3898\n",
       "72    Topic1  34.734211         make  42.157925   0.8439  -3.4127\n",
       "48    Topic1  22.859636         care  28.549000   0.8153  -3.8310\n",
       "24    Topic1  32.869621         mask  41.514469   0.8041  -3.4679\n",
       "60    Topic1  17.714666        china  22.716934   0.7889  -4.0860\n",
       "58    Topic1  12.146369         life  15.741085   0.7783  -4.4634\n",
       "77    Topic1  17.261377        equip  22.490934   0.7729  -4.1119\n",
       "96    Topic1  19.831367      protect  27.117552   0.7247  -3.9732\n",
       "49    Topic1  21.696056      patient  30.071377   0.7111  -3.8833\n",
       "33    Topic1  11.131491      support  15.485986   0.7074  -4.5506\n",
       "9     Topic1  16.548590        thank  23.384338   0.6918  -4.1541\n",
       "55    Topic1  11.359245       enough  16.549549   0.6613  -4.5304\n",
       "75    Topic1  14.688077       worker  21.427212   0.6600  -4.2734\n",
       "16    Topic1  33.231815       pandem  49.225788   0.6447  -3.4569\n",
       "106   Topic1  21.195635        trump  31.568169   0.6392  -3.9066\n",
       "79    Topic1  10.605099         look  15.834339   0.6367  -4.5991\n",
       "91    Topic1  13.315721         call  20.224052   0.6197  -4.3715\n",
       "76    Topic1  15.935677         live  25.551100   0.5655  -4.1919\n",
       "54    Topic1  12.124201      countri  19.481623   0.5633  -4.4652\n",
       "78    Topic1  22.230759         need  36.271698   0.5480  -3.8589\n",
       "30    Topic1  19.892223        today  34.669540   0.4820  -3.9701\n",
       "6     Topic1  23.575552         help  48.666515   0.3128  -3.8002\n",
       "4     Topic1  17.296143         stay  33.414848   0.3791  -4.1099\n",
       "5     Topic1  16.582310  coronavirus  92.101257  -0.6770  -4.1521\n",
       "22    Topic1  13.911275        peopl  71.587959  -0.6006  -4.3277\n",
       "38    Topic2  37.362686      poverti  38.234020   1.1060  -3.2483\n",
       "39    Topic2  37.323833       racism  38.237198   1.1049  -3.2493\n",
       "41    Topic2  37.318317         trut  38.238003   1.1047  -3.2495\n",
       "35    Topic2  39.036762       father  40.064682   1.1031  -3.2045\n",
       "40    Topic2  41.508179        speak  42.816189   1.0980  -3.1431\n",
       "34    Topic2  39.726917         caus  40.994713   1.0976  -3.1869\n",
       "36    Topic2  37.848625       murder  39.180359   1.0945  -3.2354\n",
       "82    Topic2  13.831756      gullibl  14.602859   1.0748  -4.2420\n",
       "83    Topic2  13.809934       narrat  14.605000   1.0731  -4.2436\n",
       "85    Topic2  13.809591   propaganda  14.604956   1.0730  -4.2436\n",
       "81    Topic2  13.798210         ceas  14.606109   1.0721  -4.2444\n",
       "42    Topic2  52.506233         work  56.704109   1.0521  -2.9080\n",
       "23    Topic2  14.299687        often  15.550771   1.0452  -4.2087\n",
       "37    Topic2  41.252892        natur  45.834698   1.0237  -3.1492\n",
       "100   Topic2  17.292667      confirm  19.253305   1.0216  -4.0187\n",
       "80    Topic2  14.976827         amaz  17.485838   0.9742  -4.1625\n",
       "53    Topic2  60.561043         case  72.821205   0.9447  -2.7653\n",
       "87    Topic2  19.823591        pleas  26.953558   0.8218  -3.8821\n",
       "89    Topic2  12.255836          die  17.740963   0.7592  -4.3630\n",
       "86    Topic2  11.586928         week  17.784567   0.7006  -4.4191\n",
       "47    Topic2  15.823037       report  25.336792   0.6583  -4.1075\n",
       "31    Topic2  11.530588        medic  18.812117   0.6395  -4.4240\n",
       "92    Topic2  12.258372        break  20.698896   0.6052  -4.3627\n",
       "84    Topic2  14.373879        never  24.447596   0.5979  -4.2035\n",
       "74    Topic2   8.821339        great  16.062229   0.5297  -4.6918\n",
       "93    Topic2  10.235735        total  18.954239   0.5129  -4.5431\n",
       "5     Topic2  46.671162  coronavirus  92.101257   0.4493  -3.0258\n",
       "17    Topic2  17.550144       public  35.093479   0.4361  -4.0039\n",
       "95    Topic2   7.830441         mani  16.143515   0.4055  -4.8109\n",
       "10    Topic2  12.271860       spread  25.709785   0.3895  -4.3616\n",
       "52    Topic2  12.235337         take  35.644386   0.0598  -4.3646\n",
       "2     Topic2   9.765714        order  32.974289  -0.0878  -4.5901\n",
       "19    Topic3  16.047003         rich  17.843575   1.0260  -4.0904\n",
       "104   Topic3  18.660583        posit  20.818819   1.0227  -3.9395\n",
       "64    Topic3  14.079082         cast  15.847042   1.0138  -4.2212\n",
       "69    Topic3  14.066035     religion  15.845262   1.0130  -4.2221\n",
       "45    Topic3  15.775548         isol  17.822798   1.0101  -4.1074\n",
       "63    Topic3  13.996200         basi  15.839362   1.0084  -4.2271\n",
       "13    Topic3  45.184025         test  51.757416   0.9963  -3.0551\n",
       "43    Topic3  22.323460       govern  25.790236   0.9878  -3.7602\n",
       "66    Topic3  16.250177       either  18.804361   0.9861  -4.0778\n",
       "27    Topic3  17.723028     lockdown  20.820402   0.9711  -3.9910\n",
       "3     Topic3  24.383379        state  28.835831   0.9644  -3.6720\n",
       "18    Topic3  21.710306        human  25.838713   0.9580  -3.7881\n",
       "99    Topic3  12.508401     governor  15.884512   0.8932  -4.3395\n",
       "22    Topic3  55.459259        peopl  71.587959   0.8768  -2.8502\n",
       "73    Topic3  22.409683        virus  29.829565   0.8461  -3.7564\n",
       "90    Topic3  13.353559          ask  17.819656   0.8436  -4.2741\n",
       "67    Topic3  23.382103      everyon  31.451185   0.8357  -3.7139\n",
       "65    Topic3  14.461570    discrimin  19.801117   0.8179  -4.1944\n",
       "68    Topic3  16.391636         poor  22.740755   0.8047  -4.0691\n",
       "88    Topic3  11.211126       corona  15.640184   0.7992  -4.4490\n",
       "29    Topic3  25.363220         like  37.791744   0.7333  -3.6326\n",
       "28    Topic3  14.294690       nation  21.672588   0.7160  -4.2060\n",
       "57    Topic3  18.104546       affect  28.451157   0.6801  -3.9697\n",
       "2     Topic3  20.573263        order  32.974289   0.6604  -3.8419\n",
       "32    Topic3  14.824333         want  23.851576   0.6565  -4.1696\n",
       "25    Topic3  13.472905         send  21.858028   0.6482  -4.2652\n",
       "8     Topic3  12.649776         nurs  20.702364   0.6395  -4.3282\n",
       "7     Topic3  24.093639         keep  39.544403   0.6367  -3.6839\n",
       "1     Topic3  22.823477         home  37.619656   0.6324  -3.7381\n",
       "46    Topic3  22.311689        death  37.354973   0.6168  -3.7608\n",
       "6     Topic3  24.497444         help  48.666515   0.4457  -3.6673\n",
       "5     Topic3  28.847788  coronavirus  92.101257  -0.0287  -3.5038\n",
       "21    Topic3  15.386725       hospit  28.456543   0.5173  -4.1324\n",
       "26    Topic3  13.523922        april  23.106903   0.5965  -4.2614\n",
       "30    Topic3  13.743765        today  34.669540   0.2069  -4.2453, token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "57        1  0.210888       affect\n",
       "57        2  0.175740       affect\n",
       "57        3  0.632663       affect\n",
       "80        2  0.857837         amaz\n",
       "80        3  0.114378         amaz\n",
       "26        1  0.043277        april\n",
       "26        2  0.389494        april\n",
       "26        3  0.605880        april\n",
       "90        1  0.168353          ask\n",
       "90        2  0.056118          ask\n",
       "90        3  0.729531          ask\n",
       "63        2  0.063134         basi\n",
       "63        3  0.883874         basi\n",
       "92        1  0.289871        break\n",
       "92        2  0.579741        break\n",
       "92        3  0.096624        break\n",
       "91        1  0.642799         call\n",
       "91        2  0.296676         call\n",
       "91        3  0.049446         call\n",
       "48        1  0.805632         care\n",
       "48        2  0.070055         care\n",
       "48        3  0.140110         care\n",
       "53        1  0.096126         case\n",
       "53        2  0.837668         case\n",
       "53        3  0.068661         case\n",
       "64        2  0.063103         cast\n",
       "64        3  0.883446         cast\n",
       "34        1  0.024393         caus\n",
       "34        2  0.975736         caus\n",
       "34        3  0.024393         caus\n",
       "81        2  0.958503         ceas\n",
       "60        1  0.792360        china\n",
       "60        2  0.044020        china\n",
       "60        3  0.176080        china\n",
       "100       2  0.882965      confirm\n",
       "100       3  0.103878      confirm\n",
       "88        1  0.063938       corona\n",
       "88        2  0.191814       corona\n",
       "88        3  0.703317       corona\n",
       "5         1  0.184579  coronavirus\n",
       "5         2  0.510308  coronavirus\n",
       "5         3  0.314871  coronavirus\n",
       "54        1  0.615965      countri\n",
       "54        2  0.205322      countri\n",
       "54        3  0.205322      countri\n",
       "107       1  0.902912        cover\n",
       "107       2  0.056432        cover\n",
       "46        1  0.267702        death\n",
       "46        2  0.133851        death\n",
       "46        3  0.588944        death\n",
       "89        1  0.112733          die\n",
       "89        2  0.676401          die\n",
       "89        3  0.169100          die\n",
       "65        1  0.202009    discrimin\n",
       "65        2  0.050502    discrimin\n",
       "65        3  0.707031    discrimin\n",
       "14        1  0.924344       doctor\n",
       "14        3  0.044016       doctor\n",
       "66        1  0.053179       either\n",
       "66        2  0.106358       either\n",
       "66        3  0.850866       either\n",
       "55        1  0.664671       enough\n",
       "55        2  0.181274       enough\n",
       "55        3  0.120849       enough\n",
       "77        1  0.755860        equip\n",
       "77        2  0.133387        equip\n",
       "77        3  0.088925        equip\n",
       "44        1  0.910245         even\n",
       "44        2  0.053544         even\n",
       "67        1  0.095386      everyon\n",
       "67        2  0.158977      everyon\n",
       "67        3  0.731292      everyon\n",
       "101       1  0.932160         face\n",
       "101       2  0.040529         face\n",
       "35        2  0.973426       father\n",
       "35        3  0.024960       father\n",
       "70        1  0.854041        fight\n",
       "70        2  0.118617        fight\n",
       "70        3  0.047447        fight\n",
       "15        1  0.907280       global\n",
       "15        2  0.053369       global\n",
       "15        3  0.053369       global\n",
       "43        1  0.077549       govern\n",
       "43        2  0.077549       govern\n",
       "43        3  0.853036       govern\n",
       "99        1  0.188863     governor\n",
       "99        2  0.062954     governor\n",
       "99        3  0.818407     governor\n",
       "74        1  0.249031        great\n",
       "74        2  0.560321        great\n",
       "74        3  0.186774        great\n",
       "82        2  0.958716      gullibl\n",
       "0         1  0.937976       health\n",
       "0         2  0.018760       health\n",
       "0         3  0.037519       health\n",
       "6         1  0.493152         help\n",
       "6         2  0.020548         help\n",
       "6         3  0.493152         help\n",
       "1         1  0.345564         home\n",
       "1         2  0.053164         home\n",
       "1         3  0.611383         home\n",
       "21        1  0.316272       hospit\n",
       "21        2  0.140565       hospit\n",
       "21        3  0.527120       hospit\n",
       "18        1  0.116105        human\n",
       "18        2  0.038702        human\n",
       "18        3  0.851436        human\n",
       "51        1  0.837951       import\n",
       "51        2  0.059854       import\n",
       "51        3  0.059854       import\n",
       "45        2  0.112216         isol\n",
       "45        3  0.897727         isol\n",
       "7         1  0.303456         keep\n",
       "7         2  0.075864         keep\n",
       "7         3  0.606913         keep\n",
       "58        1  0.762336         life\n",
       "58        2  0.063528         life\n",
       "58        3  0.127056         life\n",
       "29        1  0.317530         like\n",
       "29        2  0.026461         like\n",
       "29        3  0.661520         like\n",
       "76        1  0.626196         live\n",
       "76        2  0.117412         live\n",
       "76        3  0.273961         live\n",
       "27        1  0.096060     lockdown\n",
       "27        2  0.048030     lockdown\n",
       "27        3  0.864537     lockdown\n",
       "79        1  0.694693         look\n",
       "79        3  0.315769         look\n",
       "72        1  0.830212         make\n",
       "72        2  0.118602         make\n",
       "72        3  0.071161         make\n",
       "95        1  0.371666         mani\n",
       "95        2  0.495555         mani\n",
       "95        3  0.123889         mani\n",
       "24        1  0.794904         mask\n",
       "24        2  0.024088         mask\n",
       "24        3  0.168616         mask\n",
       "31        1  0.106314        medic\n",
       "31        2  0.637887        medic\n",
       "31        3  0.265786        medic\n",
       "36        1  0.025523       murder\n",
       "36        2  0.969874       murder\n",
       "83        2  0.958576       narrat\n",
       "28        1  0.230706       nation\n",
       "28        2  0.138424       nation\n",
       "28        3  0.645977       nation\n",
       "37        1  0.021818        natur\n",
       "37        2  0.894519        natur\n",
       "37        3  0.087270        natur\n",
       "78        1  0.606534         need\n",
       "78        2  0.137849         need\n",
       "78        3  0.248127         need\n",
       "84        1  0.409038        never\n",
       "84        2  0.572653        never\n",
       "8         1  0.289822         nurs\n",
       "8         2  0.096607         nurs\n",
       "8         3  0.627948         nurs\n",
       "23        1  0.064305        often\n",
       "23        2  0.900277        often\n",
       "2         1  0.090980        order\n",
       "2         2  0.303267        order\n",
       "2         3  0.636860        order\n",
       "16        1  0.670380       pandem\n",
       "16        2  0.081258       pandem\n",
       "16        3  0.243775       pandem\n",
       "49        1  0.731593      patient\n",
       "49        2  0.232780      patient\n",
       "49        3  0.066508      patient\n",
       "22        1  0.195564        peopl\n",
       "22        2  0.027938        peopl\n",
       "22        3  0.768286        peopl\n",
       "87        1  0.148403        pleas\n",
       "87        2  0.742017        pleas\n",
       "87        3  0.111303        pleas\n",
       "68        1  0.175896         poor\n",
       "68        2  0.087948         poor\n",
       "68        3  0.703583         poor\n",
       "104       1  0.048033        posit\n",
       "104       2  0.096067        posit\n",
       "104       3  0.912636        posit\n",
       "38        2  0.967725      poverti\n",
       "85        2  0.958579   propaganda\n",
       "96        1  0.737530      protect\n",
       "96        2  0.258135      protect\n",
       "96        3  0.036876      protect\n",
       "17        1  0.341944       public\n",
       "17        2  0.512916       public\n",
       "17        3  0.142477       public\n",
       "39        2  0.967644       racism\n",
       "98        1  0.823730    recommend\n",
       "98        2  0.063364    recommend\n",
       "98        3  0.126728    recommend\n",
       "69        2  0.063110     religion\n",
       "69        3  0.883545     religion\n",
       "47        1  0.236810       report\n",
       "47        2  0.631493       report\n",
       "47        3  0.118405       report\n",
       "19        2  0.056043         rich\n",
       "19        3  0.896681         rich\n",
       "11        1  0.937227          say\n",
       "11        2  0.037489          say\n",
       "11        3  0.074978          say\n",
       "25        1  0.365998         send\n",
       "25        3  0.594747         send\n",
       "40        2  0.980937        speak\n",
       "40        3  0.023356        speak\n",
       "10        1  0.155583       spread\n",
       "10        2  0.466748       spread\n",
       "10        3  0.350061       spread\n",
       "3         1  0.104037        state\n",
       "3         2  0.034679        state\n",
       "3         3  0.832298        state\n",
       "4         1  0.508756         stay\n",
       "4         2  0.119707         stay\n",
       "4         3  0.359122         stay\n",
       "33        1  0.710320      support\n",
       "33        2  0.258298      support\n",
       "52        1  0.308604         take\n",
       "52        2  0.336659         take\n",
       "52        3  0.364714         take\n",
       "13        1  0.096605         test\n",
       "13        2  0.038642         test\n",
       "13        3  0.869441         test\n",
       "9         1  0.726982        thank\n",
       "9         2  0.171055        thank\n",
       "9         3  0.128291        thank\n",
       "12        1  0.899867        think\n",
       "12        2  0.078249        think\n",
       "12        3  0.039125        think\n",
       "50        1  0.918119         time\n",
       "50        2  0.031659         time\n",
       "50        3  0.063319         time\n",
       "30        1  0.576875        today\n",
       "30        2  0.028844        today\n",
       "30        3  0.403813        today\n",
       "93        1  0.052759        total\n",
       "93        2  0.527586        total\n",
       "93        3  0.422069        total\n",
       "106       1  0.665227        trump\n",
       "106       2  0.063355        trump\n",
       "106       3  0.253420        trump\n",
       "41        2  0.967624         trut\n",
       "73        1  0.234667        virus\n",
       "73        2  0.033524        virus\n",
       "73        3  0.737523        virus\n",
       "32        1  0.377334         want\n",
       "32        3  0.628889         want\n",
       "86        1  0.224914         week\n",
       "86        2  0.674742         week\n",
       "86        3  0.112457         week\n",
       "42        1  0.017635         work\n",
       "42        2  0.934677         work\n",
       "42        3  0.052906         work\n",
       "75        1  0.700044       worker\n",
       "75        2  0.186679       worker\n",
       "75        3  0.140009       worker, R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 3, 2])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_bow, bow_corpus, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning:\n",
      "\n",
      "Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el6057352072720166019108443\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el6057352072720166019108443_data = {\"mdsDat\": {\"x\": [-0.049806861827233706, -0.13327722664995956, 0.19636622741977253, -0.013282138942579189], \"y\": [-0.13528861296944877, 0.13645983639179918, 0.05453479622373758, -0.05570601964608797], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [26.186216354370117, 25.618833541870117, 25.289398193359375, 22.905553817749023]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"Freq\": [17.0, 32.0, 25.0, 22.0, 16.0, 16.0, 16.0, 14.0, 22.0, 13.0, 42.0, 14.0, 13.0, 13.0, 13.0, 14.0, 16.0, 17.0, 12.0, 10.0, 15.0, 20.0, 10.0, 21.0, 16.0, 10.0, 12.0, 34.0, 12.0, 18.0, 12.990092277526855, 9.573904991149902, 9.143170356750488, 25.4287166595459, 17.908781051635742, 13.019506454467773, 10.592991828918457, 11.430940628051758, 15.741132736206055, 10.257074356079102, 10.31059455871582, 8.9467191696167, 5.985229969024658, 6.15994119644165, 12.747252464294434, 6.090665817260742, 9.65708065032959, 6.9428911209106445, 4.8899359703063965, 4.4961395263671875, 6.058694362640381, 5.682465553283691, 6.32789945602417, 9.610342025756836, 5.170140743255615, 9.344561576843262, 5.212822914123535, 4.05602502822876, 8.198144912719727, 5.368814468383789, 5.294270038604736, 4.828848838806152, 4.954604625701904, 9.515604019165039, 5.773789882659912, 5.179494857788086, 7.457620620727539, 5.171950817108154, 5.170471668243408, 5.169782638549805, 4.7559494972229, 4.755175590515137, 4.754245758056641, 6.133111000061035, 13.570334434509277, 9.445578575134277, 5.265066146850586, 7.7078094482421875, 5.377323627471924, 10.327668190002441, 8.085735321044922, 7.826612949371338, 12.706945419311523, 25.692548751831055, 20.9005184173584, 14.39901351928711, 6.164312839508057, 4.538717269897461, 9.81539249420166, 9.404865264892578, 6.287120819091797, 7.9399800300598145, 6.10223388671875, 6.065997123718262, 7.698523044586182, 5.498818874359131, 4.806946754455566, 12.71963882446289, 14.70595932006836, 12.356256484985352, 13.369182586669922, 12.337529182434082, 12.332863807678223, 14.646099090576172, 12.631072044372559, 18.59996795654297, 10.129411697387695, 12.157594680786133, 10.938882827758789, 8.285902976989746, 8.319101333618164, 6.977906703948975, 10.788568496704102, 14.767958641052246, 8.265266418457031, 7.396603584289551, 9.386739730834961, 5.19306755065918, 5.761498928070068, 6.400989055633545, 3.9121038913726807, 8.912276268005371, 7.841240882873535, 3.626519203186035, 3.1097614765167236, 6.832930564880371, 3.2531888484954834, 6.598042011260986, 3.9227280616760254, 4.985284805297852, 5.070840358734131, 16.188949584960938, 9.42092227935791, 14.494112968444824, 10.719801902770996, 9.704567909240723, 13.453404426574707, 8.764294624328613, 18.661211013793945, 10.092358589172363, 9.636802673339844, 11.585970878601074, 6.010853290557861, 7.462040901184082, 13.002481460571289, 6.682569980621338, 5.147390365600586, 5.5067362785339355, 4.949060916900635, 3.2666287422180176, 3.654554605484009, 5.799327850341797, 3.673856019973755, 3.6911556720733643, 3.185760021209717, 2.50480055809021, 3.656008243560791, 6.3451032638549805, 5.52982234954834, 3.722411632537842, 2.5002431869506836, 3.6024234294891357, 9.457815170288086, 4.646491050720215, 4.057280540466309, 4.074334621429443, 4.182441711425781, 3.965986967086792, 3.7197506427764893], \"Term\": [\"patient\", \"case\", \"pandem\", \"work\", \"thank\", \"natur\", \"stay\", \"caus\", \"help\", \"murder\", \"coronavirus\", \"want\", \"poverti\", \"trut\", \"racism\", \"father\", \"state\", \"spread\", \"doctor\", \"global\", \"speak\", \"need\", \"look\", \"mask\", \"pleas\", \"great\", \"week\", \"peopl\", \"lockdown\", \"time\", \"want\", \"great\", \"confirm\", \"case\", \"help\", \"pleas\", \"protect\", \"hospit\", \"mask\", \"report\", \"send\", \"break\", \"watch\", \"polic\", \"need\", \"releas\", \"govern\", \"equip\", \"cover\", \"recommend\", \"face\", \"affect\", \"nurs\", \"home\", \"total\", \"death\", \"ask\", \"life\", \"take\", \"everyon\", \"care\", \"april\", \"peopl\", \"look\", \"often\", \"narrat\", \"poor\", \"ceas\", \"gullibl\", \"propaganda\", \"basi\", \"religion\", \"cast\", \"either\", \"state\", \"posit\", \"rich\", \"corona\", \"amaz\", \"medic\", \"must\", \"crisi\", \"like\", \"coronavirus\", \"peopl\", \"test\", \"never\", \"discrimin\", \"today\", \"keep\", \"human\", \"public\", \"first\", \"everyon\", \"fight\", \"say\", \"governor\", \"murder\", \"natur\", \"poverti\", \"caus\", \"trut\", \"racism\", \"stay\", \"father\", \"work\", \"lockdown\", \"speak\", \"order\", \"call\", \"presid\", \"mani\", \"trump\", \"make\", \"virus\", \"china\", \"say\", \"isol\", \"right\", \"april\", \"enough\", \"take\", \"home\", \"india\", \"updat\", \"fight\", \"polic\", \"health\", \"nation\", \"pandem\", \"peopl\", \"patient\", \"global\", \"thank\", \"doctor\", \"week\", \"spread\", \"countri\", \"pandem\", \"live\", \"die\", \"time\", \"import\", \"think\", \"health\", \"world\", \"ask\", \"worker\", \"respons\", \"enough\", \"support\", \"care\", \"india\", \"even\", \"life\", \"discrimin\", \"face\", \"need\", \"death\", \"presid\", \"cover\", \"call\", \"coronavirus\", \"keep\", \"public\", \"fight\", \"make\", \"test\", \"case\"], \"Total\": [17.0, 32.0, 25.0, 22.0, 16.0, 16.0, 16.0, 14.0, 22.0, 13.0, 42.0, 14.0, 13.0, 13.0, 13.0, 14.0, 16.0, 17.0, 12.0, 10.0, 15.0, 20.0, 10.0, 21.0, 16.0, 10.0, 12.0, 34.0, 12.0, 18.0, 14.249693870544434, 10.619604110717773, 10.980448722839355, 32.428932189941406, 22.871105194091797, 16.944826126098633, 14.633835792541504, 15.871009826660156, 21.99346160888672, 14.338033676147461, 14.487005233764648, 13.913244247436523, 9.310346603393555, 9.947659492492676, 20.68449592590332, 9.944502830505371, 16.37038803100586, 12.092203140258789, 8.777168273925781, 8.34490966796875, 11.516432762145996, 11.174652099609375, 12.516233444213867, 19.086057662963867, 10.30947208404541, 18.742406845092773, 11.53634262084961, 9.559053421020508, 19.381017684936523, 12.730202674865723, 14.894118309020996, 12.808716773986816, 34.51250457763672, 10.606171607971191, 6.52263069152832, 5.925328254699707, 8.535865783691406, 5.926027774810791, 5.926142692565918, 5.925743579864502, 5.6110405921936035, 5.6113600730896, 5.611476421356201, 7.244779109954834, 16.036218643188477, 11.632424354553223, 6.588737487792969, 9.900166511535645, 7.032038688659668, 13.524006843566895, 10.846028327941895, 11.610549926757812, 18.973451614379883, 42.374549865722656, 34.51250457763672, 24.16334342956543, 10.548358917236328, 7.882490158081055, 17.615327835083008, 17.672130584716797, 11.868559837341309, 15.230979919433594, 12.424325942993164, 12.730202674865723, 20.853187561035156, 16.479660034179688, 10.177044868469238, 13.89388370513916, 16.085615158081055, 13.52796745300293, 14.651766777038574, 13.528528213500977, 13.529094696044922, 16.56447982788086, 14.386717796325684, 22.37957000732422, 12.334859848022461, 15.152270317077637, 15.778488159179688, 12.755517959594727, 12.807735443115234, 10.79702091217041, 17.229568481445312, 24.527685165405273, 13.782505989074707, 12.723695755004883, 16.479660034179688, 9.567211151123047, 11.501346588134766, 12.808716773986816, 8.27070426940918, 19.381017684936523, 19.086057662963867, 10.378890991210938, 9.22684383392334, 20.853187561035156, 9.947659492492676, 24.276870727539062, 12.623188018798828, 25.757640838623047, 34.51250457763672, 17.800676345825195, 10.423572540283203, 16.601572036743164, 12.791440963745117, 12.507408142089844, 17.70024871826172, 11.704838752746582, 25.757640838623047, 14.61552619934082, 14.22736930847168, 18.11443328857422, 10.692855834960938, 13.529633522033691, 24.276870727539062, 12.756983757019043, 11.53634262084961, 13.192497253417969, 12.064740180969238, 8.27070426940918, 9.354101181030273, 14.894118309020996, 10.378890991210938, 11.056838035583496, 9.559053421020508, 7.882490158081055, 11.516432762145996, 20.68449592590332, 18.742406845092773, 12.807735443115234, 8.777168273925781, 12.755517959594727, 42.374549865722656, 17.672130584716797, 15.230979919433594, 20.853187561035156, 24.527685165405273, 24.16334342956543, 32.428932189941406], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2474000453948975, 1.236299991607666, 1.1568000316619873, 1.0967999696731567, 1.0953999757766724, 1.0764000415802002, 1.016800045967102, 1.0118000507354736, 1.0054999589920044, 1.0049999952316284, 0.9998999834060669, 0.8984000086784363, 0.8981000185012817, 0.8607000112533569, 0.85589998960495, 0.8496999740600586, 0.8122000098228455, 0.785099983215332, 0.7549999952316284, 0.7214999794960022, 0.697700023651123, 0.6636999845504761, 0.6578999757766724, 0.6538000106811523, 0.6498000025749207, 0.6438999772071838, 0.5455999970436096, 0.482699990272522, 0.4796000123023987, 0.4765999913215637, 0.30559998750686646, 0.3643999993801117, -0.6011000275611877, 1.2532999515533447, 1.23989999294281, 1.2273000478744507, 1.2267999649047852, 1.2257000207901, 1.2253999710083008, 1.2253999710083008, 1.1964999437332153, 1.1963000297546387, 1.1960999965667725, 1.1952999830245972, 1.1949000358581543, 1.1535999774932861, 1.1375999450683594, 1.1115000247955322, 1.0936000347137451, 1.0922000408172607, 1.0680999755859375, 0.9674999713897705, 0.9610000252723694, 0.8615000247955322, 0.8603000044822693, 0.8442000150680542, 0.8245999813079834, 0.8098000288009644, 0.7770000100135803, 0.7311000227928162, 0.7264999747276306, 0.7103999853134155, 0.6507999897003174, 0.6205999851226807, 0.3653999865055084, 0.26420000195503235, 0.6118000149726868, 1.2864999771118164, 1.285099983215332, 1.2841999530792236, 1.2832000255584717, 1.2826000452041626, 1.2821999788284302, 1.2517000436782837, 1.2446000576019287, 1.1898000240325928, 1.1778000593185425, 1.1546000242233276, 1.0084999799728394, 0.9434000253677368, 0.9433000087738037, 0.9383000135421753, 0.9065999984741211, 0.8673999905586243, 0.8633999824523926, 0.8323000073432922, 0.8119999766349792, 0.7638000249862671, 0.6834999918937683, 0.6811000108718872, 0.6261000037193298, 0.5978999733924866, 0.4851999878883362, 0.32330000400543213, 0.287200003862381, 0.2590000033378601, 0.257099986076355, 0.07199999690055847, 0.20600000023841858, -0.26750001311302185, -0.5429999828338623, 1.3789000511169434, 1.3726999759674072, 1.3380000591278076, 1.2970999479293823, 1.2201000452041626, 1.1993999481201172, 1.184499979019165, 1.1514999866485596, 1.1035000085830688, 1.0842000246047974, 1.026900053024292, 0.8978000283241272, 0.8787000179290771, 0.849399983882904, 0.8271999955177307, 0.6668000221252441, 0.6000999808311462, 0.5827000141143799, 0.5447999835014343, 0.5339999794960022, 0.5306000113487244, 0.43529999256134033, 0.3767000138759613, 0.375, 0.32739999890327454, 0.3264000117778778, 0.2921000123023987, 0.2531999945640564, 0.23810000717639923, 0.21799999475479126, 0.2093999981880188, -0.02590000070631504, 0.1378999948501587, 0.1509999930858612, -0.1589999943971634, -0.29510000348091125, -0.33329999446868896, -0.6916000247001648], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.416800022125244, -3.7219998836517334, -3.7679998874664307, -2.7451000213623047, -3.0957000255584717, -3.414599895477295, -3.620800018310547, -3.5446999073028564, -3.2246999740600586, -3.6530001163482666, -3.6477999687194824, -3.7897000312805176, -4.191699981689453, -4.162899971008301, -3.435699939727783, -4.174200057983398, -3.7132999897003174, -4.043300151824951, -4.393799781799316, -4.477799892425537, -4.179500102996826, -4.243599891662598, -4.136000156402588, -3.7181999683380127, -4.338099956512451, -3.7462000846862793, -4.329899787902832, -4.5808000564575195, -3.8770999908447266, -4.3003997802734375, -4.3144001960754395, -4.406400203704834, -4.38070011138916, -3.706199884414673, -4.2058000564575195, -4.3144001960754395, -3.949899911880493, -4.315800189971924, -4.316100120544434, -4.316299915313721, -4.399700164794922, -4.399899959564209, -4.400100231170654, -4.145400047302246, -3.3512001037597656, -3.713599920272827, -4.297999858856201, -3.9168999195098877, -4.276899814605713, -3.624300003051758, -3.86899995803833, -3.901599884033203, -3.4168999195098877, -2.712899923324585, -2.919300079345703, -3.2918999195098877, -4.1402997970581055, -4.446499824523926, -3.675100088119507, -3.717900037765503, -4.12060022354126, -3.887200117111206, -4.150400161743164, -4.156400203704834, -3.918100118637085, -4.2546000480651855, -4.388999938964844, -3.4030001163482666, -3.2578999996185303, -3.431999921798706, -3.3531999588012695, -3.433500051498413, -3.4339001178741455, -3.26200008392334, -3.4100000858306885, -3.0230000019073486, -3.63070011138916, -3.448199987411499, -3.553800106048584, -3.8315999507904053, -3.8276000022888184, -4.003399848937988, -3.567699909210205, -3.253700017929077, -3.8341000080108643, -3.9451000690460205, -3.706899881362915, -4.298799991607666, -4.195000171661377, -4.089700222015381, -4.582099914550781, -3.758699893951416, -3.8868000507354736, -4.657899856567383, -4.811600208282471, -4.024400234222412, -4.766499996185303, -4.0594000816345215, -4.579400062561035, -4.339700222015381, -4.3225998878479, -3.0627999305725098, -3.6041998863220215, -3.1733999252319336, -3.475100040435791, -3.5745999813079834, -3.2479000091552734, -3.676500082015991, -2.9207000732421875, -3.535399913787842, -3.5815999507904053, -3.39739990234375, -4.053599834442139, -3.8373000621795654, -3.2820000648498535, -3.9475998878479004, -4.208700180053711, -4.141200065612793, -4.248000144958496, -4.663400173187256, -4.551199913024902, -4.089399814605713, -4.545899868011475, -4.541200160980225, -4.688499927520752, -4.928899765014648, -4.55079984664917, -3.999500036239624, -4.13700008392334, -4.532800197601318, -4.930799961090088, -4.565499782562256, -3.6003000736236572, -4.310999870300293, -4.446599960327148, -4.442399978637695, -4.416299819946289, -4.469399929046631, -4.5335001945495605]}, \"token.table\": {\"Topic\": [1, 2, 3, 2, 3, 1, 2, 3, 4, 1, 3, 4, 2, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 2, 2, 3, 4, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 1, 2, 4, 1, 2, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 4, 2, 3, 4, 2, 1, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 4, 1, 2, 4, 1, 2, 3, 4, 1, 3, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 2, 3, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 2, 3, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 2, 3, 4, 2, 1, 2, 3, 4, 3, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 4, 2, 1, 2, 3, 4, 1, 3, 4, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 3, 2, 2, 3, 4, 3, 4, 1, 3, 4, 2, 1, 2, 3, 4, 1, 2, 4, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 1, 3, 4, 1, 2, 4, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 1, 2, 3, 1, 3, 4, 1, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4], \"Freq\": [0.5369294881820679, 0.3579529821872711, 0.08948824554681778, 0.7110313773155212, 0.14220626652240753, 0.39035916328430176, 0.07807183265686035, 0.4684309959411621, 0.07807183265686035, 0.43341293931007385, 0.08668258786201477, 0.43341293931007385, 0.891100287437439, 0.6468656659126282, 0.14374792575836182, 0.14374792575836182, 0.14374792575836182, 0.07839744538068771, 0.6271795630455017, 0.31358978152275085, 0.3357029855251312, 0.067140594124794, 0.2014217972755432, 0.4028435945510864, 0.7709165215492249, 0.061673324555158615, 0.030836662277579308, 0.12334664911031723, 0.8910310864448547, 0.8872650265693665, 0.06825115531682968, 0.8437355160713196, 0.23578055202960968, 0.5501546263694763, 0.15718702971935272, 0.8196386098861694, 0.09107095748186111, 0.09107095748186111, 0.09107095748186111, 0.20201680064201355, 0.8080672025680542, 0.09439628571271896, 0.61357581615448, 0.09439628571271896, 0.21239162981510162, 0.17086949944496155, 0.08543474972248077, 0.7689127922058105, 0.5696598291397095, 0.11393196135759354, 0.3417958915233612, 0.17225712537765503, 0.6890285015106201, 0.17225712537765503, 0.48019444942474365, 0.16006481647491455, 0.05335494130849838, 0.3201296329498291, 0.2108611911535263, 0.0702870637178421, 0.0702870637178421, 0.7028706073760986, 0.6343173384666443, 0.38059037923812866, 0.07817727327346802, 0.07817727327346802, 0.8599500060081482, 0.828182578086853, 0.12090868502855301, 0.48363474011421204, 0.3627260625362396, 0.5788854360580444, 0.08269791305065155, 0.24809375405311584, 0.18088354170322418, 0.36176708340644836, 0.09044177085161209, 0.36176708340644836, 0.39276671409606934, 0.4713200628757477, 0.07855334132909775, 0.5209946632385254, 0.1736648827791214, 0.3473297655582428, 0.06950856000185013, 0.9036112427711487, 0.06950856000185013, 0.09590859711170197, 0.38363438844680786, 0.3356800973415375, 0.19181719422340393, 0.16097453236579895, 0.48292359709739685, 0.08048726618289948, 0.24146179854869843, 0.8634275794029236, 0.6108590960502625, 0.3054295480251312, 0.1221718117594719, 0.29478105902671814, 0.4913017451763153, 0.19652070105075836, 0.09826035052537918, 0.9416546821594238, 0.09416546672582626, 0.8437191247940063, 0.1647658795118332, 0.0411914698779583, 0.288340300321579, 0.5354891419410706, 0.7870192527770996, 0.04372329264879227, 0.04372329264879227, 0.1311698704957962, 0.5239426493644714, 0.05239426717162132, 0.4191541373729706, 0.05239426717162132, 0.6930875778198242, 0.06300796568393707, 0.18902388215065002, 0.08425622433423996, 0.5055373311042786, 0.16851244866847992, 0.2527686655521393, 0.374081552028656, 0.093520388007164, 0.5611222982406616, 0.19269882142543793, 0.09634941071271896, 0.38539764285087585, 0.38539764285087585, 0.418094664812088, 0.5226183533668518, 0.16975881159305573, 0.5092764496803284, 0.05658627301454544, 0.2829313576221466, 0.4184514880180359, 0.20922574400901794, 0.3138386011123657, 0.1581156700849533, 0.6851679086685181, 0.052705224603414536, 0.10541044920682907, 0.06842038780450821, 0.06842038780450821, 0.20526117086410522, 0.6842039227485657, 0.08107104897499084, 0.8107104897499084, 0.08107104897499084, 0.9428472518920898, 0.09428472816944122, 0.12231076508760452, 0.08154051005840302, 0.6115538477897644, 0.16308102011680603, 0.09261813759803772, 0.6483269929885864, 0.18523627519607544, 0.7274889349937439, 0.13640417158603668, 0.045468058437108994, 0.09093611687421799, 0.07394258677959442, 0.7394258379936218, 0.07394258677959442, 0.07394258677959442, 0.9356635212898254, 0.07197411358356476, 0.7375971674919128, 0.1843992918729782, 0.0921996459364891, 0.8438351154327393, 0.39609643816947937, 0.158438578248024, 0.316877156496048, 0.079219289124012, 0.9325101971626282, 0.062167346477508545, 0.6284900307655334, 0.04834539070725441, 0.29007232189178467, 0.09480147808790207, 0.5688088536262512, 0.18960295617580414, 0.18960295617580414, 0.4793774485588074, 0.1597924828529358, 0.2396887242794037, 0.9198742508888245, 0.06337742507457733, 0.1901322901248932, 0.6971517205238342, 0.06337742507457733, 0.07764685899019241, 0.19411715865135193, 0.7376452088356018, 0.056177642196416855, 0.8988422751426697, 0.14487501978874207, 0.6084750890731812, 0.14487501978874207, 0.11590001732110977, 0.7671958208084106, 0.05901506543159485, 0.1180301308631897, 0.6031569838523865, 0.30157849192619324, 0.8200691342353821, 0.7736994028091431, 0.08596660196781158, 0.08596660196781158, 0.8870512247085571, 0.07392093539237976, 0.07807781547307968, 0.6246225237846375, 0.3123112618923187, 0.8437759876251221, 0.7516826391220093, 0.06833478063344955, 0.1366695612668991, 0.1366695612668991, 0.1969669759273529, 0.5252452492713928, 0.2626226246356964, 0.8869773149490356, 0.0739147737622261, 0.47933411598205566, 0.23966705799102783, 0.11983352899551392, 0.11983352899551392, 0.603348433971405, 0.1005580723285675, 0.1005580723285675, 0.201116144657135, 0.8910495638847351, 0.6974456906318665, 0.20923371613025665, 0.06974457204341888, 0.3315446376800537, 0.24865847826004028, 0.41443079710006714, 0.15177413821220398, 0.7588707208633423, 0.2608390152454376, 0.08694633841514587, 0.5216780304908752, 0.17389267683029175, 0.06068086251616478, 0.3034043312072754, 0.5461277961730957, 0.06068086251616478, 0.6902737617492676, 0.207082137465477, 0.069027379155159, 0.06599671393632889, 0.7919605374336243, 0.13199342787265778, 0.11299276351928711, 0.056496381759643555, 0.056496381759643555, 0.7344529628753662, 0.06235884130001068, 0.8730237483978271, 0.06235884130001068, 0.06235884130001068, 0.06037014350295067, 0.06037014350295067, 0.9055521488189697, 0.32071495056152344, 0.21380996704101562, 0.10690498352050781, 0.42761993408203125, 0.4127750098705292, 0.1031937524676323, 0.4643718898296356, 0.08277000486850739, 0.5793899893760681, 0.16554000973701477, 0.16554000973701477, 0.06023525819182396, 0.06023525819182396, 0.8432936072349548, 0.07391183078289032, 0.2956473231315613, 0.07391183078289032, 0.517382800579071, 0.0552045963704586, 0.1104091927409172, 0.1656137853860855, 0.662455141544342, 0.22707496583461761, 0.5676873922348022, 0.056768741458654404, 0.1703062206506729, 0.4849908947944641, 0.2909945249557495, 0.0969981774687767, 0.0969981774687767, 0.11607951670885086, 0.1741192787885666, 0.6384373307228088, 0.11607951670885086, 0.8870144486427307, 0.07391787320375443, 0.32513827085494995, 0.32513827085494995, 0.32513827085494995, 0.29022297263145447, 0.5804459452629089, 0.07255574315786362, 0.9123002886772156, 0.07017694413661957, 0.6444443464279175, 0.21481476724147797, 0.10740738362073898, 0.07995261251926422, 0.07995261251926422, 0.15990522503852844, 0.7995261549949646, 0.044683609157800674, 0.044683609157800674, 0.8489885926246643, 0.08936721831560135, 0.30320262908935547, 0.07580065727233887, 0.2274019867181778, 0.4548039734363556, 0.07838843762874603, 0.31355375051498413, 0.548719048500061], \"Term\": [\"affect\", \"affect\", \"affect\", \"amaz\", \"amaz\", \"april\", \"april\", \"april\", \"april\", \"ask\", \"ask\", \"ask\", \"basi\", \"break\", \"break\", \"break\", \"break\", \"call\", \"call\", \"call\", \"care\", \"care\", \"care\", \"care\", \"case\", \"case\", \"case\", \"case\", \"cast\", \"caus\", \"caus\", \"ceas\", \"china\", \"china\", \"china\", \"confirm\", \"confirm\", \"confirm\", \"confirm\", \"corona\", \"corona\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"countri\", \"countri\", \"countri\", \"cover\", \"cover\", \"cover\", \"crisi\", \"crisi\", \"crisi\", \"death\", \"death\", \"death\", \"death\", \"die\", \"die\", \"die\", \"die\", \"discrimin\", \"discrimin\", \"doctor\", \"doctor\", \"doctor\", \"either\", \"enough\", \"enough\", \"enough\", \"equip\", \"equip\", \"equip\", \"even\", \"even\", \"even\", \"even\", \"everyon\", \"everyon\", \"everyon\", \"face\", \"face\", \"face\", \"father\", \"father\", \"father\", \"fight\", \"fight\", \"fight\", \"fight\", \"first\", \"first\", \"first\", \"first\", \"global\", \"govern\", \"govern\", \"govern\", \"governor\", \"governor\", \"governor\", \"governor\", \"great\", \"great\", \"gullibl\", \"health\", \"health\", \"health\", \"health\", \"help\", \"help\", \"help\", \"help\", \"home\", \"home\", \"home\", \"home\", \"hospit\", \"hospit\", \"hospit\", \"human\", \"human\", \"human\", \"human\", \"import\", \"import\", \"import\", \"india\", \"india\", \"india\", \"india\", \"isol\", \"isol\", \"keep\", \"keep\", \"keep\", \"keep\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"live\", \"live\", \"live\", \"live\", \"lockdown\", \"lockdown\", \"lockdown\", \"look\", \"look\", \"make\", \"make\", \"make\", \"make\", \"mani\", \"mani\", \"mani\", \"mask\", \"mask\", \"mask\", \"mask\", \"medic\", \"medic\", \"medic\", \"medic\", \"murder\", \"murder\", \"must\", \"must\", \"must\", \"narrat\", \"nation\", \"nation\", \"nation\", \"nation\", \"natur\", \"natur\", \"need\", \"need\", \"need\", \"never\", \"never\", \"never\", \"never\", \"nurs\", \"nurs\", \"nurs\", \"often\", \"order\", \"order\", \"order\", \"order\", \"pandem\", \"pandem\", \"pandem\", \"patient\", \"patient\", \"peopl\", \"peopl\", \"peopl\", \"peopl\", \"pleas\", \"pleas\", \"pleas\", \"polic\", \"polic\", \"poor\", \"posit\", \"posit\", \"posit\", \"poverti\", \"poverti\", \"presid\", \"presid\", \"presid\", \"propaganda\", \"protect\", \"protect\", \"protect\", \"protect\", \"public\", \"public\", \"public\", \"racism\", \"racism\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"releas\", \"releas\", \"releas\", \"releas\", \"religion\", \"report\", \"report\", \"report\", \"respons\", \"respons\", \"respons\", \"rich\", \"rich\", \"right\", \"right\", \"right\", \"right\", \"say\", \"say\", \"say\", \"say\", \"send\", \"send\", \"send\", \"speak\", \"speak\", \"speak\", \"spread\", \"spread\", \"spread\", \"spread\", \"state\", \"state\", \"state\", \"state\", \"stay\", \"stay\", \"stay\", \"support\", \"support\", \"support\", \"support\", \"take\", \"take\", \"take\", \"test\", \"test\", \"test\", \"test\", \"thank\", \"thank\", \"thank\", \"think\", \"think\", \"think\", \"think\", \"time\", \"time\", \"time\", \"time\", \"today\", \"today\", \"today\", \"today\", \"total\", \"total\", \"total\", \"total\", \"trump\", \"trump\", \"trump\", \"trump\", \"trut\", \"trut\", \"updat\", \"updat\", \"updat\", \"virus\", \"virus\", \"virus\", \"want\", \"want\", \"watch\", \"watch\", \"watch\", \"week\", \"week\", \"week\", \"week\", \"work\", \"work\", \"work\", \"work\", \"worker\", \"worker\", \"worker\", \"worker\", \"world\", \"world\", \"world\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 4, 2, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el6057352072720166019108443\", ldavis_el6057352072720166019108443_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el6057352072720166019108443\", ldavis_el6057352072720166019108443_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el6057352072720166019108443\", ldavis_el6057352072720166019108443_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.049807 -0.135289       1        1  26.186216\n",
       "3     -0.133277  0.136460       2        1  25.618834\n",
       "1      0.196366  0.054535       3        1  25.289398\n",
       "2     -0.013282 -0.055706       4        1  22.905554, topic_info=    Category       Freq         Term      Total  loglift  logprob\n",
       "49   Default  17.000000      patient  17.000000  30.0000  30.0000\n",
       "53   Default  32.000000         case  32.000000  29.0000  29.0000\n",
       "16   Default  25.000000       pandem  25.000000  28.0000  28.0000\n",
       "42   Default  22.000000         work  22.000000  27.0000  27.0000\n",
       "9    Default  16.000000        thank  16.000000  26.0000  26.0000\n",
       "37   Default  16.000000        natur  16.000000  25.0000  25.0000\n",
       "4    Default  16.000000         stay  16.000000  24.0000  24.0000\n",
       "34   Default  14.000000         caus  14.000000  23.0000  23.0000\n",
       "6    Default  22.000000         help  22.000000  22.0000  22.0000\n",
       "36   Default  13.000000       murder  13.000000  21.0000  21.0000\n",
       "5    Default  42.000000  coronavirus  42.000000  20.0000  20.0000\n",
       "32   Default  14.000000         want  14.000000  19.0000  19.0000\n",
       "38   Default  13.000000      poverti  13.000000  18.0000  18.0000\n",
       "41   Default  13.000000         trut  13.000000  17.0000  17.0000\n",
       "39   Default  13.000000       racism  13.000000  16.0000  16.0000\n",
       "35   Default  14.000000       father  14.000000  15.0000  15.0000\n",
       "3    Default  16.000000        state  16.000000  14.0000  14.0000\n",
       "10   Default  17.000000       spread  17.000000  13.0000  13.0000\n",
       "14   Default  12.000000       doctor  12.000000  12.0000  12.0000\n",
       "15   Default  10.000000       global  10.000000  11.0000  11.0000\n",
       "40   Default  15.000000        speak  15.000000  10.0000  10.0000\n",
       "78   Default  20.000000         need  20.000000   9.0000   9.0000\n",
       "79   Default  10.000000         look  10.000000   8.0000   8.0000\n",
       "24   Default  21.000000         mask  21.000000   7.0000   7.0000\n",
       "87   Default  16.000000        pleas  16.000000   6.0000   6.0000\n",
       "74   Default  10.000000        great  10.000000   5.0000   5.0000\n",
       "86   Default  12.000000         week  12.000000   4.0000   4.0000\n",
       "22   Default  34.000000        peopl  34.000000   3.0000   3.0000\n",
       "27   Default  12.000000     lockdown  12.000000   2.0000   2.0000\n",
       "50   Default  18.000000         time  18.000000   1.0000   1.0000\n",
       "32    Topic1  12.990092         want  14.249694   1.2474  -3.4168\n",
       "74    Topic1   9.573905        great  10.619604   1.2363  -3.7220\n",
       "100   Topic1   9.143170      confirm  10.980449   1.1568  -3.7680\n",
       "53    Topic1  25.428717         case  32.428932   1.0968  -2.7451\n",
       "6     Topic1  17.908781         help  22.871105   1.0954  -3.0957\n",
       "87    Topic1  13.019506        pleas  16.944826   1.0764  -3.4146\n",
       "96    Topic1  10.592992      protect  14.633836   1.0168  -3.6208\n",
       "21    Topic1  11.430941       hospit  15.871010   1.0118  -3.5447\n",
       "24    Topic1  15.741133         mask  21.993462   1.0055  -3.2247\n",
       "47    Topic1  10.257074       report  14.338034   1.0050  -3.6530\n",
       "25    Topic1  10.310595         send  14.487005   0.9999  -3.6478\n",
       "92    Topic1   8.946719        break  13.913244   0.8984  -3.7897\n",
       "103   Topic1   5.985230        watch   9.310347   0.8981  -4.1917\n",
       "97    Topic1   6.159941        polic   9.947659   0.8607  -4.1629\n",
       "78    Topic1  12.747252         need  20.684496   0.8559  -3.4357\n",
       "108   Topic1   6.090666       releas   9.944503   0.8497  -4.1742\n",
       "43    Topic1   9.657081       govern  16.370388   0.8122  -3.7133\n",
       "77    Topic1   6.942891        equip  12.092203   0.7851  -4.0433\n",
       "107   Topic1   4.889936        cover   8.777168   0.7550  -4.3938\n",
       "98    Topic1   4.496140    recommend   8.344910   0.7215  -4.4778\n",
       "101   Topic1   6.058694         face  11.516433   0.6977  -4.1795\n",
       "57    Topic1   5.682466       affect  11.174652   0.6637  -4.2436\n",
       "8     Topic1   6.327899         nurs  12.516233   0.6579  -4.1360\n",
       "1     Topic1   9.610342         home  19.086058   0.6538  -3.7182\n",
       "93    Topic1   5.170141        total  10.309472   0.6498  -4.3381\n",
       "46    Topic1   9.344562        death  18.742407   0.6439  -3.7462\n",
       "90    Topic1   5.212823          ask  11.536343   0.5456  -4.3299\n",
       "58    Topic1   4.056025         life   9.559053   0.4827  -4.5808\n",
       "52    Topic1   8.198145         take  19.381018   0.4796  -3.8771\n",
       "67    Topic1   5.368814      everyon  12.730203   0.4766  -4.3004\n",
       "48    Topic1   5.294270         care  14.894118   0.3056  -4.3144\n",
       "26    Topic1   4.828849        april  12.808717   0.3644  -4.4064\n",
       "22    Topic1   4.954605        peopl  34.512505  -0.6011  -4.3807\n",
       "79    Topic2   9.515604         look  10.606172   1.2533  -3.7062\n",
       "23    Topic2   5.773790        often   6.522631   1.2399  -4.2058\n",
       "83    Topic2   5.179495       narrat   5.925328   1.2273  -4.3144\n",
       "68    Topic2   7.457621         poor   8.535866   1.2268  -3.9499\n",
       "81    Topic2   5.171951         ceas   5.926028   1.2257  -4.3158\n",
       "82    Topic2   5.170472      gullibl   5.926143   1.2254  -4.3161\n",
       "85    Topic2   5.169783   propaganda   5.925744   1.2254  -4.3163\n",
       "63    Topic2   4.755949         basi   5.611041   1.1965  -4.3997\n",
       "69    Topic2   4.755176     religion   5.611360   1.1963  -4.3999\n",
       "64    Topic2   4.754246         cast   5.611476   1.1961  -4.4001\n",
       "66    Topic2   6.133111       either   7.244779   1.1953  -4.1454\n",
       "3     Topic2  13.570334        state  16.036219   1.1949  -3.3512\n",
       "104   Topic2   9.445579        posit  11.632424   1.1536  -3.7136\n",
       "19    Topic2   5.265066         rich   6.588737   1.1376  -4.2980\n",
       "88    Topic2   7.707809       corona   9.900167   1.1115  -3.9169\n",
       "80    Topic2   5.377324         amaz   7.032039   1.0936  -4.2769\n",
       "31    Topic2  10.327668        medic  13.524007   1.0922  -3.6243\n",
       "61    Topic2   8.085735         must  10.846028   1.0681  -3.8690\n",
       "59    Topic2   7.826613        crisi  11.610550   0.9675  -3.9016\n",
       "29    Topic2  12.706945         like  18.973452   0.9610  -3.4169\n",
       "5     Topic2  25.692549  coronavirus  42.374550   0.8615  -2.7129\n",
       "22    Topic2  20.900518        peopl  34.512505   0.8603  -2.9193\n",
       "13    Topic2  14.399014         test  24.163343   0.8442  -3.2919\n",
       "84    Topic2   6.164313        never  10.548359   0.8246  -4.1403\n",
       "65    Topic2   4.538717    discrimin   7.882490   0.8098  -4.4465\n",
       "30    Topic2   9.815392        today  17.615328   0.7770  -3.6751\n",
       "7     Topic2   9.404865         keep  17.672131   0.7311  -3.7179\n",
       "18    Topic2   6.287121        human  11.868560   0.7265  -4.1206\n",
       "17    Topic2   7.939980       public  15.230980   0.7104  -3.8872\n",
       "20    Topic2   6.102234        first  12.424326   0.6508  -4.1504\n",
       "67    Topic2   6.065997      everyon  12.730203   0.6206  -4.1564\n",
       "70    Topic2   7.698523        fight  20.853188   0.3654  -3.9181\n",
       "11    Topic2   5.498819          say  16.479660   0.2642  -4.2546\n",
       "99    Topic2   4.806947     governor  10.177045   0.6118  -4.3890\n",
       "36    Topic3  12.719639       murder  13.893884   1.2865  -3.4030\n",
       "37    Topic3  14.705959        natur  16.085615   1.2851  -3.2579\n",
       "38    Topic3  12.356256      poverti  13.527967   1.2842  -3.4320\n",
       "34    Topic3  13.369183         caus  14.651767   1.2832  -3.3532\n",
       "41    Topic3  12.337529         trut  13.528528   1.2826  -3.4335\n",
       "39    Topic3  12.332864       racism  13.529095   1.2822  -3.4339\n",
       "4     Topic3  14.646099         stay  16.564480   1.2517  -3.2620\n",
       "35    Topic3  12.631072       father  14.386718   1.2446  -3.4100\n",
       "42    Topic3  18.599968         work  22.379570   1.1898  -3.0230\n",
       "27    Topic3  10.129412     lockdown  12.334860   1.1778  -3.6307\n",
       "40    Topic3  12.157595        speak  15.152270   1.1546  -3.4482\n",
       "2     Topic3  10.938883        order  15.778488   1.0085  -3.5538\n",
       "91    Topic3   8.285903         call  12.755518   0.9434  -3.8316\n",
       "102   Topic3   8.319101       presid  12.807735   0.9433  -3.8276\n",
       "95    Topic3   6.977907         mani  10.797021   0.9383  -4.0034\n",
       "106   Topic3  10.788568        trump  17.229568   0.9066  -3.5677\n",
       "72    Topic3  14.767959         make  24.527685   0.8674  -3.2537\n",
       "73    Topic3   8.265266        virus  13.782506   0.8634  -3.8341\n",
       "60    Topic3   7.396604        china  12.723696   0.8323  -3.9451\n",
       "11    Topic3   9.386740          say  16.479660   0.8120  -3.7069\n",
       "45    Topic3   5.193068         isol   9.567211   0.7638  -4.2988\n",
       "105   Topic3   5.761499        right  11.501347   0.6835  -4.1950\n",
       "26    Topic3   6.400989        april  12.808717   0.6811  -4.0897\n",
       "55    Topic3   3.912104       enough   8.270704   0.6261  -4.5821\n",
       "52    Topic3   8.912276         take  19.381018   0.5979  -3.7587\n",
       "1     Topic3   7.841241         home  19.086058   0.4852  -3.8868\n",
       "71    Topic3   3.626519        india  10.378891   0.3233  -4.6579\n",
       "94    Topic3   3.109761        updat   9.226844   0.2872  -4.8116\n",
       "70    Topic3   6.832931        fight  20.853188   0.2590  -4.0244\n",
       "97    Topic3   3.253189        polic   9.947659   0.2571  -4.7665\n",
       "0     Topic3   6.598042       health  24.276871   0.0720  -4.0594\n",
       "28    Topic3   3.922728       nation  12.623188   0.2060  -4.5794\n",
       "16    Topic3   4.985285       pandem  25.757641  -0.2675  -4.3397\n",
       "22    Topic3   5.070840        peopl  34.512505  -0.5430  -4.3226\n",
       "49    Topic4  16.188950      patient  17.800676   1.3789  -3.0628\n",
       "15    Topic4   9.420922       global  10.423573   1.3727  -3.6042\n",
       "9     Topic4  14.494113        thank  16.601572   1.3380  -3.1734\n",
       "14    Topic4  10.719802       doctor  12.791441   1.2971  -3.4751\n",
       "86    Topic4   9.704568         week  12.507408   1.2201  -3.5746\n",
       "10    Topic4  13.453404       spread  17.700249   1.1994  -3.2479\n",
       "54    Topic4   8.764295      countri  11.704839   1.1845  -3.6765\n",
       "16    Topic4  18.661211       pandem  25.757641   1.1515  -2.9207\n",
       "76    Topic4  10.092359         live  14.615526   1.1035  -3.5354\n",
       "89    Topic4   9.636803          die  14.227369   1.0842  -3.5816\n",
       "50    Topic4  11.585971         time  18.114433   1.0269  -3.3974\n",
       "51    Topic4   6.010853       import  10.692856   0.8978  -4.0536\n",
       "12    Topic4   7.462041        think  13.529634   0.8787  -3.8373\n",
       "0     Topic4  13.002481       health  24.276871   0.8494  -3.2820\n",
       "56    Topic4   6.682570        world  12.756984   0.8272  -3.9476\n",
       "90    Topic4   5.147390          ask  11.536343   0.6668  -4.2087\n",
       "75    Topic4   5.506736       worker  13.192497   0.6001  -4.1412\n",
       "62    Topic4   4.949061      respons  12.064740   0.5827  -4.2480\n",
       "55    Topic4   3.266629       enough   8.270704   0.5448  -4.6634\n",
       "33    Topic4   3.654555      support   9.354101   0.5340  -4.5512\n",
       "48    Topic4   5.799328         care  14.894118   0.5306  -4.0894\n",
       "71    Topic4   3.673856        india  10.378891   0.4353  -4.5459\n",
       "44    Topic4   3.691156         even  11.056838   0.3767  -4.5412\n",
       "58    Topic4   3.185760         life   9.559053   0.3750  -4.6885\n",
       "65    Topic4   2.504801    discrimin   7.882490   0.3274  -4.9289\n",
       "101   Topic4   3.656008         face  11.516433   0.3264  -4.5508\n",
       "78    Topic4   6.345103         need  20.684496   0.2921  -3.9995\n",
       "46    Topic4   5.529822        death  18.742407   0.2532  -4.1370\n",
       "102   Topic4   3.722412       presid  12.807735   0.2381  -4.5328\n",
       "107   Topic4   2.500243        cover   8.777168   0.2180  -4.9308\n",
       "91    Topic4   3.602423         call  12.755518   0.2094  -4.5655\n",
       "5     Topic4   9.457815  coronavirus  42.374550  -0.0259  -3.6003\n",
       "7     Topic4   4.646491         keep  17.672131   0.1379  -4.3110\n",
       "17    Topic4   4.057281       public  15.230980   0.1510  -4.4466\n",
       "70    Topic4   4.074335        fight  20.853188  -0.1590  -4.4424\n",
       "72    Topic4   4.182442         make  24.527685  -0.2951  -4.4163\n",
       "13    Topic4   3.965987         test  24.163343  -0.3333  -4.4694\n",
       "53    Topic4   3.719751         case  32.428932  -0.6916  -4.5335, token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "57        1  0.536929       affect\n",
       "57        2  0.357953       affect\n",
       "57        3  0.089488       affect\n",
       "80        2  0.711031         amaz\n",
       "80        3  0.142206         amaz\n",
       "26        1  0.390359        april\n",
       "26        2  0.078072        april\n",
       "26        3  0.468431        april\n",
       "26        4  0.078072        april\n",
       "90        1  0.433413          ask\n",
       "90        3  0.086683          ask\n",
       "90        4  0.433413          ask\n",
       "63        2  0.891100         basi\n",
       "92        1  0.646866        break\n",
       "92        2  0.143748        break\n",
       "92        3  0.143748        break\n",
       "92        4  0.143748        break\n",
       "91        1  0.078397         call\n",
       "91        3  0.627180         call\n",
       "91        4  0.313590         call\n",
       "48        1  0.335703         care\n",
       "48        2  0.067141         care\n",
       "48        3  0.201422         care\n",
       "48        4  0.402844         care\n",
       "53        1  0.770917         case\n",
       "53        2  0.061673         case\n",
       "53        3  0.030837         case\n",
       "53        4  0.123347         case\n",
       "64        2  0.891031         cast\n",
       "34        3  0.887265         caus\n",
       "34        4  0.068251         caus\n",
       "81        2  0.843736         ceas\n",
       "60        2  0.235781        china\n",
       "60        3  0.550155        china\n",
       "60        4  0.157187        china\n",
       "100       1  0.819639      confirm\n",
       "100       2  0.091071      confirm\n",
       "100       3  0.091071      confirm\n",
       "100       4  0.091071      confirm\n",
       "88        1  0.202017       corona\n",
       "88        2  0.808067       corona\n",
       "5         1  0.094396  coronavirus\n",
       "5         2  0.613576  coronavirus\n",
       "5         3  0.094396  coronavirus\n",
       "5         4  0.212392  coronavirus\n",
       "54        1  0.170869      countri\n",
       "54        2  0.085435      countri\n",
       "54        4  0.768913      countri\n",
       "107       1  0.569660        cover\n",
       "107       2  0.113932        cover\n",
       "107       4  0.341796        cover\n",
       "59        1  0.172257        crisi\n",
       "59        2  0.689029        crisi\n",
       "59        4  0.172257        crisi\n",
       "46        1  0.480194        death\n",
       "46        2  0.160065        death\n",
       "46        3  0.053355        death\n",
       "46        4  0.320130        death\n",
       "89        1  0.210861          die\n",
       "89        2  0.070287          die\n",
       "89        3  0.070287          die\n",
       "89        4  0.702871          die\n",
       "65        2  0.634317    discrimin\n",
       "65        4  0.380590    discrimin\n",
       "14        2  0.078177       doctor\n",
       "14        3  0.078177       doctor\n",
       "14        4  0.859950       doctor\n",
       "66        2  0.828183       either\n",
       "55        1  0.120909       enough\n",
       "55        3  0.483635       enough\n",
       "55        4  0.362726       enough\n",
       "77        1  0.578885        equip\n",
       "77        2  0.082698        equip\n",
       "77        4  0.248094        equip\n",
       "44        1  0.180884         even\n",
       "44        2  0.361767         even\n",
       "44        3  0.090442         even\n",
       "44        4  0.361767         even\n",
       "67        1  0.392767      everyon\n",
       "67        2  0.471320      everyon\n",
       "67        3  0.078553      everyon\n",
       "101       1  0.520995         face\n",
       "101       2  0.173665         face\n",
       "101       4  0.347330         face\n",
       "35        1  0.069509       father\n",
       "35        3  0.903611       father\n",
       "35        4  0.069509       father\n",
       "70        1  0.095909        fight\n",
       "70        2  0.383634        fight\n",
       "70        3  0.335680        fight\n",
       "70        4  0.191817        fight\n",
       "20        1  0.160975        first\n",
       "20        2  0.482924        first\n",
       "20        3  0.080487        first\n",
       "20        4  0.241462        first\n",
       "15        4  0.863428       global\n",
       "43        1  0.610859       govern\n",
       "43        2  0.305430       govern\n",
       "43        4  0.122172       govern\n",
       "99        1  0.294781     governor\n",
       "99        2  0.491302     governor\n",
       "99        3  0.196521     governor\n",
       "99        4  0.098260     governor\n",
       "74        1  0.941655        great\n",
       "74        3  0.094165        great\n",
       "82        2  0.843719      gullibl\n",
       "0         1  0.164766       health\n",
       "0         2  0.041191       health\n",
       "0         3  0.288340       health\n",
       "0         4  0.535489       health\n",
       "6         1  0.787019         help\n",
       "6         2  0.043723         help\n",
       "6         3  0.043723         help\n",
       "6         4  0.131170         help\n",
       "1         1  0.523943         home\n",
       "1         2  0.052394         home\n",
       "1         3  0.419154         home\n",
       "1         4  0.052394         home\n",
       "21        1  0.693088       hospit\n",
       "21        2  0.063008       hospit\n",
       "21        4  0.189024       hospit\n",
       "18        1  0.084256        human\n",
       "18        2  0.505537        human\n",
       "18        3  0.168512        human\n",
       "18        4  0.252769        human\n",
       "51        2  0.374082       import\n",
       "51        3  0.093520       import\n",
       "51        4  0.561122       import\n",
       "71        1  0.192699        india\n",
       "71        2  0.096349        india\n",
       "71        3  0.385398        india\n",
       "71        4  0.385398        india\n",
       "45        2  0.418095         isol\n",
       "45        3  0.522618         isol\n",
       "7         1  0.169759         keep\n",
       "7         2  0.509276         keep\n",
       "7         3  0.056586         keep\n",
       "7         4  0.282931         keep\n",
       "58        1  0.418451         life\n",
       "58        3  0.209226         life\n",
       "58        4  0.313839         life\n",
       "29        1  0.158116         like\n",
       "29        2  0.685168         like\n",
       "29        3  0.052705         like\n",
       "29        4  0.105410         like\n",
       "76        1  0.068420         live\n",
       "76        2  0.068420         live\n",
       "76        3  0.205261         live\n",
       "76        4  0.684204         live\n",
       "27        1  0.081071     lockdown\n",
       "27        3  0.810710     lockdown\n",
       "27        4  0.081071     lockdown\n",
       "79        2  0.942847         look\n",
       "79        3  0.094285         look\n",
       "72        1  0.122311         make\n",
       "72        2  0.081541         make\n",
       "72        3  0.611554         make\n",
       "72        4  0.163081         make\n",
       "95        1  0.092618         mani\n",
       "95        3  0.648327         mani\n",
       "95        4  0.185236         mani\n",
       "24        1  0.727489         mask\n",
       "24        2  0.136404         mask\n",
       "24        3  0.045468         mask\n",
       "24        4  0.090936         mask\n",
       "31        1  0.073943        medic\n",
       "31        2  0.739426        medic\n",
       "31        3  0.073943        medic\n",
       "31        4  0.073943        medic\n",
       "36        3  0.935664       murder\n",
       "36        4  0.071974       murder\n",
       "61        2  0.737597         must\n",
       "61        3  0.184399         must\n",
       "61        4  0.092200         must\n",
       "83        2  0.843835       narrat\n",
       "28        1  0.396096       nation\n",
       "28        2  0.158439       nation\n",
       "28        3  0.316877       nation\n",
       "28        4  0.079219       nation\n",
       "37        3  0.932510        natur\n",
       "37        4  0.062167        natur\n",
       "78        1  0.628490         need\n",
       "78        3  0.048345         need\n",
       "78        4  0.290072         need\n",
       "84        1  0.094801        never\n",
       "84        2  0.568809        never\n",
       "84        3  0.189603        never\n",
       "84        4  0.189603        never\n",
       "8         1  0.479377         nurs\n",
       "8         2  0.159792         nurs\n",
       "8         4  0.239689         nurs\n",
       "23        2  0.919874        often\n",
       "2         1  0.063377        order\n",
       "2         2  0.190132        order\n",
       "2         3  0.697152        order\n",
       "2         4  0.063377        order\n",
       "16        1  0.077647       pandem\n",
       "16        3  0.194117       pandem\n",
       "16        4  0.737645       pandem\n",
       "49        3  0.056178      patient\n",
       "49        4  0.898842      patient\n",
       "22        1  0.144875        peopl\n",
       "22        2  0.608475        peopl\n",
       "22        3  0.144875        peopl\n",
       "22        4  0.115900        peopl\n",
       "87        1  0.767196        pleas\n",
       "87        3  0.059015        pleas\n",
       "87        4  0.118030        pleas\n",
       "97        1  0.603157        polic\n",
       "97        3  0.301578        polic\n",
       "68        2  0.820069         poor\n",
       "104       2  0.773699        posit\n",
       "104       3  0.085967        posit\n",
       "104       4  0.085967        posit\n",
       "38        3  0.887051      poverti\n",
       "38        4  0.073921      poverti\n",
       "102       1  0.078078       presid\n",
       "102       3  0.624623       presid\n",
       "102       4  0.312311       presid\n",
       "85        2  0.843776   propaganda\n",
       "96        1  0.751683      protect\n",
       "96        2  0.068335      protect\n",
       "96        3  0.136670      protect\n",
       "96        4  0.136670      protect\n",
       "17        1  0.196967       public\n",
       "17        2  0.525245       public\n",
       "17        4  0.262623       public\n",
       "39        3  0.886977       racism\n",
       "39        4  0.073915       racism\n",
       "98        1  0.479334    recommend\n",
       "98        2  0.239667    recommend\n",
       "98        3  0.119834    recommend\n",
       "98        4  0.119834    recommend\n",
       "108       1  0.603348       releas\n",
       "108       2  0.100558       releas\n",
       "108       3  0.100558       releas\n",
       "108       4  0.201116       releas\n",
       "69        2  0.891050     religion\n",
       "47        1  0.697446       report\n",
       "47        3  0.209234       report\n",
       "47        4  0.069745       report\n",
       "62        1  0.331545      respons\n",
       "62        2  0.248658      respons\n",
       "62        4  0.414431      respons\n",
       "19        1  0.151774         rich\n",
       "19        2  0.758871         rich\n",
       "105       1  0.260839        right\n",
       "105       2  0.086946        right\n",
       "105       3  0.521678        right\n",
       "105       4  0.173893        right\n",
       "11        1  0.060681          say\n",
       "11        2  0.303404          say\n",
       "11        3  0.546128          say\n",
       "11        4  0.060681          say\n",
       "25        1  0.690274         send\n",
       "25        2  0.207082         send\n",
       "25        4  0.069027         send\n",
       "40        1  0.065997        speak\n",
       "40        3  0.791961        speak\n",
       "40        4  0.131993        speak\n",
       "10        1  0.112993       spread\n",
       "10        2  0.056496       spread\n",
       "10        3  0.056496       spread\n",
       "10        4  0.734453       spread\n",
       "3         1  0.062359        state\n",
       "3         2  0.873024        state\n",
       "3         3  0.062359        state\n",
       "3         4  0.062359        state\n",
       "4         1  0.060370         stay\n",
       "4         2  0.060370         stay\n",
       "4         3  0.905552         stay\n",
       "33        1  0.320715      support\n",
       "33        2  0.213810      support\n",
       "33        3  0.106905      support\n",
       "33        4  0.427620      support\n",
       "52        1  0.412775         take\n",
       "52        2  0.103194         take\n",
       "52        3  0.464372         take\n",
       "13        1  0.082770         test\n",
       "13        2  0.579390         test\n",
       "13        3  0.165540         test\n",
       "13        4  0.165540         test\n",
       "9         1  0.060235        thank\n",
       "9         2  0.060235        thank\n",
       "9         4  0.843294        thank\n",
       "12        1  0.073912        think\n",
       "12        2  0.295647        think\n",
       "12        3  0.073912        think\n",
       "12        4  0.517383        think\n",
       "50        1  0.055205         time\n",
       "50        2  0.110409         time\n",
       "50        3  0.165614         time\n",
       "50        4  0.662455         time\n",
       "30        1  0.227075        today\n",
       "30        2  0.567687        today\n",
       "30        3  0.056769        today\n",
       "30        4  0.170306        today\n",
       "93        1  0.484991        total\n",
       "93        2  0.290995        total\n",
       "93        3  0.096998        total\n",
       "93        4  0.096998        total\n",
       "106       1  0.116080        trump\n",
       "106       2  0.174119        trump\n",
       "106       3  0.638437        trump\n",
       "106       4  0.116080        trump\n",
       "41        3  0.887014         trut\n",
       "41        4  0.073918         trut\n",
       "94        1  0.325138        updat\n",
       "94        2  0.325138        updat\n",
       "94        3  0.325138        updat\n",
       "73        1  0.290223        virus\n",
       "73        3  0.580446        virus\n",
       "73        4  0.072556        virus\n",
       "32        1  0.912300         want\n",
       "32        4  0.070177         want\n",
       "103       1  0.644444        watch\n",
       "103       2  0.214815        watch\n",
       "103       4  0.107407        watch\n",
       "86        1  0.079953         week\n",
       "86        2  0.079953         week\n",
       "86        3  0.159905         week\n",
       "86        4  0.799526         week\n",
       "42        1  0.044684         work\n",
       "42        2  0.044684         work\n",
       "42        3  0.848989         work\n",
       "42        4  0.089367         work\n",
       "75        1  0.303203       worker\n",
       "75        2  0.075801       worker\n",
       "75        3  0.227402       worker\n",
       "75        4  0.454804       worker\n",
       "56        1  0.078388        world\n",
       "56        2  0.313554        world\n",
       "56        4  0.548719        world, R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 4, 2, 3])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_tfidf, tfidf_corpus, dictionary=lda_model_tfidf.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Dominant Topic for each tweet\n",
    "So far we built a topic model with tuned hyper parameters. Now, let's find out which topic does each of the tweets belong to. We'll do this by running the tweet through the model. The model would return with a list of topic_id and corresponding Percentage contribution of each topic in the tweet. We'll take the topic with the highest percentage contribution as the dominant topic for the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_Num</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6320</td>\n",
       "      <td>peopl, test, coronavirus, like, help, state, keep, everyon, home, virus</td>\n",
       "      <td>[state, health, offic, scott, harri, issu, stay, home, order, strict, quarantin, requir, read, full]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6530</td>\n",
       "      <td>peopl, test, coronavirus, like, help, state, keep, everyon, home, virus</td>\n",
       "      <td>[thank, nurs, help, keep, healthi, covid, coronavirus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6262</td>\n",
       "      <td>case, work, coronavirus, speak, natur, caus, father, murder, poverti, racism</td>\n",
       "      <td>[togetherapart, slow, spread, covid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>health, fight, make, pandem, mask, time, say, help, face, think</td>\n",
       "      <td>[smoker, greater, risk, contract, coronavirus, elli, cannon, say, equal, risk, contract]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>health, fight, make, pandem, mask, time, say, help, face, think</td>\n",
       "      <td>[video, model, scan, show, extent, covid, damag, lung, tissu, stayhom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5925</td>\n",
       "      <td>health, fight, make, pandem, mask, time, say, help, face, think</td>\n",
       "      <td>[leader, hous, parti, caucus, arizona, andi, bigg, think, spread, covid, much, possib]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6636</td>\n",
       "      <td>peopl, test, coronavirus, like, help, state, keep, everyon, home, virus</td>\n",
       "      <td>[covid, test, administ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9117</td>\n",
       "      <td>health, fight, make, pandem, mask, time, say, help, face, think</td>\n",
       "      <td>[keep, think, master, public, health, write, doctor, dissert, global, effort, tackl, aid, pandem]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8284</td>\n",
       "      <td>peopl, test, coronavirus, like, help, state, keep, everyon, home, virus</td>\n",
       "      <td>[ceylonblacktea, rich, theaflavin, help, increas, human, immun, covid, srilankatea, industri, successfu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8221</td>\n",
       "      <td>peopl, test, coronavirus, like, help, state, keep, everyon, home, virus</td>\n",
       "      <td>[peopl, first, group, hospit, sick, place, infirmari, convert, convent]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet_Num  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0          0             1.0              0.6320   \n",
       "1          1             1.0              0.6530   \n",
       "2          2             2.0              0.6262   \n",
       "3          3             0.0              0.6249   \n",
       "4          4             0.0              0.3333   \n",
       "5          5             0.0              0.5925   \n",
       "6          6             1.0              0.6636   \n",
       "7          7             0.0              0.9117   \n",
       "8          8             1.0              0.8284   \n",
       "9          9             1.0              0.8221   \n",
       "\n",
       "                                                                       Keywords  \\\n",
       "0       peopl, test, coronavirus, like, help, state, keep, everyon, home, virus   \n",
       "1       peopl, test, coronavirus, like, help, state, keep, everyon, home, virus   \n",
       "2  case, work, coronavirus, speak, natur, caus, father, murder, poverti, racism   \n",
       "3               health, fight, make, pandem, mask, time, say, help, face, think   \n",
       "4               health, fight, make, pandem, mask, time, say, help, face, think   \n",
       "5               health, fight, make, pandem, mask, time, say, help, face, think   \n",
       "6       peopl, test, coronavirus, like, help, state, keep, everyon, home, virus   \n",
       "7               health, fight, make, pandem, mask, time, say, help, face, think   \n",
       "8       peopl, test, coronavirus, like, help, state, keep, everyon, home, virus   \n",
       "9       peopl, test, coronavirus, like, help, state, keep, everyon, home, virus   \n",
       "\n",
       "                                                                                                       Text  \n",
       "0      [state, health, offic, scott, harri, issu, stay, home, order, strict, quarantin, requir, read, full]  \n",
       "1                                                    [thank, nurs, help, keep, healthi, covid, coronavirus]  \n",
       "2                                                                      [togetherapart, slow, spread, covid]  \n",
       "3                  [smoker, greater, risk, contract, coronavirus, elli, cannon, say, equal, risk, contract]  \n",
       "4                                    [video, model, scan, show, extent, covid, damag, lung, tissu, stayhom]  \n",
       "5                    [leader, hous, parti, caucus, arizona, andi, bigg, think, spread, covid, much, possib]  \n",
       "6                                                                                   [covid, test, administ]  \n",
       "7         [keep, think, master, public, health, write, doctor, dissert, global, effort, tackl, aid, pandem]  \n",
       "8  [ceylonblacktea, rich, theaflavin, help, increas, human, immun, covid, srilankatea, industri, successfu]  \n",
       "9                                   [peopl, first, group, hospit, sick, place, infirmari, convert, convent]  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=None):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    \n",
    "    # Let's get the topics and their keywords\n",
    "    topicsDF = pd.DataFrame()\n",
    "    for topic_id in range(lda_model_bow.num_topics):\n",
    "        wp = lda_model_bow.show_topic(topic_id)\n",
    "        topic_keywords = \", \".join([word for word, prop in wp])\n",
    "        topicsDF = topicsDF.append(pd.Series([int(topic_id),topic_keywords]),ignore_index=True)\n",
    "    topicsDF.columns=['Topic_Num','Keywords']\n",
    "    \n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row_list, key=lambda x: (x[1]), reverse=True)\n",
    "        \n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        (topic_num, prop_topic) = row[0]\n",
    "        sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4)]), ignore_index=True)\n",
    "    sent_topics_df.columns = ['Topic_Num', 'Perc_Contribution']\n",
    "\n",
    "    # Merge with topicsDF to add the Keywords from each topic\n",
    "    sent_topics_df = sent_topics_df.merge(topicsDF, on='Topic_Num', how='left')\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "tweet_topics = format_topics_sentences(ldamodel=lda_model_bow, corpus=bow_corpus, texts=processed_tweets)\n",
    "\n",
    "# Format\n",
    "tweet_topics = tweet_topics.reset_index()\n",
    "tweet_topics.columns = ['Tweet_Num', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "tweet_topics.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Most representative tweet for each topic\n",
    "In this section, let's find out the tweet that matches with the topic strongly. For this, we'll group all tweets by their corresponding dominant topics. For each topic, we'll identify the tweet with highest topic percentage contribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Tweet_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Representative Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>571</td>\n",
       "      <td>0.9117</td>\n",
       "      <td>health, fight, make, pandem, mask, time, say, help, face, think</td>\n",
       "      <td>[keep, think, master, public, health, write, doctor, dissert, global, effort, tackl, aid, pandem]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145</td>\n",
       "      <td>0.9376</td>\n",
       "      <td>peopl, test, coronavirus, like, help, state, keep, everyon, home, virus</td>\n",
       "      <td>[coronavirus, affect, everyon, discrimin, basi, religion, cast, rich, poor, either]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>861</td>\n",
       "      <td>0.9332</td>\n",
       "      <td>case, work, coronavirus, speak, natur, caus, father, murder, poverti, racism</td>\n",
       "      <td>[natur, caus, father, murder, work, racism, poverti, speak, trut]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Tweet_Num  Topic_Perc_Contrib  \\\n",
       "0        0.0        571              0.9117   \n",
       "1        1.0        145              0.9376   \n",
       "2        2.0        861              0.9332   \n",
       "\n",
       "                                                                       Keywords  \\\n",
       "0               health, fight, make, pandem, mask, time, say, help, face, think   \n",
       "1       peopl, test, coronavirus, like, help, state, keep, everyon, home, virus   \n",
       "2  case, work, coronavirus, speak, natur, caus, father, murder, poverti, racism   \n",
       "\n",
       "                                                                                 Representative Text  \n",
       "0  [keep, think, master, public, health, write, doctor, dissert, global, effort, tackl, aid, pandem]  \n",
       "1                [coronavirus, affect, everyon, discrimin, basi, religion, cast, rich, poor, either]  \n",
       "2                                  [natur, caus, father, murder, work, racism, poverti, speak, trut]  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sent_topics_sorteddf_mallet -> topic_sent\n",
    "topic_sent = pd.DataFrame()\n",
    "topic_sent_grpd = tweet_topics.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in topic_sent_grpd:\n",
    "    topic_sent = pd.concat([topic_sent, grp.sort_values(['Topic_Perc_Contrib'], ascending=False).head(1)], axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "topic_sent.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "topic_sent.columns = ['Tweet_Num','Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "topic_sent = topic_sent.reindex(columns=['Topic_Num','Tweet_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"])\n",
    "\n",
    "# Show\n",
    "topic_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "EarlyIndicatorsFromNews",
  "notebookId": 3907296281459545
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
