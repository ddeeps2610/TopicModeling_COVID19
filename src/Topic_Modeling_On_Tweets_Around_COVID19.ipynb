{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling for COVID19\n",
    "COVID 19 has been the biggest pandemic people have seen in the recent times. It almost feels like one of those apocalyptic movies in real life. People hiding in their homes trying to save themselves from the infection. Some brave souls trying to find a better destination to survive this pandemic. With so much happening around the world, I have one question. **What are people around the world thinking about COVID 19?**\n",
    "\n",
    "In this notebook, we will try to answer the above question using Topic Modeling. Let's explore the various topics people are talking about Corona Virus Disease 2019(COVID-19) in Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/deepakawari/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Twitter data collection library\n",
    "import tweepy as tw\n",
    "\n",
    "# Data processing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Loading Gensim and nltk libraries\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "#Other stop_words: gensim.parsing.preprocessing.STOPWORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure notebook display to show data from pandas dataframe more clearly.\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.width',100)\n",
    "pd.set_option('display.max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Gather the textual data for Topic Modeling\n",
    "To begin with the topic modeling we need the textual data. The textual data for what people are talking about COVID 19 can be pulled from many places such as social media, news articles, web scraping etc. In this notebook, we'll download the data from Twitter. Tweepy is an amazing library to pull data from twitter using your Twitter Developer Account. \n",
    "\n",
    "## Extracting tweets from Twitter API:\n",
    "To load the data from Twitter using Tweepy API, you'll have to create Developer account with Twitter. Then download the credentials to authenticate using Tweepy API. **Please do not share these credentials with anybody else.**\n",
    "* Here is the link to [apply for twitter developer access](https://developer.twitter.com/en/apply-for-access)\n",
    "* You can follow the below code to use Tweepy API to authenticate and load the data. Here is the [Tweepy Documentation for reference](http://docs.tweepy.org/en/latest/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LoadFromTwitter - \n",
    "    If true, pull the latest set of tweets from Twitter using the Tweepy library.\n",
    "    If false, load the data from the datafile '../data/tweets.csv' if it exists, \n",
    "    otherwise load the tweets from Twitter using the Tweepy library.\n",
    "    Set the LoadFromTwitter to True if you want to override loading the tweets afresh from twitter.\n",
    "'''\n",
    "LoadFromTwitter = False\n",
    "\n",
    "fileName = '../data/tweets.csv'\n",
    "tweetsDF = None\n",
    "\n",
    "# Load the data\n",
    "if os.path.exists(fileName) and not LoadFromTwitter:\n",
    "    tweetsDF = pd.read_csv(fileName)\n",
    "else:\n",
    "    from TwitterDevSecrets import getTwitterDevCreds\n",
    "    consumer_key, consumer_secret, access_token, access_secret = getTwitterDevCreds()\n",
    "\n",
    "    auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "    # Set the wait_on_rate_limit and wait_on_rate_limit_notify to True\n",
    "    # wait_on_rate_limit – \n",
    "    #    Whether or not to automatically wait for rate limits to replenish\n",
    "    # wait_on_rate_limit_notify – \n",
    "    #    Whether or not to print a notification when Tweepy is waiting \n",
    "    #    for rate limits to replenish\n",
    "    api = tw.API(\n",
    "        auth, \n",
    "        wait_on_rate_limit=True, \n",
    "        wait_on_rate_limit_notify=True)\n",
    "\n",
    "    # Define the search term and the date_since date as variables\n",
    "    search_words = \"#covid OR #covid19 OR #COVID OR #COVID19 OR #ncov OR #corona OR #coronaviru\"\n",
    "    date_since = \"2020-05-16\"\n",
    "    \n",
    "    # Read the tweets\n",
    "    tweets = tw.Cursor(api.search, \n",
    "                   q=search_words,\n",
    "                   lang=\"en\",\n",
    "                   since=date_since)\n",
    "\n",
    "    # extract the data in pandas dataframe\n",
    "    # Other parameters: tweet.user.screen_name, retweet_counts, favorite_counts\n",
    "    tweetsDF = pd.DataFrame()\n",
    "    for tweet in tweets.items(1000):\n",
    "        id = tweet.id\n",
    "        text = tweet.text\n",
    "        loc = tweet.user.location\n",
    "        tweetsDF = tweetsDF.append({'Id':id, 'Text':text, 'Location':loc},ignore_index=True)\n",
    "    \n",
    "    tweetsDF['index'] = tweetsDF.index\n",
    "    \n",
    "    # Save the new set of tweets in the file.\n",
    "    tweetsDF.to_csv(fileName,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how doest he textual data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @Carol_D_Johnson: Thank you nurses for helping to keep us healthy  ❤ #COVID19 \\n#StayHomeSaveLives \\n#coronavirus https://t.co/HGv0HfuTgt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF.Text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Data preprocessing\n",
    "As you can see from the above text, a tweet contains a lot of textual data which probably doesn't contain any useful informaiton for Topic Modeling. So, these tweets needs to be processed to extract only useful textual data for further analysis. We will perform the following data processing steps:\n",
    "\n",
    "* Tweet Preprocessing:\n",
    "> * Remove the leading **RT** - RT indicates that the user is re-posting someone else's tweet. We can remove this token.\n",
    "> * Remove the references to other accounts. The other accounts are usually referenced with '@' symbol.\n",
    "> * Remove urls mentioned in the tweets.\n",
    "\n",
    "* Generic text preprocessing:\n",
    "> * **Tokenization**: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "> * Remove words that have fewer than 3 characters.\n",
    "> * Remove all **stopwords**. [Stop words](https://en.wikipedia.org/wiki/Stop_words) usually do not contain any usual information. As such these words are generally removed from the text in the preprocessing stage. \n",
    "> * **Lemmatize** the words: words in third person are changed to first person and verbs in past and future tenses are changed into present.  \n",
    "> Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words. \n",
    ">> **WordnetLemmatizer**: uses lookup table from nltk wordnet corpus to lookup the lemma to return a valid language lemma.\n",
    "> * **Stem** the Words: words are reduced to their root form.  \n",
    "> Stemming is the process of reducing inflection in words to their root forms such as \n",
    "mapping a group of words to the same stem even if the stem itself is not a valid word \n",
    "in the Language.\n",
    ">> **PorterStemmer**: is known for simplicity and ease. The algorithm does not follow linguistics rather a set of rules that are applied in phases (step by step) to generate stems. This is the reason why PorterStemmer does not often generate stems that are actual English words.  \n",
    ">> **SnowballStemmer**: One can generate their own set of rules for any language. Python nltk introduced SnowballStemmers that are used to create non-English Stemmers!  \n",
    ">> **LancasterStemmer**: is simple, but heavy stemming due to iterations and over-stemming may occur. Over-stemming causes the stems to be not linguistic, or they may have no meaning.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data preprocessing for all tweets.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def tweet_cleanup(text):\n",
    "    # Remove the leading RT from the tweet\n",
    "    text = text.replace('RT','')\n",
    "    # Remove the references to the account names starting with '@'\n",
    "    text = re.sub(r'(@[a-zA-Z]*)','',text)\n",
    "    # Remove the urls in the tweet.\n",
    "    text = re.sub(r'((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)','',text)\n",
    "    \n",
    "    return text\n",
    "  \n",
    "# Clean up the tweets and then Tokenize and lemmatize\n",
    "def preprocess(text, stop_words=stop_words):\n",
    "    result=[]\n",
    "    text = tweet_cleanup(text)\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in stop_words and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: \n",
      "['RT', '@ALPublicHealth:', 'State', 'Health', 'Officer', 'Dr.', 'Scott', 'Harris', 'has', 'issued', 'a', 'stay', 'at', 'home', 'order', 'and', 'strict', 'quarantine', 'requirements.', 'Read', 'our', 'full…']\n",
      "\n",
      "\n",
      "Preprocessed tweet: \n",
      "['state', 'health', 'offic', 'scott', 'harri', 'issu', 'stay', 'home', 'order', 'strict', 'quarantin', 'requir', 'read', 'full']\n"
     ]
    }
   ],
   "source": [
    "# Test the preprocessing step on a sample tweet\n",
    "tweet_num = 0\n",
    "sampleTweet = tweetsDF[tweetsDF['index'] == tweet_num].Text.iloc[0]\n",
    "\n",
    "print(\"Original tweet: \")\n",
    "words = []\n",
    "for word in sampleTweet.split():\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nPreprocessed tweet: \")\n",
    "print(preprocess(sampleTweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [state, health, offic, scott, harri, issu, stay, home, order, strict, quarantin, requir, read, full]\n",
       "1                                                      [thank, nurs, help, keep, healthi, covid, coronavirus]\n",
       "2                                                                        [togetherapart, slow, spread, covid]\n",
       "3                    [smoker, greater, risk, contract, coronavirus, elli, cannon, say, equal, risk, contract]\n",
       "4                                      [video, model, scan, show, extent, covid, damag, lung, tissu, stayhom]\n",
       "5                      [leader, hous, parti, caucus, arizona, andi, bigg, think, spread, covid, much, possib]\n",
       "6                                                                                     [covid, test, administ]\n",
       "7           [keep, think, master, public, health, write, doctor, dissert, global, effort, tackl, aid, pandem]\n",
       "8    [ceylonblacktea, rich, theaflavin, help, increas, human, immun, covid, srilankatea, industri, successfu]\n",
       "9                                     [peopl, first, group, hospit, sick, place, infirmari, convert, convent]\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess all tweets and generate a new processed tweet text dataset.\n",
    "processed_tweets = tweetsDF['Text'].map(preprocess)\n",
    "processed_tweets[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Text representation\n",
    "Computers don't understand natural language texts. Text is a mere sequence of letters for computers. While its still difficult for computer to understand what the sequence of letters mean, language is way more complicated than that. For an example, let us consider an idiom \"Kicked the bucket\". You know where I am going right? When I first heard that phrase as a kid I thought it meant someone was actually kicking a bucket. That's fun! But, when I realized that it meant someone died, it was no more fun! So, natural language is hard and computers don't understand it. \n",
    "\n",
    "Computers love numbers. At the core, computers perform their operations on numbers. So, it'd be good to represent the natural language text with numbers for computer algorithms to process easily. In the below section, we'll explore two different models for text representation namely Bag of words and TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1: Bag of words on the dataset\n",
    "Create a dictionary of words present in the preprocessed_tweets dataset. Gensim offers a great api for the same. This dictionary assigns a numerical id to each word so that you can work on the number representations of the word. This makes the data processing very easy than working on strings. \n",
    "\n",
    "Then create a corpus of Bag of words where words are represented by their numerical ids along with the frequency of occurence of that word in the tweet for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_tweets)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 full\n",
      "1 harri\n",
      "2 health\n",
      "3 home\n",
      "4 issu\n",
      "5 offic\n"
     ]
    }
   ],
   "source": [
    "# Check the id to word mapping from the dictionary created above\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the text corpus is very huge and sparse, we should try to minimize the amount of text being used for modeling. For this reason, let us remove very rare and very common words. Gensim dictionary object provides a good api to perform this operation.\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "\n",
    "Then convert it into bag of word corpus with very rare and very common wordsd filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_tweets]\n",
    "\n",
    "# Test the Bag of Words representation of the tweet --> (token_id, token_count)\n",
    "bow_corpus[tweet_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"health\") appears 1 time.\n",
      "Word 1 (\"home\") appears 1 time.\n",
      "Word 2 (\"order\") appears 1 time.\n",
      "Word 3 (\"state\") appears 1 time.\n",
      "Word 4 (\"stay\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Preview BOW for our sample preprocessed tweet\n",
    "bow_tweet_0 = bow_corpus[tweet_num]\n",
    "\n",
    "for i in range(len(bow_tweet_0)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_tweet_0[i][0], \n",
    "                                                     dictionary[bow_tweet_0[i][0]], \n",
    "                                                     bow_tweet_0[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2: TF-IDF on the data set\n",
    "TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. Summing the Tf-idf of all possible terms and documents recovers the mutual information between documents and term taking into account all the specificities of their joint distribution.\n",
    "\n",
    "TF (Term Frequency) - number of times a term occurs in a document.  \n",
    "IDF (Inverse Document Frequency) diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.40009295170061265),\n",
       " (1, 0.44472924895798494),\n",
       " (2, 0.45253501051552114),\n",
       " (3, 0.47433139975516847),\n",
       " (4, 0.4608289406979316)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tf-idf model object using models.TfidfModel\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# Apply transformation to the entire corpus\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "\n",
    "# Test the tf-idf representation of the sample tweet. Each word is represented by (token_id, tf-idf score).\n",
    "tfidf_corpus[tweet_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"health\") TF-IDF score: 0.40009295170061265.\n",
      "Word 1 (\"home\") TF-IDF score: 0.44472924895798494.\n",
      "Word 2 (\"order\") TF-IDF score: 0.45253501051552114.\n",
      "Word 3 (\"state\") TF-IDF score: 0.47433139975516847.\n",
      "Word 4 (\"stay\") TF-IDF score: 0.4608289406979316.\n"
     ]
    }
   ],
   "source": [
    "# Preview TF-IDF for our sample preprocessed tweet\n",
    "tfidf_tweet_0 = tfidf_corpus[tweet_num]\n",
    "\n",
    "for i in range(len(tfidf_tweet_0)):\n",
    "    print(\"Word {} (\\\"{}\\\") TF-IDF score: {}.\".format(tfidf_tweet_0[i][0], \n",
    "                                                     dictionary[tfidf_tweet_0[i][0]], \n",
    "                                                     tfidf_tweet_0[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Topic modeling using LDA\n",
    "Topic modeling is a statistical model to discover the abstract topics in a collection of documents. Probabilistic Latent Semantic Analysis(PLSA) is one of the ealiest models for topic modeling. Latent Dirichlet Allocation(LDA) is the most common topic model algorithm in use today which is a generalization of PLSA. \n",
    "\n",
    "LDA introduces sparse Dirichlet prior distributions over document-topic and topic-word distributions. This algorithm tries to model the intuition that each document has different abstract topics and that each topic is generalized by a small number of words.\n",
    "\n",
    "In this section we'll be building the topic models using LDA for both text representations developed above. \n",
    "\n",
    "\n",
    "## Step 4.1: Modeling using Bag of Words\n",
    "The LDA algorithm requires a few inputs to build the clusters. The main parameter it requires in the number of clusters we want the model to cluster the tweets into. But how do we identify the number of topics? The best way to identify that is by visualizing the clusters itself. \n",
    "\n",
    "Start with a high number of topics like 10 or 20. Then map the clusters into a vector space and see if the clusters have clear boundaries. If the clusters overlap, reduce the number of clusters, build the model and visualize again. Repeat the process until you are satisfied with the segregation of the clusters.\n",
    "\n",
    "### Step 4.1.1: Running LDA using bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the lda model using gensim.models.LdaMulticore on Bag of word corpus\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=3, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.035*\"mask\" + 0.033*\"keep\" + 0.028*\"stay\" + 0.025*\"home\" + 0.021*\"say\" + 0.021*\"human\" + 0.020*\"health\" + 0.020*\"order\" + 0.020*\"public\" + 0.019*\"time\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.070*\"coronavirus\" + 0.043*\"natur\" + 0.042*\"work\" + 0.040*\"speak\" + 0.039*\"caus\" + 0.037*\"father\" + 0.036*\"racism\" + 0.036*\"trut\" + 0.036*\"murder\" + 0.035*\"poverti\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.072*\"case\" + 0.051*\"test\" + 0.043*\"peopl\" + 0.041*\"make\" + 0.038*\"like\" + 0.036*\"today\" + 0.035*\"health\" + 0.031*\"fight\" + 0.030*\"death\" + 0.027*\"pandem\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the words occuring in that topic and its relative weight\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the top words in each topic, we can identify the generic topic in that cluster. In the above clustering, the topics could be around  \n",
    "Topic 0: Self quarantining     \n",
    "Topic 1: Impact of COVID-19 on work, racism, and poverty.   \n",
    "Topic 2: Testing and health concerns due to COVID 19.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Visualization using pyLDAVis for LDA with BOW\n",
    "In this section, we'll visualize the topics generated by the above LDA model using pyLDAvis library. pyLDAvis provides an amazing interactive visualization tool to see how different clusters are generated. It produces the intertopic distance map and shows top relevant terms for each topic amongst other features. \n",
    "\n",
    "As mentioned above, we'll visualize the intertopic distance map to see if there is a good segregation of clusters. If there is an overlap between multiple clusters, we'd reduce the number of topics and run the LDA model with reduced number of clusters and visualize again. Once we are satisfied with the cluster segregation in the intertopic distance map, we can start looking into the terms to see what each topic represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el2179951287154724218672534\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el2179951287154724218672534_data = {\"mdsDat\": {\"x\": [0.19961726730052343, -0.10114444227867034, -0.09847282502185312], \"y\": [-0.0010792161252844818, -0.12041531533042563, 0.1214945314557101], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [38.65716552734375, 33.89848327636719, 27.444345474243164]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [69.0, 49.0, 47.0, 39.0, 39.0, 39.0, 33.0, 41.0, 40.0, 41.0, 37.0, 42.0, 44.0, 22.0, 33.0, 53.0, 58.0, 36.0, 74.0, 38.0, 17.0, 18.0, 18.0, 21.0, 37.0, 36.0, 16.0, 15.0, 26.0, 22.0, 38.57549285888672, 38.574134826660156, 38.51972198486328, 39.08568572998047, 39.58877182006836, 40.191837310791016, 44.58781433105469, 41.05835723876953, 13.655355453491211, 14.31758975982666, 13.418522834777832, 13.345067977905273, 14.82521915435791, 16.03598403930664, 48.898685455322266, 24.11419105529785, 14.6765718460083, 57.902523040771484, 19.020647048950195, 13.826069831848145, 10.937500953674316, 12.826176643371582, 19.123701095581055, 20.293621063232422, 14.079419136047363, 13.113734245300293, 17.190200805664062, 15.61044979095459, 11.572696685791016, 52.088294982910156, 20.397802352905273, 15.726219177246094, 15.221929550170898, 13.639001846313477, 13.705309867858887, 17.478891372680664, 14.695377349853516, 14.656790733337402, 14.646321296691895, 14.609508514404297, 15.498537063598633, 15.272173881530762, 31.44013786315918, 22.057722091674805, 58.368080139160156, 16.08063316345215, 20.054895401000977, 12.776440620422363, 14.75717544555664, 22.453357696533203, 15.4237699508667, 13.045330047607422, 19.71506118774414, 13.447131156921387, 19.04054832458496, 12.633013725280762, 13.24755859375, 27.916479110717773, 25.86976432800293, 18.945526123046875, 10.06675910949707, 13.704747200012207, 21.423948287963867, 10.708971977233887, 16.40898323059082, 17.130207061767578, 15.225712776184082, 29.43738555908203, 17.628707885742188, 16.56667137145996, 15.799521446228027, 14.776631355285645, 21.649940490722656, 16.573360443115234, 29.886436462402344, 14.079360961914062, 16.594308853149414, 14.653337478637695, 31.239931106567383, 17.62010383605957, 33.63153076171875, 39.888614654541016, 12.115120887756348, 25.557260513305664, 15.375232696533203, 16.705272674560547, 11.19855785369873, 12.603233337402344, 13.958622932434082, 33.73883056640625, 12.611385345458984, 27.03384780883789, 10.63567066192627, 10.744231224060059, 11.212385177612305, 17.64234161376953, 11.048873901367188, 14.053714752197266, 9.830140113830566, 6.606180667877197, 18.36005401611328, 6.363324165344238, 10.519267082214355, 10.021646499633789, 11.051831245422363], \"Term\": [\"peopl\", \"pandem\", \"natur\", \"poverti\", \"racism\", \"trut\", \"stay\", \"mask\", \"murder\", \"father\", \"home\", \"caus\", \"speak\", \"presid\", \"order\", \"health\", \"work\", \"death\", \"case\", \"keep\", \"ask\", \"global\", \"amaz\", \"worker\", \"like\", \"public\", \"import\", \"look\", \"spread\", \"doctor\", \"poverti\", \"racism\", \"trut\", \"murder\", \"father\", \"caus\", \"natur\", \"speak\", \"cast\", \"enough\", \"basi\", \"religion\", \"rich\", \"india\", \"work\", \"make\", \"either\", \"case\", \"report\", \"discrimin\", \"releas\", \"respons\", \"virus\", \"everyon\", \"poor\", \"break\", \"affect\", \"say\", \"confirm\", \"coronavirus\", \"fight\", \"patient\", \"trump\", \"govern\", \"protect\", \"amaz\", \"propaganda\", \"gullibl\", \"ceas\", \"narrat\", \"great\", \"often\", \"death\", \"spread\", \"peopl\", \"total\", \"face\", \"support\", \"die\", \"care\", \"medic\", \"crisi\", \"human\", \"updat\", \"never\", \"cover\", \"week\", \"keep\", \"like\", \"hospit\", \"recommend\", \"first\", \"today\", \"isol\", \"state\", \"time\", \"pleas\", \"coronavirus\", \"public\", \"help\", \"fight\", \"health\", \"presid\", \"ask\", \"stay\", \"look\", \"global\", \"import\", \"home\", \"worker\", \"mask\", \"pandem\", \"governor\", \"order\", \"call\", \"doctor\", \"life\", \"take\", \"nurs\", \"health\", \"send\", \"test\", \"lockdown\", \"nation\", \"equip\", \"public\", \"april\", \"trump\", \"china\", \"polic\", \"help\", \"isol\", \"keep\", \"like\", \"coronavirus\"], \"Total\": [69.0, 49.0, 47.0, 39.0, 39.0, 39.0, 33.0, 41.0, 40.0, 41.0, 37.0, 42.0, 44.0, 22.0, 33.0, 53.0, 58.0, 36.0, 74.0, 38.0, 17.0, 18.0, 18.0, 21.0, 37.0, 36.0, 16.0, 15.0, 26.0, 22.0, 39.489479064941406, 39.49093246459961, 39.49177932739258, 40.457489013671875, 41.3968620300293, 42.375614166259766, 47.04800796508789, 44.321197509765625, 15.107057571411133, 16.0283203125, 15.123391151428223, 15.127920150756836, 17.01508140563965, 18.943307876586914, 58.74215316772461, 29.275362014770508, 17.99876594543457, 74.93093872070312, 25.739501953125, 18.968873977661133, 15.166129112243652, 18.963476181030273, 28.53145980834961, 30.52812957763672, 22.07207489013672, 20.923580169677734, 27.6442928314209, 25.899776458740234, 20.196704864501953, 92.5775146484375, 41.11167907714844, 29.81240463256836, 30.998329162597656, 25.01679229736328, 26.81597137451172, 18.324399948120117, 15.433683395385742, 15.43413257598877, 15.434281349182129, 15.434202194213867, 16.399797439575195, 16.38962745666504, 36.6620979309082, 26.0560302734375, 69.3893814086914, 19.258995056152344, 24.130966186523438, 15.481045722961426, 18.291322708129883, 27.90400505065918, 19.326980590820312, 16.368179321289062, 25.200889587402344, 17.31848907470703, 25.285234451293945, 17.372329711914062, 18.49058723449707, 38.966243743896484, 37.00276184082031, 28.155048370361328, 15.4483003616333, 21.06366729736328, 33.63383102416992, 17.59070587158203, 28.1298770904541, 30.846012115478516, 27.69695472717285, 92.5775146484375, 36.33644104003906, 47.62582778930664, 41.11167907714844, 53.257755279541016, 22.99790382385254, 17.97956657409668, 33.85731506347656, 15.963956832885742, 18.936552047729492, 16.9144229888916, 37.81048583984375, 21.845115661621094, 41.75605010986328, 49.722782135009766, 15.878440856933594, 33.763885498046875, 20.790660858154297, 22.70594024658203, 15.784022331237793, 17.772708892822266, 20.768373489379883, 53.257755279541016, 21.605422973632812, 50.90328598022461, 20.647655487060547, 21.37395477294922, 22.584043502807617, 36.33644104003906, 23.322343826293945, 30.998329162597656, 22.546634674072266, 16.63755226135254, 47.62582778930664, 17.59070587158203, 38.966243743896484, 37.00276184082031, 92.5775146484375], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9269999861717224, 0.9269000291824341, 0.9254999756813049, 0.9158999919891357, 0.9057999849319458, 0.8974999785423279, 0.8967000246047974, 0.8740000128746033, 0.849399983882904, 0.8375999927520752, 0.8307999968528748, 0.824999988079071, 0.8126999735832214, 0.7838000059127808, 0.7670000195503235, 0.7565000057220459, 0.746399998664856, 0.6926000118255615, 0.6478999853134155, 0.6341999769210815, 0.6236000061035156, 0.5594000220298767, 0.5504000186920166, 0.5421000123023987, 0.5008000135421753, 0.4832000136375427, 0.47540000081062317, 0.4440999925136566, 0.3935999870300293, 0.37529999017715454, 0.24959999322891235, 0.3107999861240387, 0.23919999599456787, 0.34380000829696655, 0.2791999876499176, 1.034600019454956, 1.0327999591827393, 1.0300999879837036, 1.0293999910354614, 1.026900053024292, 1.0253000259399414, 1.011199951171875, 0.9280999898910522, 0.9151999950408936, 0.9088000059127808, 0.9014000296592712, 0.8967999815940857, 0.8898000121116638, 0.8671000003814697, 0.8644999861717224, 0.8561999797821045, 0.8549000024795532, 0.8363000154495239, 0.8288000226020813, 0.7982000112533569, 0.7631999850273132, 0.7483999729156494, 0.7483000159263611, 0.7239000201225281, 0.6855999827384949, 0.6535000205039978, 0.6520000100135803, 0.6308000087738037, 0.5855000019073486, 0.5428000092506409, 0.4936000108718872, 0.48350000381469727, -0.06400000303983688, 0.35850000381469727, 0.025800000876188278, 0.12549999356269836, -0.20029999315738678, 1.2325999736785889, 1.2115999460220337, 1.1683000326156616, 1.1674000024795532, 1.1610000133514404, 1.1495000123977661, 1.1021000146865845, 1.0780999660491943, 1.0765999555587769, 1.07260000705719, 1.0225000381469727, 1.0145000219345093, 0.9912999868392944, 0.9861000180244446, 0.9498000144958496, 0.9492999911308289, 0.8956999778747559, 0.8364999890327454, 0.7547000050544739, 0.6601999998092651, 0.6295999884605408, 0.6051999926567078, 0.5928000211715698, 0.5705000162124634, 0.5458999872207642, 0.5019999742507935, 0.4629000127315521, 0.3693999946117401, 0.33980000019073486, 0.27619999647140503, -0.016499999910593033, -0.013199999928474426, -0.8324000239372253], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.328200101852417, -3.3282999992370605, -3.329699993133545, -3.3150999546051025, -3.302299976348877, -3.2871999740600586, -3.1833999156951904, -3.265899896621704, -4.366700172424316, -4.319399833679199, -4.384200096130371, -4.389699935913086, -4.2845001220703125, -4.205999851226807, -3.091099977493286, -3.7980000972747803, -4.294600009918213, -2.922100067138672, -4.035299777984619, -4.354300022125244, -4.588600158691406, -4.4293999671936035, -4.029900074005127, -3.9704999923706055, -4.336100101470947, -4.407199859619141, -4.136499881744385, -4.232900142669678, -4.532199859619141, -3.027899980545044, -3.965399980545044, -4.225500106811523, -4.2581000328063965, -4.3678998947143555, -4.363100051879883, -3.9885001182556152, -4.1620001792907715, -4.164599895477295, -4.165299892425537, -4.167799949645996, -4.108699798583984, -4.123499870300293, -3.401400089263916, -3.7558000087738037, -2.7827000617980957, -4.071899890899658, -3.8510000705718994, -4.3018999099731445, -4.157800197601318, -3.73799991607666, -4.11359977722168, -4.281099796295166, -3.8680999279022217, -4.250699996948242, -3.902899980545044, -4.313199996948242, -4.265699863433838, -3.5202999114990234, -3.596400022506714, -3.907900094985962, -4.540200233459473, -4.2316999435424805, -3.7850000858306885, -4.478400230407715, -4.051700115203857, -4.008600234985352, -4.126500129699707, -3.4672000408172607, -3.9800000190734863, -4.042099952697754, -4.0894999504089355, -4.156400203704834, -3.5632998943328857, -3.8304998874664307, -3.2409000396728516, -3.9935998916625977, -3.829200029373169, -3.9535999298095703, -3.1965999603271484, -3.769200086593628, -3.12280011177063, -2.952199935913086, -4.143799781799316, -3.39739990234375, -3.9054999351501465, -3.8225998878479004, -4.222499847412109, -4.104300022125244, -4.002200126647949, -3.1196000576019287, -4.103700160980225, -3.341200113296509, -4.274099826812744, -4.263899803161621, -4.22130012512207, -3.7679998874664307, -4.235899925231934, -3.9953999519348145, -4.352799892425537, -4.75029993057251, -3.728100061416626, -4.787700176239014, -4.285099983215332, -4.333499908447266, -4.2357001304626465]}, \"token.table\": {\"Topic\": [1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 2, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 1, 3, 2, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \"Freq\": [0.6149551272392273, 0.25321683287620544, 0.14469532668590546, 0.9277247786521912, 0.3858960270881653, 0.12863200902938843, 0.47165071964263916, 0.05561869218945503, 0.05561869218945503, 0.9455177783966064, 0.8595955967903137, 0.06612273305654526, 0.6213085651397705, 0.28675779700279236, 0.09558593481779099, 0.09619703888893127, 0.1442955583333969, 0.7214778065681458, 0.1433486044406891, 0.7884172797203064, 0.03583715111017227, 0.7740460634231567, 0.12011060118675232, 0.10676497966051102, 0.9267191886901855, 0.06619422882795334, 0.9439391493797302, 0.023598477244377136, 0.023598477244377136, 0.9718625545501709, 0.044352516531944275, 0.5322301983833313, 0.44352516531944275, 0.5941563248634338, 0.09902605414390564, 0.34659120440483093, 0.5616914629936218, 0.3132510185241699, 0.11881934851408005, 0.17268840968608856, 0.7483164668083191, 0.11512560397386551, 0.12218829989433289, 0.794223964214325, 0.06109414994716644, 0.08182837814092636, 0.8455598950386047, 0.05455225333571434, 0.16401219367980957, 0.8200609683990479, 0.05467073246836662, 0.738051176071167, 0.21087177097797394, 0.052717942744493484, 0.22020669281482697, 0.04404133930802345, 0.7487027645111084, 0.833390474319458, 0.11111872643232346, 0.11111872643232346, 0.8734539747238159, 0.062389567494392395, 0.04427905008196831, 0.44279050827026367, 0.4870695471763611, 0.6551334857940674, 0.16378337144851685, 0.16378337144851685, 0.08288105577230453, 0.8288105726242065, 0.08288105577230453, 0.9662567973136902, 0.024156419560313225, 0.024156419560313225, 0.4864797592163086, 0.38918381929397583, 0.12161993980407715, 0.3323257863521576, 0.6646515727043152, 0.04747511446475983, 0.052807923406362534, 0.052807923406362534, 0.8977347016334534, 0.5596240758895874, 0.11991944909095764, 0.3197852075099945, 0.06297847628593445, 0.18893542885780334, 0.7557417154312134, 0.9146454334259033, 0.9718719124794006, 0.09388303756713867, 0.281649112701416, 0.6384046673774719, 0.2729611396789551, 0.35694918036460876, 0.3779461979866028, 0.052895378321409225, 0.1322384476661682, 0.8198783993721008, 0.10655282437801361, 0.6748345494270325, 0.24862326681613922, 0.03968114033341408, 0.7936227917671204, 0.1587245613336563, 0.1182422861456871, 0.8868170976638794, 0.8446254730224609, 0.05278909206390381, 0.10557818412780762, 0.056848201900720596, 0.6253302097320557, 0.3410892188549042, 0.025663238018751144, 0.7185706496238708, 0.2822956144809723, 0.19006562232971191, 0.1267104148864746, 0.6969072818756104, 0.0270250104367733, 0.702650249004364, 0.2702501118183136, 0.048431649804115295, 0.43588483333587646, 0.5327481627464294, 0.06264111399650574, 0.8769755363464355, 0.8198019862174988, 0.1707920879125595, 0.03415841609239578, 0.04789724946022034, 0.143691748380661, 0.8142532706260681, 0.10348227620124817, 0.7761170864105225, 0.10348227620124817, 0.9639747738838196, 0.02471730299293995, 0.971867561340332, 0.467859148979187, 0.04678591340780258, 0.5146450400352478, 0.9564698338508606, 0.0425097718834877, 0.02125488594174385, 0.7514266967773438, 0.23729263246059418, 0.048150137066841125, 0.28890082240104675, 0.6741018891334534, 0.06101419776678085, 0.9152129888534546, 0.029617443680763245, 0.23693954944610596, 0.7700535655021667, 0.04022300988435745, 0.1608920395374298, 0.8044602274894714, 0.5366893410682678, 0.1677154153585434, 0.30188775062561035, 0.10087998956441879, 0.8358627557754517, 0.057645708322525024, 0.3971555829048157, 0.5415757894515991, 0.03610505163669586, 0.6010499596595764, 0.4207349717617035, 0.6342856287956238, 0.045306116342544556, 0.3171428143978119, 0.9876047968864441, 0.0434822253882885, 0.9566089510917664, 0.971900224685669, 0.5220769047737122, 0.2983296811580658, 0.18645603954792023, 0.02752058207988739, 0.495370477437973, 0.495370477437973, 0.9875684976577759, 0.19419611990451813, 0.6473203897476196, 0.1294640749692917, 0.7253004312515259, 0.19780920445919037, 0.06593640148639679, 0.8593382239341736, 0.06610293686389923, 0.738165020942688, 0.03885079175233841, 0.19425395131111145, 0.6855283379554749, 0.31639769673347473, 0.05273294821381569, 0.8815708756446838, 0.058771390467882156, 0.058771390467882156, 0.6177659630775452, 0.11583111435174942, 0.2702726125717163, 0.18513870239257812, 0.23142337799072266, 0.6017007827758789, 0.925065279006958, 0.02256256714463234, 0.04512513428926468, 0.07675766199827194, 0.8443343043327332, 0.07675766199827194, 0.14219756424427032, 0.5687902569770813, 0.24884574115276337, 0.08860714733600616, 0.02953571453690529, 0.8860714435577393, 0.06459511816501617, 0.8397365808486938, 0.12919023633003235, 0.16879813373088837, 0.11253208667039871, 0.7314585447311401, 0.21609607338905334, 0.25538626313209534, 0.5304176211357117, 0.25935280323028564, 0.5511247515678406, 0.16209550201892853, 0.26758772134780884, 0.624371349811554, 0.08919590711593628, 0.10384757816791534, 0.8307806253433228, 0.05192378908395767, 0.48389706015586853, 0.06451960653066635, 0.45163723826408386, 0.9875472784042358, 0.025321725755929947, 0.1732252687215805, 0.7506428360939026, 0.05774175748229027, 0.6659315824508667, 0.24534320831298828, 0.1051470935344696, 0.054081570357084274, 0.7030604481697083, 0.27040785551071167, 0.834153950214386, 0.051070649176836014, 0.1191648542881012, 0.09155364334583282, 0.09155364334583282, 0.823982834815979], \"Term\": [\"affect\", \"affect\", \"affect\", \"amaz\", \"april\", \"april\", \"april\", \"ask\", \"ask\", \"ask\", \"basi\", \"basi\", \"break\", \"break\", \"break\", \"call\", \"call\", \"call\", \"care\", \"care\", \"care\", \"case\", \"case\", \"case\", \"cast\", \"cast\", \"caus\", \"caus\", \"caus\", \"ceas\", \"china\", \"china\", \"china\", \"confirm\", \"confirm\", \"confirm\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"cover\", \"cover\", \"cover\", \"crisi\", \"crisi\", \"crisi\", \"death\", \"death\", \"death\", \"die\", \"die\", \"die\", \"discrimin\", \"discrimin\", \"discrimin\", \"doctor\", \"doctor\", \"doctor\", \"either\", \"either\", \"either\", \"enough\", \"enough\", \"equip\", \"equip\", \"equip\", \"everyon\", \"everyon\", \"everyon\", \"face\", \"face\", \"face\", \"father\", \"father\", \"father\", \"fight\", \"fight\", \"fight\", \"first\", \"first\", \"first\", \"global\", \"global\", \"global\", \"govern\", \"govern\", \"govern\", \"governor\", \"governor\", \"governor\", \"great\", \"gullibl\", \"health\", \"health\", \"health\", \"help\", \"help\", \"help\", \"home\", \"home\", \"home\", \"hospit\", \"hospit\", \"hospit\", \"human\", \"human\", \"human\", \"import\", \"import\", \"india\", \"india\", \"india\", \"isol\", \"isol\", \"isol\", \"keep\", \"keep\", \"keep\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"lockdown\", \"lockdown\", \"lockdown\", \"look\", \"look\", \"make\", \"make\", \"make\", \"mask\", \"mask\", \"mask\", \"medic\", \"medic\", \"medic\", \"murder\", \"murder\", \"narrat\", \"nation\", \"nation\", \"nation\", \"natur\", \"natur\", \"natur\", \"never\", \"never\", \"nurs\", \"nurs\", \"nurs\", \"often\", \"often\", \"order\", \"order\", \"order\", \"pandem\", \"pandem\", \"pandem\", \"patient\", \"patient\", \"patient\", \"peopl\", \"peopl\", \"peopl\", \"pleas\", \"pleas\", \"pleas\", \"polic\", \"polic\", \"poor\", \"poor\", \"poor\", \"poverti\", \"presid\", \"presid\", \"propaganda\", \"protect\", \"protect\", \"protect\", \"public\", \"public\", \"public\", \"racism\", \"recommend\", \"recommend\", \"recommend\", \"releas\", \"releas\", \"releas\", \"religion\", \"religion\", \"report\", \"report\", \"report\", \"respons\", \"respons\", \"respons\", \"rich\", \"rich\", \"rich\", \"say\", \"say\", \"say\", \"send\", \"send\", \"send\", \"speak\", \"speak\", \"speak\", \"spread\", \"spread\", \"spread\", \"state\", \"state\", \"state\", \"stay\", \"stay\", \"stay\", \"support\", \"support\", \"support\", \"take\", \"take\", \"take\", \"test\", \"test\", \"test\", \"time\", \"time\", \"time\", \"today\", \"today\", \"today\", \"total\", \"total\", \"total\", \"trump\", \"trump\", \"trump\", \"trut\", \"trut\", \"updat\", \"updat\", \"updat\", \"virus\", \"virus\", \"virus\", \"week\", \"week\", \"week\", \"work\", \"work\", \"work\", \"worker\", \"worker\", \"worker\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el2179951287154724218672534\", ldavis_el2179951287154724218672534_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el2179951287154724218672534\", ldavis_el2179951287154724218672534_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el2179951287154724218672534\", ldavis_el2179951287154724218672534_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1      0.199617 -0.001079       1        1  38.657166\n",
       "0     -0.101144 -0.120415       2        1  33.898483\n",
       "2     -0.098473  0.121495       3        1  27.444345, topic_info=    Category       Freq         Term      Total  loglift  logprob\n",
       "20   Default  69.000000        peopl  69.000000  30.0000  30.0000\n",
       "14   Default  49.000000       pandem  49.000000  29.0000  29.0000\n",
       "34   Default  47.000000        natur  47.000000  28.0000  28.0000\n",
       "35   Default  39.000000      poverti  39.000000  27.0000  27.0000\n",
       "36   Default  39.000000       racism  39.000000  26.0000  26.0000\n",
       "38   Default  39.000000         trut  39.000000  25.0000  25.0000\n",
       "4    Default  33.000000         stay  33.000000  24.0000  24.0000\n",
       "22   Default  41.000000         mask  41.000000  23.0000  23.0000\n",
       "33   Default  40.000000       murder  40.000000  22.0000  22.0000\n",
       "32   Default  41.000000       father  41.000000  21.0000  21.0000\n",
       "1    Default  37.000000         home  37.000000  20.0000  20.0000\n",
       "31   Default  42.000000         caus  42.000000  19.0000  19.0000\n",
       "37   Default  44.000000        speak  44.000000  18.0000  18.0000\n",
       "95   Default  22.000000       presid  22.000000  17.0000  17.0000\n",
       "2    Default  33.000000        order  33.000000  16.0000  16.0000\n",
       "0    Default  53.000000       health  53.000000  15.0000  15.0000\n",
       "39   Default  58.000000         work  58.000000  14.0000  14.0000\n",
       "42   Default  36.000000        death  36.000000  13.0000  13.0000\n",
       "48   Default  74.000000         case  74.000000  12.0000  12.0000\n",
       "7    Default  38.000000         keep  38.000000  11.0000  11.0000\n",
       "84   Default  17.000000          ask  17.000000  10.0000  10.0000\n",
       "13   Default  18.000000       global  18.000000   9.0000   9.0000\n",
       "74   Default  18.000000         amaz  18.000000   8.0000   8.0000\n",
       "70   Default  21.000000       worker  21.000000   7.0000   7.0000\n",
       "27   Default  37.000000         like  37.000000   6.0000   6.0000\n",
       "15   Default  36.000000       public  36.000000   5.0000   5.0000\n",
       "47   Default  16.000000       import  16.000000   4.0000   4.0000\n",
       "73   Default  15.000000         look  15.000000   3.0000   3.0000\n",
       "9    Default  26.000000       spread  26.000000   2.0000   2.0000\n",
       "12   Default  22.000000       doctor  22.000000   1.0000   1.0000\n",
       "35    Topic1  38.575493      poverti  39.489479   0.9270  -3.3282\n",
       "36    Topic1  38.574135       racism  39.490932   0.9269  -3.3283\n",
       "38    Topic1  38.519722         trut  39.491779   0.9255  -3.3297\n",
       "33    Topic1  39.085686       murder  40.457489   0.9159  -3.3151\n",
       "32    Topic1  39.588772       father  41.396862   0.9058  -3.3023\n",
       "31    Topic1  40.191837         caus  42.375614   0.8975  -3.2872\n",
       "34    Topic1  44.587814        natur  47.048008   0.8967  -3.1834\n",
       "37    Topic1  41.058357        speak  44.321198   0.8740  -3.2659\n",
       "59    Topic1  13.655355         cast  15.107058   0.8494  -4.3667\n",
       "50    Topic1  14.317590       enough  16.028320   0.8376  -4.3194\n",
       "58    Topic1  13.418523         basi  15.123391   0.8308  -4.3842\n",
       "64    Topic1  13.345068     religion  15.127920   0.8250  -4.3897\n",
       "17    Topic1  14.825219         rich  17.015081   0.8127  -4.2845\n",
       "66    Topic1  16.035984        india  18.943308   0.7838  -4.2060\n",
       "39    Topic1  48.898685         work  58.742153   0.7670  -3.0911\n",
       "67    Topic1  24.114191         make  29.275362   0.7565  -3.7980\n",
       "61    Topic1  14.676572       either  17.998766   0.7464  -4.2946\n",
       "48    Topic1  57.902523         case  74.930939   0.6926  -2.9221\n",
       "43    Topic1  19.020647       report  25.739502   0.6479  -4.0353\n",
       "60    Topic1  13.826070    discrimin  18.968874   0.6342  -4.3543\n",
       "100   Topic1  10.937501       releas  15.166129   0.6236  -4.5886\n",
       "57    Topic1  12.826177      respons  18.963476   0.5594  -4.4294\n",
       "68    Topic1  19.123701        virus  28.531460   0.5504  -4.0299\n",
       "62    Topic1  20.293621      everyon  30.528130   0.5421  -3.9705\n",
       "63    Topic1  14.079419         poor  22.072075   0.5008  -4.3361\n",
       "86    Topic1  13.113734        break  20.923580   0.4832  -4.4072\n",
       "52    Topic1  17.190201       affect  27.644293   0.4754  -4.1365\n",
       "10    Topic1  15.610450          say  25.899776   0.4441  -4.2329\n",
       "93    Topic1  11.572697      confirm  20.196705   0.3936  -4.5322\n",
       "5     Topic1  52.088295  coronavirus  92.577515   0.3753  -3.0279\n",
       "65    Topic1  20.397802        fight  41.111679   0.2496  -3.9654\n",
       "45    Topic1  15.726219      patient  29.812405   0.3108  -4.2255\n",
       "98    Topic1  15.221930        trump  30.998329   0.2392  -4.2581\n",
       "40    Topic1  13.639002       govern  25.016792   0.3438  -4.3679\n",
       "89    Topic1  13.705310      protect  26.815971   0.2792  -4.3631\n",
       "74    Topic2  17.478891         amaz  18.324400   1.0346  -3.9885\n",
       "79    Topic2  14.695377   propaganda  15.433683   1.0328  -4.1620\n",
       "76    Topic2  14.656791      gullibl  15.434133   1.0301  -4.1646\n",
       "75    Topic2  14.646321         ceas  15.434281   1.0294  -4.1653\n",
       "77    Topic2  14.609509       narrat  15.434202   1.0269  -4.1678\n",
       "69    Topic2  15.498537        great  16.399797   1.0253  -4.1087\n",
       "21    Topic2  15.272174        often  16.389627   1.0112  -4.1235\n",
       "42    Topic2  31.440138        death  36.662098   0.9281  -3.4014\n",
       "9     Topic2  22.057722       spread  26.056030   0.9152  -3.7558\n",
       "20    Topic2  58.368080        peopl  69.389381   0.9088  -2.7827\n",
       "87    Topic2  16.080633        total  19.258995   0.9014  -4.0719\n",
       "94    Topic2  20.054895         face  24.130966   0.8968  -3.8510\n",
       "30    Topic2  12.776441      support  15.481046   0.8898  -4.3019\n",
       "83    Topic2  14.757175          die  18.291323   0.8671  -4.1578\n",
       "44    Topic2  22.453358         care  27.904005   0.8645  -3.7380\n",
       "29    Topic2  15.423770        medic  19.326981   0.8562  -4.1136\n",
       "54    Topic2  13.045330        crisi  16.368179   0.8549  -4.2811\n",
       "16    Topic2  19.715061        human  25.200890   0.8363  -3.8681\n",
       "88    Topic2  13.447131        updat  17.318489   0.8288  -4.2507\n",
       "78    Topic2  19.040548        never  25.285234   0.7982  -3.9029\n",
       "99    Topic2  12.633014        cover  17.372330   0.7632  -4.3132\n",
       "80    Topic2  13.247559         week  18.490587   0.7484  -4.2657\n",
       "7     Topic2  27.916479         keep  38.966244   0.7483  -3.5203\n",
       "27    Topic2  25.869764         like  37.002762   0.7239  -3.5964\n",
       "19    Topic2  18.945526       hospit  28.155048   0.6856  -3.9079\n",
       "91    Topic2  10.066759    recommend  15.448300   0.6535  -4.5402\n",
       "18    Topic2  13.704747        first  21.063667   0.6520  -4.2317\n",
       "28    Topic2  21.423948        today  33.633831   0.6308  -3.7850\n",
       "41    Topic2  10.708972         isol  17.590706   0.5855  -4.4784\n",
       "3     Topic2  16.408983        state  28.129877   0.5428  -4.0517\n",
       "46    Topic2  17.130207         time  30.846012   0.4936  -4.0086\n",
       "81    Topic2  15.225713        pleas  27.696955   0.4835  -4.1265\n",
       "5     Topic2  29.437386  coronavirus  92.577515  -0.0640  -3.4672\n",
       "15    Topic2  17.628708       public  36.336441   0.3585  -3.9800\n",
       "6     Topic2  16.566671         help  47.625828   0.0258  -4.0421\n",
       "65    Topic2  15.799521        fight  41.111679   0.1255  -4.0895\n",
       "0     Topic2  14.776631       health  53.257755  -0.2003  -4.1564\n",
       "95    Topic3  21.649940       presid  22.997904   1.2326  -3.5633\n",
       "84    Topic3  16.573360          ask  17.979567   1.2116  -3.8305\n",
       "4     Topic3  29.886436         stay  33.857315   1.1683  -3.2409\n",
       "73    Topic3  14.079361         look  15.963957   1.1674  -3.9936\n",
       "13    Topic3  16.594309       global  18.936552   1.1610  -3.8292\n",
       "47    Topic3  14.653337       import  16.914423   1.1495  -3.9536\n",
       "1     Topic3  31.239931         home  37.810486   1.1021  -3.1966\n",
       "70    Topic3  17.620104       worker  21.845116   1.0781  -3.7692\n",
       "22    Topic3  33.631531         mask  41.756050   1.0766  -3.1228\n",
       "14    Topic3  39.888615       pandem  49.722782   1.0726  -2.9522\n",
       "92    Topic3  12.115121     governor  15.878441   1.0225  -4.1438\n",
       "2     Topic3  25.557261        order  33.763885   1.0145  -3.3974\n",
       "85    Topic3  15.375233         call  20.790661   0.9913  -3.9055\n",
       "12    Topic3  16.705273       doctor  22.705940   0.9861  -3.8226\n",
       "53    Topic3  11.198558         life  15.784022   0.9498  -4.2225\n",
       "101   Topic3  12.603233         take  17.772709   0.9493  -4.1043\n",
       "8     Topic3  13.958623         nurs  20.768373   0.8957  -4.0022\n",
       "0     Topic3  33.738831       health  53.257755   0.8365  -3.1196\n",
       "23    Topic3  12.611385         send  21.605423   0.7547  -4.1037\n",
       "11    Topic3  27.033848         test  50.903286   0.6602  -3.3412\n",
       "25    Topic3  10.635671     lockdown  20.647655   0.6296  -4.2741\n",
       "26    Topic3  10.744231       nation  21.373955   0.6052  -4.2639\n",
       "72    Topic3  11.212385        equip  22.584044   0.5928  -4.2213\n",
       "15    Topic3  17.642342       public  36.336441   0.5705  -3.7680\n",
       "24    Topic3  11.048874        april  23.322344   0.5459  -4.2359\n",
       "98    Topic3  14.053715        trump  30.998329   0.5020  -3.9954\n",
       "55    Topic3   9.830140        china  22.546635   0.4629  -4.3528\n",
       "90    Topic3   6.606181        polic  16.637552   0.3694  -4.7503\n",
       "6     Topic3  18.360054         help  47.625828   0.3398  -3.7281\n",
       "41    Topic3   6.363324         isol  17.590706   0.2762  -4.7877\n",
       "7     Topic3  10.519267         keep  38.966244  -0.0165  -4.2851\n",
       "27    Topic3  10.021646         like  37.002762  -0.0132  -4.3335\n",
       "5     Topic3  11.051831  coronavirus  92.577515  -0.8324  -4.2357, token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "52        1  0.614955       affect\n",
       "52        2  0.253217       affect\n",
       "52        3  0.144695       affect\n",
       "74        2  0.927725         amaz\n",
       "24        1  0.385896        april\n",
       "24        2  0.128632        april\n",
       "24        3  0.471651        april\n",
       "84        1  0.055619          ask\n",
       "84        2  0.055619          ask\n",
       "84        3  0.945518          ask\n",
       "58        1  0.859596         basi\n",
       "58        3  0.066123         basi\n",
       "86        1  0.621309        break\n",
       "86        2  0.286758        break\n",
       "86        3  0.095586        break\n",
       "85        1  0.096197         call\n",
       "85        2  0.144296         call\n",
       "85        3  0.721478         call\n",
       "44        1  0.143349         care\n",
       "44        2  0.788417         care\n",
       "44        3  0.035837         care\n",
       "48        1  0.774046         case\n",
       "48        2  0.120111         case\n",
       "48        3  0.106765         case\n",
       "59        1  0.926719         cast\n",
       "59        3  0.066194         cast\n",
       "31        1  0.943939         caus\n",
       "31        2  0.023598         caus\n",
       "31        3  0.023598         caus\n",
       "75        2  0.971863         ceas\n",
       "55        1  0.044353        china\n",
       "55        2  0.532230        china\n",
       "55        3  0.443525        china\n",
       "93        1  0.594156      confirm\n",
       "93        2  0.099026      confirm\n",
       "93        3  0.346591      confirm\n",
       "5         1  0.561691  coronavirus\n",
       "5         2  0.313251  coronavirus\n",
       "5         3  0.118819  coronavirus\n",
       "99        1  0.172688        cover\n",
       "99        2  0.748316        cover\n",
       "99        3  0.115126        cover\n",
       "54        1  0.122188        crisi\n",
       "54        2  0.794224        crisi\n",
       "54        3  0.061094        crisi\n",
       "42        1  0.081828        death\n",
       "42        2  0.845560        death\n",
       "42        3  0.054552        death\n",
       "83        1  0.164012          die\n",
       "83        2  0.820061          die\n",
       "83        3  0.054671          die\n",
       "60        1  0.738051    discrimin\n",
       "60        2  0.210872    discrimin\n",
       "60        3  0.052718    discrimin\n",
       "12        1  0.220207       doctor\n",
       "12        2  0.044041       doctor\n",
       "12        3  0.748703       doctor\n",
       "61        1  0.833390       either\n",
       "61        2  0.111119       either\n",
       "61        3  0.111119       either\n",
       "50        1  0.873454       enough\n",
       "50        2  0.062390       enough\n",
       "72        1  0.044279        equip\n",
       "72        2  0.442791        equip\n",
       "72        3  0.487070        equip\n",
       "62        1  0.655133      everyon\n",
       "62        2  0.163783      everyon\n",
       "62        3  0.163783      everyon\n",
       "94        1  0.082881         face\n",
       "94        2  0.828811         face\n",
       "94        3  0.082881         face\n",
       "32        1  0.966257       father\n",
       "32        2  0.024156       father\n",
       "32        3  0.024156       father\n",
       "65        1  0.486480        fight\n",
       "65        2  0.389184        fight\n",
       "65        3  0.121620        fight\n",
       "18        1  0.332326        first\n",
       "18        2  0.664652        first\n",
       "18        3  0.047475        first\n",
       "13        1  0.052808       global\n",
       "13        2  0.052808       global\n",
       "13        3  0.897735       global\n",
       "40        1  0.559624       govern\n",
       "40        2  0.119919       govern\n",
       "40        3  0.319785       govern\n",
       "92        1  0.062978     governor\n",
       "92        2  0.188935     governor\n",
       "92        3  0.755742     governor\n",
       "69        2  0.914645        great\n",
       "76        2  0.971872      gullibl\n",
       "0         1  0.093883       health\n",
       "0         2  0.281649       health\n",
       "0         3  0.638405       health\n",
       "6         1  0.272961         help\n",
       "6         2  0.356949         help\n",
       "6         3  0.377946         help\n",
       "1         1  0.052895         home\n",
       "1         2  0.132238         home\n",
       "1         3  0.819878         home\n",
       "19        1  0.106553       hospit\n",
       "19        2  0.674835       hospit\n",
       "19        3  0.248623       hospit\n",
       "16        1  0.039681        human\n",
       "16        2  0.793623        human\n",
       "16        3  0.158725        human\n",
       "47        1  0.118242       import\n",
       "47        3  0.886817       import\n",
       "66        1  0.844625        india\n",
       "66        2  0.052789        india\n",
       "66        3  0.105578        india\n",
       "41        1  0.056848         isol\n",
       "41        2  0.625330         isol\n",
       "41        3  0.341089         isol\n",
       "7         1  0.025663         keep\n",
       "7         2  0.718571         keep\n",
       "7         3  0.282296         keep\n",
       "53        1  0.190066         life\n",
       "53        2  0.126710         life\n",
       "53        3  0.696907         life\n",
       "27        1  0.027025         like\n",
       "27        2  0.702650         like\n",
       "27        3  0.270250         like\n",
       "25        1  0.048432     lockdown\n",
       "25        2  0.435885     lockdown\n",
       "25        3  0.532748     lockdown\n",
       "73        2  0.062641         look\n",
       "73        3  0.876976         look\n",
       "67        1  0.819802         make\n",
       "67        2  0.170792         make\n",
       "67        3  0.034158         make\n",
       "22        1  0.047897         mask\n",
       "22        2  0.143692         mask\n",
       "22        3  0.814253         mask\n",
       "29        1  0.103482        medic\n",
       "29        2  0.776117        medic\n",
       "29        3  0.103482        medic\n",
       "33        1  0.963975       murder\n",
       "33        3  0.024717       murder\n",
       "77        2  0.971868       narrat\n",
       "26        1  0.467859       nation\n",
       "26        2  0.046786       nation\n",
       "26        3  0.514645       nation\n",
       "34        1  0.956470        natur\n",
       "34        2  0.042510        natur\n",
       "34        3  0.021255        natur\n",
       "78        2  0.751427        never\n",
       "78        3  0.237293        never\n",
       "8         1  0.048150         nurs\n",
       "8         2  0.288901         nurs\n",
       "8         3  0.674102         nurs\n",
       "21        1  0.061014        often\n",
       "21        2  0.915213        often\n",
       "2         1  0.029617        order\n",
       "2         2  0.236940        order\n",
       "2         3  0.770054        order\n",
       "14        1  0.040223       pandem\n",
       "14        2  0.160892       pandem\n",
       "14        3  0.804460       pandem\n",
       "45        1  0.536689      patient\n",
       "45        2  0.167715      patient\n",
       "45        3  0.301888      patient\n",
       "20        1  0.100880        peopl\n",
       "20        2  0.835863        peopl\n",
       "20        3  0.057646        peopl\n",
       "81        1  0.397156        pleas\n",
       "81        2  0.541576        pleas\n",
       "81        3  0.036105        pleas\n",
       "90        2  0.601050        polic\n",
       "90        3  0.420735        polic\n",
       "63        1  0.634286         poor\n",
       "63        2  0.045306         poor\n",
       "63        3  0.317143         poor\n",
       "35        1  0.987605      poverti\n",
       "95        1  0.043482       presid\n",
       "95        3  0.956609       presid\n",
       "79        2  0.971900   propaganda\n",
       "89        1  0.522077      protect\n",
       "89        2  0.298330      protect\n",
       "89        3  0.186456      protect\n",
       "15        1  0.027521       public\n",
       "15        2  0.495370       public\n",
       "15        3  0.495370       public\n",
       "36        1  0.987568       racism\n",
       "91        1  0.194196    recommend\n",
       "91        2  0.647320    recommend\n",
       "91        3  0.129464    recommend\n",
       "100       1  0.725300       releas\n",
       "100       2  0.197809       releas\n",
       "100       3  0.065936       releas\n",
       "64        1  0.859338     religion\n",
       "64        3  0.066103     religion\n",
       "43        1  0.738165       report\n",
       "43        2  0.038851       report\n",
       "43        3  0.194254       report\n",
       "57        1  0.685528      respons\n",
       "57        2  0.316398      respons\n",
       "57        3  0.052733      respons\n",
       "17        1  0.881571         rich\n",
       "17        2  0.058771         rich\n",
       "17        3  0.058771         rich\n",
       "10        1  0.617766          say\n",
       "10        2  0.115831          say\n",
       "10        3  0.270273          say\n",
       "23        1  0.185139         send\n",
       "23        2  0.231423         send\n",
       "23        3  0.601701         send\n",
       "37        1  0.925065        speak\n",
       "37        2  0.022563        speak\n",
       "37        3  0.045125        speak\n",
       "9         1  0.076758       spread\n",
       "9         2  0.844334       spread\n",
       "9         3  0.076758       spread\n",
       "3         1  0.142198        state\n",
       "3         2  0.568790        state\n",
       "3         3  0.248846        state\n",
       "4         1  0.088607         stay\n",
       "4         2  0.029536         stay\n",
       "4         3  0.886071         stay\n",
       "30        1  0.064595      support\n",
       "30        2  0.839737      support\n",
       "30        3  0.129190      support\n",
       "101       1  0.168798         take\n",
       "101       2  0.112532         take\n",
       "101       3  0.731459         take\n",
       "11        1  0.216096         test\n",
       "11        2  0.255386         test\n",
       "11        3  0.530418         test\n",
       "46        1  0.259353         time\n",
       "46        2  0.551125         time\n",
       "46        3  0.162096         time\n",
       "28        1  0.267588        today\n",
       "28        2  0.624371        today\n",
       "28        3  0.089196        today\n",
       "87        1  0.103848        total\n",
       "87        2  0.830781        total\n",
       "87        3  0.051924        total\n",
       "98        1  0.483897        trump\n",
       "98        2  0.064520        trump\n",
       "98        3  0.451637        trump\n",
       "38        1  0.987547         trut\n",
       "38        2  0.025322         trut\n",
       "88        1  0.173225        updat\n",
       "88        2  0.750643        updat\n",
       "88        3  0.057742        updat\n",
       "68        1  0.665932        virus\n",
       "68        2  0.245343        virus\n",
       "68        3  0.105147        virus\n",
       "80        1  0.054082         week\n",
       "80        2  0.703060         week\n",
       "80        3  0.270408         week\n",
       "39        1  0.834154         work\n",
       "39        2  0.051071         work\n",
       "39        3  0.119165         work\n",
       "70        1  0.091554       worker\n",
       "70        2  0.091554       worker\n",
       "70        3  0.823983       worker, R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the visualizations, it'd be best to create 3 clusters instead of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2: Modeling using TF-IDF\n",
    "TF-IDF intends to reflect on the importance of each word in the tweet amongst other tweets. Thus it tries to create a better model instead of using mere Term Frequency as in Bag of words model. However, for TF-IDF to work it needs to have a good size of text in each document. However, tweet is usually very small in size. Thus, most of the times each word ends up being mentioned only once. Thus, TF-IDF doesn't work better for short texts. However, let's train the model and evaluate the performance and see how does it perform.\n",
    "\n",
    "### 4.2.1. Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train lda model using corpus_tfidf\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(tfidf_corpus, \n",
    "                                             num_topics=3, \n",
    "                                             id2word = dictionary, \n",
    "                                             passes = 2, \n",
    "                                             workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.036*\"trump\" + 0.033*\"mask\" + 0.032*\"order\" + 0.032*\"death\" + 0.028*\"call\" + 0.027*\"state\" + 0.025*\"pandem\" + 0.025*\"human\" + 0.024*\"presid\" + 0.024*\"worker\"\n",
      "\n",
      "\n",
      "Topic: 1 Word: 0.057*\"coronavirus\" + 0.039*\"case\" + 0.036*\"make\" + 0.034*\"test\" + 0.034*\"health\" + 0.027*\"help\" + 0.024*\"patient\" + 0.024*\"home\" + 0.024*\"spread\" + 0.022*\"keep\"\n",
      "\n",
      "\n",
      "Topic: 2 Word: 0.054*\"peopl\" + 0.040*\"work\" + 0.032*\"like\" + 0.030*\"natur\" + 0.029*\"fight\" + 0.026*\"caus\" + 0.026*\"father\" + 0.026*\"murder\" + 0.026*\"speak\" + 0.025*\"poverti\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the words occuring in that topic and its relative weight\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is my attempt at generalizing the topics from their corresponding relevant terms.  \n",
    "Topic 0: President Trump's announcements around COVID19.   \n",
    "Topic 1: Quarantining and stopping the spread of the disease.  \n",
    "Topic 2: Impact of COVID on people\n",
    "\n",
    "### 4.2.2. Visualization using pyLDAVis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el3909550972822565047257556\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el3909550972822565047257556_data = {\"mdsDat\": {\"x\": [-0.024508919346712985, -0.12099354291244066, 0.14550246225915375], \"y\": [-0.14679536532636758, 0.0936482438316631, 0.053147121494704554], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [37.26504135131836, 33.19129180908203, 29.543668746948242]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [22.0, 34.0, 19.0, 41.0, 16.0, 13.0, 17.0, 14.0, 13.0, 13.0, 23.0, 13.0, 14.0, 18.0, 14.0, 13.0, 23.0, 15.0, 22.0, 19.0, 22.0, 10.0, 14.0, 13.0, 20.0, 12.0, 15.0, 13.0, 17.0, 31.0, 11.722426414489746, 6.691922187805176, 4.8203840255737305, 4.7825798988342285, 4.77781867980957, 20.466205596923828, 13.7684907913208, 5.42415714263916, 8.611781120300293, 19.023029327392578, 9.728036880493164, 13.314895629882812, 19.20378875732422, 5.73448371887207, 9.720009803771973, 9.872032165527344, 32.34721755981445, 8.109492301940918, 9.109094619750977, 13.374412536621094, 12.619586944580078, 7.506303787231445, 15.177083015441895, 21.761743545532227, 6.003836154937744, 8.194889068603516, 8.963255882263184, 12.561077117919922, 8.827634811401367, 8.95704460144043, 9.612785339355469, 8.787888526916504, 8.141594886779785, 7.717586517333984, 7.70773458480835, 8.337221145629883, 8.812243461608887, 13.08846664428711, 12.643603324890137, 12.603837966918945, 12.59397029876709, 14.893354415893555, 13.100122451782227, 20.034255981445312, 13.14561653137207, 12.856959342956543, 7.516140937805176, 15.806909561157227, 8.511821746826172, 27.238758087158203, 12.348856925964355, 11.809483528137207, 7.428688049316406, 14.77994155883789, 6.5725789070129395, 11.13965892791748, 5.54804801940918, 9.214778900146484, 8.402994155883789, 5.494791507720947, 6.480501174926758, 9.831814765930176, 5.585620403289795, 6.717870712280273, 9.02575397491455, 9.605059623718262, 4.258387088775635, 4.895151138305664, 7.055623531341553, 6.0695390701293945, 5.55518913269043, 6.040899753570557, 6.032197952270508, 6.028588771820068, 5.997853755950928, 6.4264140129089355, 9.450310707092285, 12.295162200927734, 11.178848266601562, 16.17645835876465, 6.637543678283691, 14.26729679107666, 8.537724494934082, 9.471317291259766, 10.92503547668457, 10.727812767028809, 9.406426429748535, 9.311819076538086, 14.140952110290527, 10.55859375, 10.372583389282227, 9.138647079467773, 11.95151138305664, 7.710203170776367, 14.797720909118652, 7.480721473693848, 8.46976089477539, 6.205231666564941, 7.800815105438232, 5.141671180725098, 7.425685405731201, 11.368743896484375, 6.882570266723633, 6.054179668426514, 6.053897380828857], \"Term\": [\"work\", \"peopl\", \"trump\", \"coronavirus\", \"natur\", \"murder\", \"order\", \"call\", \"poverti\", \"racism\", \"make\", \"trut\", \"father\", \"like\", \"caus\", \"human\", \"test\", \"speak\", \"health\", \"death\", \"mask\", \"isol\", \"presid\", \"worker\", \"fight\", \"must\", \"govern\", \"break\", \"state\", \"case\", \"break\", \"discrimin\", \"basi\", \"cast\", \"religion\", \"make\", \"patient\", \"rich\", \"confirm\", \"health\", \"nurs\", \"spread\", \"test\", \"either\", \"everyon\", \"april\", \"coronavirus\", \"mani\", \"lockdown\", \"home\", \"keep\", \"even\", \"help\", \"case\", \"poor\", \"week\", \"die\", \"take\", \"want\", \"care\", \"today\", \"public\", \"live\", \"nation\", \"think\", \"say\", \"need\", \"murder\", \"poverti\", \"racism\", \"trut\", \"natur\", \"father\", \"work\", \"caus\", \"speak\", \"watch\", \"like\", \"look\", \"peopl\", \"govern\", \"thank\", \"corona\", \"fight\", \"life\", \"pleas\", \"recommend\", \"virus\", \"first\", \"cover\", \"india\", \"stay\", \"great\", \"china\", \"time\", \"need\", \"support\", \"face\", \"pandem\", \"mask\", \"say\", \"propaganda\", \"gullibl\", \"narrat\", \"ceas\", \"often\", \"isol\", \"call\", \"human\", \"trump\", \"amaz\", \"order\", \"polic\", \"must\", \"presid\", \"worker\", \"crisi\", \"equip\", \"death\", \"send\", \"report\", \"world\", \"state\", \"never\", \"mask\", \"right\", \"medic\", \"governor\", \"protect\", \"total\", \"hospit\", \"pandem\", \"case\", \"coronavirus\", \"public\"], \"Total\": [22.0, 34.0, 19.0, 41.0, 16.0, 13.0, 17.0, 14.0, 13.0, 13.0, 23.0, 13.0, 14.0, 18.0, 14.0, 13.0, 23.0, 15.0, 22.0, 19.0, 22.0, 10.0, 14.0, 13.0, 20.0, 12.0, 15.0, 13.0, 17.0, 31.0, 13.075983047485352, 7.609565734863281, 5.5606489181518555, 5.565119743347168, 5.5671067237854, 23.93662452697754, 16.390235900878906, 6.505161762237549, 10.37927532196045, 22.952987670898438, 11.752337455749512, 16.325382232666016, 23.546783447265625, 7.231760501861572, 12.281673431396484, 12.580766677856445, 41.72907257080078, 10.521080017089844, 12.277692794799805, 18.487516403198242, 17.490751266479492, 10.562677383422852, 21.381547927856445, 31.03485870361328, 8.579297065734863, 11.982878684997559, 13.215494155883789, 18.803998947143555, 13.364988327026367, 14.51826286315918, 17.428752899169922, 15.405130386352539, 13.687643051147461, 12.663640022277832, 12.811786651611328, 16.675064086914062, 19.279884338378906, 13.953145027160645, 13.593385696411133, 13.599180221557617, 13.59582805633545, 16.199739456176758, 14.408329010009766, 22.379940032958984, 14.80748462677002, 15.275572776794434, 8.947608947753906, 18.875276565551758, 10.632332801818848, 34.34315872192383, 15.971100807189941, 15.565059661865234, 9.852266311645508, 20.487878799438477, 9.211968421936035, 16.01024055480957, 8.003750801086426, 13.508723258972168, 12.517518043518066, 8.270561218261719, 10.004476547241211, 16.58516502380371, 9.995031356811523, 12.61798095703125, 17.46601104736328, 19.279884338378906, 9.371660232543945, 11.510391235351562, 25.544841766357422, 22.824462890625, 16.675064086914062, 6.729724407196045, 6.728410720825195, 6.727960586547852, 6.723710060119629, 7.379643440246582, 10.861647605895996, 14.153755187988281, 13.16159439086914, 19.252445220947266, 7.902903079986572, 17.66545867919922, 10.697535514831543, 12.111961364746094, 14.008049011230469, 13.938796997070312, 12.648624420166016, 12.634201049804688, 19.573619842529297, 15.121789932250977, 14.967663764953613, 13.34323501586914, 17.587779998779297, 11.414414405822754, 22.824462890625, 12.172307968139648, 14.536211967468262, 10.78510856628418, 14.863704681396484, 10.604717254638672, 15.807735443115234, 25.544841766357422, 31.03485870361328, 41.72907257080078, 15.405130386352539], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8777999877929688, 0.8586000204086304, 0.8442999720573425, 0.8356000185012817, 0.8342000246047974, 0.8305000066757202, 0.8127999901771545, 0.805400013923645, 0.8004000186920166, 0.7993000149726868, 0.7980999946594238, 0.78329998254776, 0.7832000255584717, 0.7551000118255615, 0.7531999945640564, 0.744700014591217, 0.7324000000953674, 0.7268000245094299, 0.6886000037193298, 0.6633999943733215, 0.6607000231742859, 0.6455000042915344, 0.6444000005722046, 0.6322000026702881, 0.6302000284194946, 0.6071000099182129, 0.5989000201225281, 0.5835999846458435, 0.5723999738693237, 0.5041000247001648, 0.3921000063419342, 0.42579999566078186, 0.4675999879837036, 0.4918999969959259, 0.4790000021457672, 0.2939000129699707, 0.20419999957084656, 1.0389000177383423, 1.030500054359436, 1.026900053024292, 1.0262999534606934, 1.0188000202178955, 1.007699966430664, 0.9922000169754028, 0.9837999939918518, 0.9304999709129333, 0.9284999966621399, 0.9254999756813049, 0.8804000020027161, 0.8711000084877014, 0.8457000255584717, 0.8267999887466431, 0.8205000162124634, 0.7763000130653381, 0.7652999758720398, 0.7401999831199646, 0.7364000082015991, 0.7203999757766724, 0.7042999863624573, 0.6940000057220459, 0.6686000227928162, 0.5799999833106995, 0.5210000276565552, 0.4724999964237213, 0.44269999861717224, 0.40610000491142273, 0.3140999972820282, 0.24789999425411224, -0.18369999527931213, -0.22169999778270721, 0.003700000001117587, 1.111299991607666, 1.1101000308990479, 1.1095000505447388, 1.1051000356674194, 1.0809999704360962, 1.0801000595092773, 1.0785000324249268, 1.055999994277954, 1.045199990272522, 1.044800043106079, 1.0056999921798706, 0.9937999844551086, 0.9733999967575073, 0.9707000255584717, 0.9574999809265137, 0.9230999946594238, 0.9142000079154968, 0.8942000269889832, 0.8600999712944031, 0.8525999784469604, 0.8407999873161316, 0.8330000042915344, 0.8270000219345093, 0.7858999967575073, 0.7325000166893005, 0.6791999936103821, 0.6664999723434448, 0.5745999813079834, 0.49540001153945923, 0.46369999647140503, 0.4097000062465668, -0.28679999709129333, -0.7110999822616577, 0.28529998660087585], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.872299909591675, -4.4328999519348145, -4.761000156402588, -4.768799781799316, -4.769800186157227, -3.3150999546051025, -3.711400032043457, -4.64300012588501, -4.180699825286865, -3.388200044631958, -4.058800220489502, -3.7448999881744385, -3.378700017929077, -4.587299823760986, -4.059599876403809, -4.044099807739258, -2.857300043106079, -4.240799903869629, -4.124599933624268, -3.740499973297119, -3.7985999584198, -4.3180999755859375, -3.614000082015991, -3.253700017929077, -4.541399955749512, -4.230299949645996, -4.140699863433838, -3.8032000064849854, -4.155900001525879, -4.14139986038208, -4.070700168609619, -4.1605000495910645, -4.236800193786621, -4.290299892425537, -4.291600227355957, -4.213099956512451, -4.157700061798096, -3.6463000774383545, -3.6809000968933105, -3.6840999126434326, -3.684799909591675, -3.5171000957489014, -3.645400047302246, -3.220599889755249, -3.6419999599456787, -3.6642000675201416, -4.201000213623047, -3.4576001167297363, -4.076600074768066, -2.9133999347686768, -3.7044999599456787, -3.7492001056671143, -4.212699890136719, -3.5248000621795654, -4.33519983291626, -3.807499885559082, -4.5046000480651855, -3.9972000122070312, -4.0894999504089355, -4.5142998695373535, -4.349299907684326, -3.9323999881744385, -4.497900009155273, -4.313300132751465, -4.01800012588501, -3.9558000564575195, -4.769199848175049, -4.629799842834473, -4.264200210571289, -4.41480016708374, -4.503300189971924, -4.303100109100342, -4.304500102996826, -4.305099964141846, -4.310200214385986, -4.241199970245361, -3.855600118637085, -3.592400074005127, -3.6875998973846436, -3.3180999755859375, -4.208899974822998, -3.443700075149536, -3.9570999145507812, -3.8533999919891357, -3.710599899291992, -3.728800058364868, -3.8601999282836914, -3.8703999519348145, -3.4526000022888184, -3.7446999549865723, -3.762500047683716, -3.8891000747680664, -3.620800018310547, -4.059100151062012, -3.4072000980377197, -4.089300155639648, -3.965100049972534, -4.276199817657471, -4.047399997711182, -4.464300155639648, -4.096700191497803, -3.670799970626831, -4.172599792480469, -4.300899982452393, -4.300899982452393]}, \"token.table\": {\"Topic\": [1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 2, 3, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \"Freq\": [0.12653577327728271, 0.12653577327728271, 0.8857504725456238, 0.7948641180992126, 0.0794864147901535, 0.158972829580307, 0.8991756439208984, 0.9177130460739136, 0.0764760822057724, 0.0764760822057724, 0.07065262645483017, 0.07065262645483017, 0.8478315472602844, 0.61990886926651, 0.06887876242399216, 0.27551504969596863, 0.7088803052902222, 0.0644436627626419, 0.22555282711982727, 0.8984532356262207, 0.8779343962669373, 0.06753341108560562, 0.892364501953125, 0.3962599039077759, 0.5547638535499573, 0.0792519822716713, 0.8671125769615173, 0.09634584188461304, 0.20299898087978363, 0.7104964256286621, 0.10149949043989182, 0.7668514251708984, 0.07189232110977173, 0.14378464221954346, 0.2418215572834015, 0.6045538783073425, 0.07905998080968857, 0.15811996161937714, 0.7115398049354553, 0.1532675176858902, 0.10217834264039993, 0.7152484059333801, 0.6810188293457031, 0.22700627148151398, 0.07566875219345093, 0.919894814491272, 0.8296734690666199, 0.13827891647815704, 0.13827891647815704, 0.0791502371430397, 0.1583004742860794, 0.7123521566390991, 0.7573837041854858, 0.18934592604637146, 0.09467296302318573, 0.8142212629318237, 0.16284425556659698, 0.08142212778329849, 0.08687801659107208, 0.4343900978565216, 0.4343900978565216, 0.06940430402755737, 0.9022558927536011, 0.06940430402755737, 0.24404673278331757, 0.7321402430534363, 0.04880934953689575, 0.07988803833723068, 0.6391043066978455, 0.23966412246227264, 0.0626130923628807, 0.7513570785522461, 0.18783926963806152, 0.09272044152021408, 0.27816131711006165, 0.5563226342201233, 0.4001988470554352, 0.600298285484314, 0.1000497117638588, 0.8917410373687744, 0.8277789354324341, 0.08713462948799133, 0.08713462948799133, 0.7015394568443298, 0.2338465005159378, 0.09353859722614288, 0.7031772136688232, 0.1622716635465622, 0.10818110406398773, 0.2530406713485718, 0.2530406713485718, 0.4428211748600006, 0.07597863674163818, 0.07597863674163818, 0.83576500415802, 0.2998657524585724, 0.5997315049171448, 0.099955253303051, 0.09206706285476685, 0.09206706285476685, 0.8286035656929016, 0.7432499527931213, 0.057173073291778564, 0.22869229316711426, 0.1085544303059578, 0.7598810195922852, 0.1085544303059578, 0.05297935754060745, 0.8476697206497192, 0.1059587150812149, 0.5844687819480896, 0.3652929961681366, 0.0730585977435112, 0.7330367565155029, 0.1628970503807068, 0.0814485251903534, 0.1881054788827896, 0.8464746475219727, 0.0940527394413948, 0.835539698600769, 0.0835539698600769, 0.0835539698600769, 0.7603781819343567, 0.19009454548358917, 0.09504727274179459, 0.08762528002262115, 0.26287585496902466, 0.6571896076202393, 0.06879371404647827, 0.34396857023239136, 0.5503497123718262, 0.9316896200180054, 0.1651260256767273, 0.7430670857429504, 0.8918007016181946, 0.6317299008369446, 0.07896623760461807, 0.3158649504184723, 0.9259408116340637, 0.06172938644886017, 0.4668077826499939, 0.5186753273010254, 0.0518675297498703, 0.17521704733371735, 0.17521704733371735, 0.7008681893348694, 0.850894570350647, 0.08508945256471634, 0.08508945256471634, 0.13550789654254913, 0.8130474090576172, 0.11321529000997543, 0.11321529000997543, 0.792506992816925, 0.27402791380882263, 0.27402791380882263, 0.43061530590057373, 0.854167103767395, 0.06101193279027939, 0.12202386558055878, 0.11647152155637741, 0.7861827611923218, 0.08735363930463791, 0.24984009563922882, 0.6870602369308472, 0.062460023909807205, 0.09347947686910629, 0.09347947686910629, 0.8413152694702148, 0.6993579864501953, 0.11655966937541962, 0.11655966937541962, 0.9563474655151367, 0.07356519252061844, 0.07138752937316895, 0.1427750587463379, 0.7852628231048584, 0.8915669918060303, 0.13455595076084137, 0.3363898992538452, 0.5382238030433655, 0.5842209458351135, 0.06491343677043915, 0.3894806504249573, 0.9559399485588074, 0.07353384792804718, 0.24988284707069397, 0.7496485114097595, 0.12494142353534698, 0.8981326222419739, 0.26724278926849365, 0.06681069731712341, 0.6681069135665894, 0.7686203718185425, 0.1537240743637085, 0.24646106362342834, 0.1643073707818985, 0.5750758051872253, 0.47975826263427734, 0.359818696975708, 0.179909348487854, 0.13225947320461273, 0.13225947320461273, 0.7274271249771118, 0.8510318994522095, 0.13092799484729767, 0.7963060140609741, 0.12250861525535583, 0.06125430762767792, 0.11371532082557678, 0.22743064165115356, 0.6822919249534607, 0.36176908016204834, 0.6029484868049622, 0.060294847935438156, 0.10670468211174011, 0.42681872844696045, 0.42681872844696045, 0.691342294216156, 0.2659008800983429, 0.05318017676472664, 0.8069042563438416, 0.12740594148635864, 0.04246864467859268, 0.12849292159080505, 0.7709575295448303, 0.12849292159080505, 0.6244249939918518, 0.39026564359664917, 0.07805312424898148, 0.28627029061317444, 0.5152865052223206, 0.17176216840744019, 0.5737645030021667, 0.2868822515010834, 0.17212934792041779, 0.09429765492677689, 0.37719061970710754, 0.4714882969856262, 0.05194145441055298, 0.10388290882110596, 0.8310632705688477, 0.956175684928894, 0.0735519751906395, 0.29610496759414673, 0.666236162185669, 0.07402624189853668, 0.6734012365341187, 0.22446708381175995, 0.07482235878705978, 0.11176170408725739, 0.8940936326980591, 0.11176170408725739, 0.6676192283630371, 0.08345240354537964, 0.2503572106361389, 0.04468287155032158, 0.8936574459075928, 0.04468287155032158, 0.14348439872264862, 0.07174219936132431, 0.7891642451286316, 0.14988869428634644, 0.22483302652835846, 0.6744990944862366], \"Term\": [\"amaz\", \"amaz\", \"amaz\", \"april\", \"april\", \"april\", \"basi\", \"break\", \"break\", \"break\", \"call\", \"call\", \"call\", \"care\", \"care\", \"care\", \"case\", \"case\", \"case\", \"cast\", \"caus\", \"caus\", \"ceas\", \"china\", \"china\", \"china\", \"confirm\", \"confirm\", \"corona\", \"corona\", \"corona\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"cover\", \"cover\", \"crisi\", \"crisi\", \"crisi\", \"death\", \"death\", \"death\", \"die\", \"die\", \"die\", \"discrimin\", \"either\", \"either\", \"either\", \"equip\", \"equip\", \"equip\", \"even\", \"even\", \"even\", \"everyon\", \"everyon\", \"everyon\", \"face\", \"face\", \"face\", \"father\", \"father\", \"father\", \"fight\", \"fight\", \"fight\", \"first\", \"first\", \"first\", \"govern\", \"govern\", \"govern\", \"governor\", \"governor\", \"governor\", \"great\", \"great\", \"great\", \"gullibl\", \"health\", \"health\", \"health\", \"help\", \"help\", \"help\", \"home\", \"home\", \"home\", \"hospit\", \"hospit\", \"hospit\", \"human\", \"human\", \"human\", \"india\", \"india\", \"india\", \"isol\", \"isol\", \"isol\", \"keep\", \"keep\", \"keep\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"live\", \"live\", \"live\", \"lockdown\", \"lockdown\", \"lockdown\", \"look\", \"look\", \"look\", \"make\", \"make\", \"make\", \"mani\", \"mani\", \"mani\", \"mask\", \"mask\", \"mask\", \"medic\", \"medic\", \"medic\", \"murder\", \"must\", \"must\", \"narrat\", \"nation\", \"nation\", \"nation\", \"natur\", \"natur\", \"need\", \"need\", \"need\", \"never\", \"never\", \"never\", \"nurs\", \"nurs\", \"nurs\", \"often\", \"often\", \"order\", \"order\", \"order\", \"pandem\", \"pandem\", \"pandem\", \"patient\", \"patient\", \"patient\", \"peopl\", \"peopl\", \"peopl\", \"pleas\", \"pleas\", \"pleas\", \"polic\", \"polic\", \"polic\", \"poor\", \"poor\", \"poor\", \"poverti\", \"poverti\", \"presid\", \"presid\", \"presid\", \"propaganda\", \"protect\", \"protect\", \"protect\", \"public\", \"public\", \"public\", \"racism\", \"racism\", \"recommend\", \"recommend\", \"recommend\", \"religion\", \"report\", \"report\", \"report\", \"rich\", \"rich\", \"right\", \"right\", \"right\", \"say\", \"say\", \"say\", \"send\", \"send\", \"send\", \"speak\", \"speak\", \"spread\", \"spread\", \"spread\", \"state\", \"state\", \"state\", \"stay\", \"stay\", \"stay\", \"support\", \"support\", \"support\", \"take\", \"take\", \"take\", \"test\", \"test\", \"test\", \"thank\", \"thank\", \"thank\", \"think\", \"think\", \"think\", \"time\", \"time\", \"time\", \"today\", \"today\", \"today\", \"total\", \"total\", \"total\", \"trump\", \"trump\", \"trump\", \"trut\", \"trut\", \"virus\", \"virus\", \"virus\", \"want\", \"want\", \"want\", \"watch\", \"watch\", \"watch\", \"week\", \"week\", \"week\", \"work\", \"work\", \"work\", \"worker\", \"worker\", \"worker\", \"world\", \"world\", \"world\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el3909550972822565047257556\", ldavis_el3909550972822565047257556_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el3909550972822565047257556\", ldavis_el3909550972822565047257556_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el3909550972822565047257556\", ldavis_el3909550972822565047257556_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1     -0.024509 -0.146795       1        1  37.265041\n",
       "2     -0.120994  0.093648       2        1  33.191292\n",
       "0      0.145502  0.053147       3        1  29.543669, topic_info=    Category       Freq         Term      Total  loglift  logprob\n",
       "42   Default  22.000000         work  22.000000  30.0000  30.0000\n",
       "22   Default  34.000000        peopl  34.000000  29.0000  29.0000\n",
       "106  Default  19.000000        trump  19.000000  28.0000  28.0000\n",
       "5    Default  41.000000  coronavirus  41.000000  27.0000  27.0000\n",
       "37   Default  16.000000        natur  16.000000  26.0000  26.0000\n",
       "36   Default  13.000000       murder  13.000000  25.0000  25.0000\n",
       "2    Default  17.000000        order  17.000000  24.0000  24.0000\n",
       "91   Default  14.000000         call  14.000000  23.0000  23.0000\n",
       "38   Default  13.000000      poverti  13.000000  22.0000  22.0000\n",
       "39   Default  13.000000       racism  13.000000  21.0000  21.0000\n",
       "72   Default  23.000000         make  23.000000  20.0000  20.0000\n",
       "41   Default  13.000000         trut  13.000000  19.0000  19.0000\n",
       "35   Default  14.000000       father  14.000000  18.0000  18.0000\n",
       "29   Default  18.000000         like  18.000000  17.0000  17.0000\n",
       "34   Default  14.000000         caus  14.000000  16.0000  16.0000\n",
       "18   Default  13.000000        human  13.000000  15.0000  15.0000\n",
       "13   Default  23.000000         test  23.000000  14.0000  14.0000\n",
       "40   Default  15.000000        speak  15.000000  13.0000  13.0000\n",
       "0    Default  22.000000       health  22.000000  12.0000  12.0000\n",
       "46   Default  19.000000        death  19.000000  11.0000  11.0000\n",
       "24   Default  22.000000         mask  22.000000  10.0000  10.0000\n",
       "45   Default  10.000000         isol  10.000000   9.0000   9.0000\n",
       "102  Default  14.000000       presid  14.000000   8.0000   8.0000\n",
       "75   Default  13.000000       worker  13.000000   7.0000   7.0000\n",
       "70   Default  20.000000        fight  20.000000   6.0000   6.0000\n",
       "61   Default  12.000000         must  12.000000   5.0000   5.0000\n",
       "43   Default  15.000000       govern  15.000000   4.0000   4.0000\n",
       "92   Default  13.000000        break  13.000000   3.0000   3.0000\n",
       "3    Default  17.000000        state  17.000000   2.0000   2.0000\n",
       "53   Default  31.000000         case  31.000000   1.0000   1.0000\n",
       "92    Topic1  11.722426        break  13.075983   0.8778  -3.8723\n",
       "65    Topic1   6.691922    discrimin   7.609566   0.8586  -4.4329\n",
       "63    Topic1   4.820384         basi   5.560649   0.8443  -4.7610\n",
       "64    Topic1   4.782580         cast   5.565120   0.8356  -4.7688\n",
       "69    Topic1   4.777819     religion   5.567107   0.8342  -4.7698\n",
       "72    Topic1  20.466206         make  23.936625   0.8305  -3.3151\n",
       "49    Topic1  13.768491      patient  16.390236   0.8128  -3.7114\n",
       "19    Topic1   5.424157         rich   6.505162   0.8054  -4.6430\n",
       "100   Topic1   8.611781      confirm  10.379275   0.8004  -4.1807\n",
       "0     Topic1  19.023029       health  22.952988   0.7993  -3.3882\n",
       "8     Topic1   9.728037         nurs  11.752337   0.7981  -4.0588\n",
       "10    Topic1  13.314896       spread  16.325382   0.7833  -3.7449\n",
       "13    Topic1  19.203789         test  23.546783   0.7832  -3.3787\n",
       "66    Topic1   5.734484       either   7.231761   0.7551  -4.5873\n",
       "67    Topic1   9.720010      everyon  12.281673   0.7532  -4.0596\n",
       "26    Topic1   9.872032        april  12.580767   0.7447  -4.0441\n",
       "5     Topic1  32.347218  coronavirus  41.729073   0.7324  -2.8573\n",
       "95    Topic1   8.109492         mani  10.521080   0.7268  -4.2408\n",
       "27    Topic1   9.109095     lockdown  12.277693   0.6886  -4.1246\n",
       "1     Topic1  13.374413         home  18.487516   0.6634  -3.7405\n",
       "7     Topic1  12.619587         keep  17.490751   0.6607  -3.7986\n",
       "44    Topic1   7.506304         even  10.562677   0.6455  -4.3181\n",
       "6     Topic1  15.177083         help  21.381548   0.6444  -3.6140\n",
       "53    Topic1  21.761744         case  31.034859   0.6322  -3.2537\n",
       "68    Topic1   6.003836         poor   8.579297   0.6302  -4.5414\n",
       "86    Topic1   8.194889         week  11.982879   0.6071  -4.2303\n",
       "89    Topic1   8.963256          die  13.215494   0.5989  -4.1407\n",
       "52    Topic1  12.561077         take  18.803999   0.5836  -3.8032\n",
       "32    Topic1   8.827635         want  13.364988   0.5724  -4.1559\n",
       "48    Topic1   8.957045         care  14.518263   0.5041  -4.1414\n",
       "30    Topic1   9.612785        today  17.428753   0.3921  -4.0707\n",
       "17    Topic1   8.787889       public  15.405130   0.4258  -4.1605\n",
       "76    Topic1   8.141595         live  13.687643   0.4676  -4.2368\n",
       "28    Topic1   7.717587       nation  12.663640   0.4919  -4.2903\n",
       "12    Topic1   7.707735        think  12.811787   0.4790  -4.2916\n",
       "11    Topic1   8.337221          say  16.675064   0.2939  -4.2131\n",
       "78    Topic1   8.812243         need  19.279884   0.2042  -4.1577\n",
       "36    Topic2  13.088467       murder  13.953145   1.0389  -3.6463\n",
       "38    Topic2  12.643603      poverti  13.593386   1.0305  -3.6809\n",
       "39    Topic2  12.603838       racism  13.599180   1.0269  -3.6841\n",
       "41    Topic2  12.593970         trut  13.595828   1.0263  -3.6848\n",
       "37    Topic2  14.893354        natur  16.199739   1.0188  -3.5171\n",
       "35    Topic2  13.100122       father  14.408329   1.0077  -3.6454\n",
       "42    Topic2  20.034256         work  22.379940   0.9922  -3.2206\n",
       "34    Topic2  13.145617         caus  14.807485   0.9838  -3.6420\n",
       "40    Topic2  12.856959        speak  15.275573   0.9305  -3.6642\n",
       "103   Topic2   7.516141        watch   8.947609   0.9285  -4.2010\n",
       "29    Topic2  15.806910         like  18.875277   0.9255  -3.4576\n",
       "79    Topic2   8.511822         look  10.632333   0.8804  -4.0766\n",
       "22    Topic2  27.238758        peopl  34.343159   0.8711  -2.9134\n",
       "43    Topic2  12.348857       govern  15.971101   0.8457  -3.7045\n",
       "9     Topic2  11.809484        thank  15.565060   0.8268  -3.7492\n",
       "88    Topic2   7.428688       corona   9.852266   0.8205  -4.2127\n",
       "70    Topic2  14.779942        fight  20.487879   0.7763  -3.5248\n",
       "58    Topic2   6.572579         life   9.211968   0.7653  -4.3352\n",
       "87    Topic2  11.139659        pleas  16.010241   0.7402  -3.8075\n",
       "98    Topic2   5.548048    recommend   8.003751   0.7364  -4.5046\n",
       "73    Topic2   9.214779        virus  13.508723   0.7204  -3.9972\n",
       "20    Topic2   8.402994        first  12.517518   0.7043  -4.0895\n",
       "107   Topic2   5.494792        cover   8.270561   0.6940  -4.5143\n",
       "71    Topic2   6.480501        india  10.004477   0.6686  -4.3493\n",
       "4     Topic2   9.831815         stay  16.585165   0.5800  -3.9324\n",
       "74    Topic2   5.585620        great   9.995031   0.5210  -4.4979\n",
       "60    Topic2   6.717871        china  12.617981   0.4725  -4.3133\n",
       "50    Topic2   9.025754         time  17.466011   0.4427  -4.0180\n",
       "78    Topic2   9.605060         need  19.279884   0.4061  -3.9558\n",
       "33    Topic2   4.258387      support   9.371660   0.3141  -4.7692\n",
       "101   Topic2   4.895151         face  11.510391   0.2479  -4.6298\n",
       "16    Topic2   7.055624       pandem  25.544842  -0.1837  -4.2642\n",
       "24    Topic2   6.069539         mask  22.824463  -0.2217  -4.4148\n",
       "11    Topic2   5.555189          say  16.675064   0.0037  -4.5033\n",
       "85    Topic3   6.040900   propaganda   6.729724   1.1113  -4.3031\n",
       "82    Topic3   6.032198      gullibl   6.728411   1.1101  -4.3045\n",
       "83    Topic3   6.028589       narrat   6.727961   1.1095  -4.3051\n",
       "81    Topic3   5.997854         ceas   6.723710   1.1051  -4.3102\n",
       "23    Topic3   6.426414        often   7.379643   1.0810  -4.2412\n",
       "45    Topic3   9.450311         isol  10.861648   1.0801  -3.8556\n",
       "91    Topic3  12.295162         call  14.153755   1.0785  -3.5924\n",
       "18    Topic3  11.178848        human  13.161594   1.0560  -3.6876\n",
       "106   Topic3  16.176458        trump  19.252445   1.0452  -3.3181\n",
       "80    Topic3   6.637544         amaz   7.902903   1.0448  -4.2089\n",
       "2     Topic3  14.267297        order  17.665459   1.0057  -3.4437\n",
       "97    Topic3   8.537724        polic  10.697536   0.9938  -3.9571\n",
       "61    Topic3   9.471317         must  12.111961   0.9734  -3.8534\n",
       "102   Topic3  10.925035       presid  14.008049   0.9707  -3.7106\n",
       "75    Topic3  10.727813       worker  13.938797   0.9575  -3.7288\n",
       "59    Topic3   9.406426        crisi  12.648624   0.9231  -3.8602\n",
       "77    Topic3   9.311819        equip  12.634201   0.9142  -3.8704\n",
       "46    Topic3  14.140952        death  19.573620   0.8942  -3.4526\n",
       "25    Topic3  10.558594         send  15.121790   0.8601  -3.7447\n",
       "47    Topic3  10.372583       report  14.967664   0.8526  -3.7625\n",
       "56    Topic3   9.138647        world  13.343235   0.8408  -3.8891\n",
       "3     Topic3  11.951511        state  17.587780   0.8330  -3.6208\n",
       "84    Topic3   7.710203        never  11.414414   0.8270  -4.0591\n",
       "24    Topic3  14.797721         mask  22.824463   0.7859  -3.4072\n",
       "105   Topic3   7.480721        right  12.172308   0.7325  -4.0893\n",
       "31    Topic3   8.469761        medic  14.536212   0.6792  -3.9651\n",
       "99    Topic3   6.205232     governor  10.785109   0.6665  -4.2762\n",
       "96    Topic3   7.800815      protect  14.863705   0.5746  -4.0474\n",
       "93    Topic3   5.141671        total  10.604717   0.4954  -4.4643\n",
       "21    Topic3   7.425685       hospit  15.807735   0.4637  -4.0967\n",
       "16    Topic3  11.368744       pandem  25.544842   0.4097  -3.6708\n",
       "53    Topic3   6.882570         case  31.034859  -0.2868  -4.1726\n",
       "5     Topic3   6.054180  coronavirus  41.729073  -0.7111  -4.3009\n",
       "17    Topic3   6.053897       public  15.405130   0.2853  -4.3009, token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "80        1  0.126536         amaz\n",
       "80        2  0.126536         amaz\n",
       "80        3  0.885750         amaz\n",
       "26        1  0.794864        april\n",
       "26        2  0.079486        april\n",
       "26        3  0.158973        april\n",
       "63        1  0.899176         basi\n",
       "92        1  0.917713        break\n",
       "92        2  0.076476        break\n",
       "92        3  0.076476        break\n",
       "91        1  0.070653         call\n",
       "91        2  0.070653         call\n",
       "91        3  0.847832         call\n",
       "48        1  0.619909         care\n",
       "48        2  0.068879         care\n",
       "48        3  0.275515         care\n",
       "53        1  0.708880         case\n",
       "53        2  0.064444         case\n",
       "53        3  0.225553         case\n",
       "64        1  0.898453         cast\n",
       "34        2  0.877934         caus\n",
       "34        3  0.067533         caus\n",
       "81        3  0.892365         ceas\n",
       "60        1  0.396260        china\n",
       "60        2  0.554764        china\n",
       "60        3  0.079252        china\n",
       "100       1  0.867113      confirm\n",
       "100       3  0.096346      confirm\n",
       "88        1  0.202999       corona\n",
       "88        2  0.710496       corona\n",
       "88        3  0.101499       corona\n",
       "5         1  0.766851  coronavirus\n",
       "5         2  0.071892  coronavirus\n",
       "5         3  0.143785  coronavirus\n",
       "107       1  0.241822        cover\n",
       "107       2  0.604554        cover\n",
       "59        1  0.079060        crisi\n",
       "59        2  0.158120        crisi\n",
       "59        3  0.711540        crisi\n",
       "46        1  0.153268        death\n",
       "46        2  0.102178        death\n",
       "46        3  0.715248        death\n",
       "89        1  0.681019          die\n",
       "89        2  0.227006          die\n",
       "89        3  0.075669          die\n",
       "65        1  0.919895    discrimin\n",
       "66        1  0.829673       either\n",
       "66        2  0.138279       either\n",
       "66        3  0.138279       either\n",
       "77        1  0.079150        equip\n",
       "77        2  0.158300        equip\n",
       "77        3  0.712352        equip\n",
       "44        1  0.757384         even\n",
       "44        2  0.189346         even\n",
       "44        3  0.094673         even\n",
       "67        1  0.814221      everyon\n",
       "67        2  0.162844      everyon\n",
       "67        3  0.081422      everyon\n",
       "101       1  0.086878         face\n",
       "101       2  0.434390         face\n",
       "101       3  0.434390         face\n",
       "35        1  0.069404       father\n",
       "35        2  0.902256       father\n",
       "35        3  0.069404       father\n",
       "70        1  0.244047        fight\n",
       "70        2  0.732140        fight\n",
       "70        3  0.048809        fight\n",
       "20        1  0.079888        first\n",
       "20        2  0.639104        first\n",
       "20        3  0.239664        first\n",
       "43        1  0.062613       govern\n",
       "43        2  0.751357       govern\n",
       "43        3  0.187839       govern\n",
       "99        1  0.092720     governor\n",
       "99        2  0.278161     governor\n",
       "99        3  0.556323     governor\n",
       "74        1  0.400199        great\n",
       "74        2  0.600298        great\n",
       "74        3  0.100050        great\n",
       "82        3  0.891741      gullibl\n",
       "0         1  0.827779       health\n",
       "0         2  0.087135       health\n",
       "0         3  0.087135       health\n",
       "6         1  0.701539         help\n",
       "6         2  0.233847         help\n",
       "6         3  0.093539         help\n",
       "1         1  0.703177         home\n",
       "1         2  0.162272         home\n",
       "1         3  0.108181         home\n",
       "21        1  0.253041       hospit\n",
       "21        2  0.253041       hospit\n",
       "21        3  0.442821       hospit\n",
       "18        1  0.075979        human\n",
       "18        2  0.075979        human\n",
       "18        3  0.835765        human\n",
       "71        1  0.299866        india\n",
       "71        2  0.599732        india\n",
       "71        3  0.099955        india\n",
       "45        1  0.092067         isol\n",
       "45        2  0.092067         isol\n",
       "45        3  0.828604         isol\n",
       "7         1  0.743250         keep\n",
       "7         2  0.057173         keep\n",
       "7         3  0.228692         keep\n",
       "58        1  0.108554         life\n",
       "58        2  0.759881         life\n",
       "58        3  0.108554         life\n",
       "29        1  0.052979         like\n",
       "29        2  0.847670         like\n",
       "29        3  0.105959         like\n",
       "76        1  0.584469         live\n",
       "76        2  0.365293         live\n",
       "76        3  0.073059         live\n",
       "27        1  0.733037     lockdown\n",
       "27        2  0.162897     lockdown\n",
       "27        3  0.081449     lockdown\n",
       "79        1  0.188105         look\n",
       "79        2  0.846475         look\n",
       "79        3  0.094053         look\n",
       "72        1  0.835540         make\n",
       "72        2  0.083554         make\n",
       "72        3  0.083554         make\n",
       "95        1  0.760378         mani\n",
       "95        2  0.190095         mani\n",
       "95        3  0.095047         mani\n",
       "24        1  0.087625         mask\n",
       "24        2  0.262876         mask\n",
       "24        3  0.657190         mask\n",
       "31        1  0.068794        medic\n",
       "31        2  0.343969        medic\n",
       "31        3  0.550350        medic\n",
       "36        2  0.931690       murder\n",
       "61        2  0.165126         must\n",
       "61        3  0.743067         must\n",
       "83        3  0.891801       narrat\n",
       "28        1  0.631730       nation\n",
       "28        2  0.078966       nation\n",
       "28        3  0.315865       nation\n",
       "37        2  0.925941        natur\n",
       "37        3  0.061729        natur\n",
       "78        1  0.466808         need\n",
       "78        2  0.518675         need\n",
       "78        3  0.051868         need\n",
       "84        1  0.175217        never\n",
       "84        2  0.175217        never\n",
       "84        3  0.700868        never\n",
       "8         1  0.850895         nurs\n",
       "8         2  0.085089         nurs\n",
       "8         3  0.085089         nurs\n",
       "23        2  0.135508        often\n",
       "23        3  0.813047        often\n",
       "2         1  0.113215        order\n",
       "2         2  0.113215        order\n",
       "2         3  0.792507        order\n",
       "16        1  0.274028       pandem\n",
       "16        2  0.274028       pandem\n",
       "16        3  0.430615       pandem\n",
       "49        1  0.854167      patient\n",
       "49        2  0.061012      patient\n",
       "49        3  0.122024      patient\n",
       "22        1  0.116472        peopl\n",
       "22        2  0.786183        peopl\n",
       "22        3  0.087354        peopl\n",
       "87        1  0.249840        pleas\n",
       "87        2  0.687060        pleas\n",
       "87        3  0.062460        pleas\n",
       "97        1  0.093479        polic\n",
       "97        2  0.093479        polic\n",
       "97        3  0.841315        polic\n",
       "68        1  0.699358         poor\n",
       "68        2  0.116560         poor\n",
       "68        3  0.116560         poor\n",
       "38        2  0.956347      poverti\n",
       "38        3  0.073565      poverti\n",
       "102       1  0.071388       presid\n",
       "102       2  0.142775       presid\n",
       "102       3  0.785263       presid\n",
       "85        3  0.891567   propaganda\n",
       "96        1  0.134556      protect\n",
       "96        2  0.336390      protect\n",
       "96        3  0.538224      protect\n",
       "17        1  0.584221       public\n",
       "17        2  0.064913       public\n",
       "17        3  0.389481       public\n",
       "39        2  0.955940       racism\n",
       "39        3  0.073534       racism\n",
       "98        1  0.249883    recommend\n",
       "98        2  0.749649    recommend\n",
       "98        3  0.124941    recommend\n",
       "69        1  0.898133     religion\n",
       "47        1  0.267243       report\n",
       "47        2  0.066811       report\n",
       "47        3  0.668107       report\n",
       "19        1  0.768620         rich\n",
       "19        2  0.153724         rich\n",
       "105       1  0.246461        right\n",
       "105       2  0.164307        right\n",
       "105       3  0.575076        right\n",
       "11        1  0.479758          say\n",
       "11        2  0.359819          say\n",
       "11        3  0.179909          say\n",
       "25        1  0.132259         send\n",
       "25        2  0.132259         send\n",
       "25        3  0.727427         send\n",
       "40        2  0.851032        speak\n",
       "40        3  0.130928        speak\n",
       "10        1  0.796306       spread\n",
       "10        2  0.122509       spread\n",
       "10        3  0.061254       spread\n",
       "3         1  0.113715        state\n",
       "3         2  0.227431        state\n",
       "3         3  0.682292        state\n",
       "4         1  0.361769         stay\n",
       "4         2  0.602948         stay\n",
       "4         3  0.060295         stay\n",
       "33        1  0.106705      support\n",
       "33        2  0.426819      support\n",
       "33        3  0.426819      support\n",
       "52        1  0.691342         take\n",
       "52        2  0.265901         take\n",
       "52        3  0.053180         take\n",
       "13        1  0.806904         test\n",
       "13        2  0.127406         test\n",
       "13        3  0.042469         test\n",
       "9         1  0.128493        thank\n",
       "9         2  0.770958        thank\n",
       "9         3  0.128493        thank\n",
       "12        1  0.624425        think\n",
       "12        2  0.390266        think\n",
       "12        3  0.078053        think\n",
       "50        1  0.286270         time\n",
       "50        2  0.515287         time\n",
       "50        3  0.171762         time\n",
       "30        1  0.573765        today\n",
       "30        2  0.286882        today\n",
       "30        3  0.172129        today\n",
       "93        1  0.094298        total\n",
       "93        2  0.377191        total\n",
       "93        3  0.471488        total\n",
       "106       1  0.051941        trump\n",
       "106       2  0.103883        trump\n",
       "106       3  0.831063        trump\n",
       "41        2  0.956176         trut\n",
       "41        3  0.073552         trut\n",
       "73        1  0.296105        virus\n",
       "73        2  0.666236        virus\n",
       "73        3  0.074026        virus\n",
       "32        1  0.673401         want\n",
       "32        2  0.224467         want\n",
       "32        3  0.074822         want\n",
       "103       1  0.111762        watch\n",
       "103       2  0.894094        watch\n",
       "103       3  0.111762        watch\n",
       "86        1  0.667619         week\n",
       "86        2  0.083452         week\n",
       "86        3  0.250357         week\n",
       "42        1  0.044683         work\n",
       "42        2  0.893657         work\n",
       "42        3  0.044683         work\n",
       "75        1  0.143484       worker\n",
       "75        2  0.071742       worker\n",
       "75        3  0.789164       worker\n",
       "56        1  0.149889        world\n",
       "56        2  0.224833        world\n",
       "56        3  0.674499        world, R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 3, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_tfidf, tfidf_corpus, dictionary=lda_model_tfidf.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Model evaluation\n",
    "Topic cluster is a statistical tool that imposes probability distribution over words. We can use different coherence measures to quantify the quality of the clusters and the probability distribution of the words. However, before we dwelve into the coherence measures, lets try to approach the evaluation with a more intuitive way i.e. human validation. \n",
    "\n",
    "## 5.1. Evaluation by human validation\n",
    "In this section, we'll cluster a sample tweet and see how well does this clustering matches the overall topics generated above.\n",
    "\n",
    "### 5.1.1: Human validation for Bag of words model\n",
    "Classify a sample tweet into the topics and then evaluate if the general topic matches with the tweet better than other topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our test tweet is: 0: ['health', 'home', 'order', 'state', 'stay']\n",
      "\n",
      "Score: 0.882530689239502\t \n",
      "Topic: 0 : 0.035*\"mask\" + 0.033*\"keep\" + 0.028*\"stay\" + 0.025*\"home\" + 0.021*\"say\" + 0.021*\"human\" + 0.020*\"health\" + 0.020*\"order\" + 0.020*\"public\" + 0.019*\"time\"\n",
      "\n",
      "Score: 0.060176584869623184\t \n",
      "Topic: 2 : 0.072*\"case\" + 0.051*\"test\" + 0.043*\"peopl\" + 0.041*\"make\" + 0.038*\"like\" + 0.036*\"today\" + 0.035*\"health\" + 0.031*\"fight\" + 0.030*\"death\" + 0.027*\"pandem\"\n",
      "\n",
      "Score: 0.05729272961616516\t \n",
      "Topic: 1 : 0.070*\"coronavirus\" + 0.043*\"natur\" + 0.042*\"work\" + 0.040*\"speak\" + 0.039*\"caus\" + 0.037*\"father\" + 0.036*\"racism\" + 0.036*\"trut\" + 0.036*\"murder\" + 0.035*\"poverti\"\n"
     ]
    }
   ],
   "source": [
    "# Our test tweet is \n",
    "print('Our test tweet is: {}: {}'.format(tweet_num, [dictionary[word[0]] for word in bow_corpus[tweet_num]]))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_corpus[tweet_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {} : {}\".format(score, index, lda_model.print_topic(index, 10))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample tweet is classified to Topic 0 with 88% score. Topic 0 in Bag of word model was centered around self quarantining. The sample tweet matches with this topic. We could try evaluating more tweets manually. Seems like the BOW based LDA model worked well.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Human validation for TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our test tweet is: 0: ['health', 'home', 'order', 'state', 'stay']\n",
      "\n",
      "Score: 0.48965832591056824\t \n",
      "Topic: 1 : 0.070*\"coronavirus\" + 0.043*\"natur\" + 0.042*\"work\" + 0.040*\"speak\" + 0.039*\"caus\" + 0.037*\"father\" + 0.036*\"racism\" + 0.036*\"trut\" + 0.036*\"murder\" + 0.035*\"poverti\"\n",
      "\n",
      "Score: 0.38157591223716736\t \n",
      "Topic: 0 : 0.035*\"mask\" + 0.033*\"keep\" + 0.028*\"stay\" + 0.025*\"home\" + 0.021*\"say\" + 0.021*\"human\" + 0.020*\"health\" + 0.020*\"order\" + 0.020*\"public\" + 0.019*\"time\"\n",
      "\n",
      "Score: 0.1287657469511032\t \n",
      "Topic: 2 : 0.072*\"case\" + 0.051*\"test\" + 0.043*\"peopl\" + 0.041*\"make\" + 0.038*\"like\" + 0.036*\"today\" + 0.035*\"health\" + 0.031*\"fight\" + 0.030*\"death\" + 0.027*\"pandem\"\n"
     ]
    }
   ],
   "source": [
    "# Our test tweet is \n",
    "print('Our test tweet is: {}: {}'.format(tweet_num, [dictionary[word[0]] for word in tfidf_corpus[tweet_num]]))\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[tfidf_corpus[tweet_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {} : {}\".format(score, index, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the sample tweet is split between topics 0 with 49% score and topic 1 with 38% score. As we saw in the section 4.2.1, Topic 0 was centered around President Trump's announcement around COVID 19 and Topic 1 around Quarantine and fight the spread of COVID-19. \n",
    "\n",
    "The sample tweet matches well with Topic 1. However, the confidence of this clustering is low compared to 88% confidence we saw for Bag of Words model. \n",
    "\n",
    "The TF-IDF modeling didn't have good confidence in the classification. This was expected as TF-IDF doesn't work good for short text documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "EarlyIndicatorsFromNews",
  "notebookId": 3907296281459545
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
