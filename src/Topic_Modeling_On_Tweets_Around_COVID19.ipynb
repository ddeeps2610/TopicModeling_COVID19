{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling for COVID19\n",
    "COVID 19 has been the biggest pandemic people have seen in the recent times. It almost feels like one of those apocalyptic movies in real life. People hiding in their homes trying to save themselves from the infection. Some brave souls trying to find a better destination to survive this pandemic. With so much happening around the world, I have one question. **What are people around the world thinking about COVID 19?**\n",
    "\n",
    "In this notebook, we will try to answer the above question using Topic Modeling. Let's explore the various topics people are talking about Corona Virus Disease 2019(COVID-19) in Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/deepakawari/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Twitter data collection library\n",
    "import tweepy as tw\n",
    "\n",
    "# Data processing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Loading Gensim and nltk libraries\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "#Other stop_words: gensim.parsing.preprocessing.STOPWORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure notebook display to show data from pandas dataframe more clearly.\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.width',100)\n",
    "pd.set_option('display.max_colwidth',800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Gather the textual data for Topic Modeling\n",
    "To begin with the topic modeling we need the textual data. The textual data for what people are talking about COVID 19 can be pulled from many places such as social media, news articles, web scraping etc. In this notebook, we'll download the data from Twitter. Tweepy is an amazing library to pull data from twitter using your Twitter Developer Account. \n",
    "\n",
    "## Extracting tweets from Twitter API:\n",
    "To load the data from Twitter using Tweepy API, you'll have to create Developer account with Twitter. Then download the credentials to authenticate using Tweepy API. **Please do not share these credentials with anybody else.**\n",
    "* Here is the link to [apply for twitter developer access](https://developer.twitter.com/en/apply-for-access)\n",
    "* You can follow the below code to use Tweepy API to authenticate and load the data. Here is the [Tweepy Documentation for reference](http://docs.tweepy.org/en/latest/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LoadFromTwitter - \n",
    "    If true, pull the latest set of tweets from Twitter using the Tweepy library.\n",
    "    If false, load the data from the datafile '../data/tweets.csv' if it exists, \n",
    "    otherwise load the tweets from Twitter using the Tweepy library.\n",
    "    Set the LoadFromTwitter to True if you want to override loading the tweets afresh from twitter.\n",
    "'''\n",
    "LoadFromTwitter = False\n",
    "\n",
    "fileName = '../data/tweets.csv'\n",
    "tweetsDF = None\n",
    "\n",
    "# Load the data\n",
    "if os.path.exists(fileName) and not LoadFromTwitter:\n",
    "    tweetsDF = pd.read_csv(fileName)\n",
    "else:\n",
    "    from TwitterDevSecrets import getTwitterDevCreds\n",
    "    consumer_key, consumer_secret, access_token, access_secret = getTwitterDevCreds()\n",
    "\n",
    "    auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "    # Set the wait_on_rate_limit and wait_on_rate_limit_notify to True\n",
    "    # wait_on_rate_limit – \n",
    "    #    Whether or not to automatically wait for rate limits to replenish\n",
    "    # wait_on_rate_limit_notify – \n",
    "    #    Whether or not to print a notification when Tweepy is waiting \n",
    "    #    for rate limits to replenish\n",
    "    api = tw.API(\n",
    "        auth, \n",
    "        wait_on_rate_limit=True, \n",
    "        wait_on_rate_limit_notify=True)\n",
    "\n",
    "    # Define the search term and the date_since date as variables\n",
    "    search_words = \"#covid OR #covid19 OR #COVID OR #COVID19 OR #ncov OR #corona OR #coronaviru\"\n",
    "    date_since = \"2020-05-16\"\n",
    "    \n",
    "    # Read the tweets\n",
    "    tweets = tw.Cursor(api.search, \n",
    "                   q=search_words,\n",
    "                   lang=\"en\",\n",
    "                   since=date_since)\n",
    "\n",
    "    # extract the data in pandas dataframe\n",
    "    # Other parameters: tweet.user.screen_name, retweet_counts, favorite_counts\n",
    "    tweetsDF = pd.DataFrame()\n",
    "    for tweet in tweets.items(10000):\n",
    "        id = tweet.id\n",
    "        text = tweet.text\n",
    "        loc = tweet.user.location\n",
    "        tweetsDF = tweetsDF.append({'Id':id, 'Text':text, 'Location':loc},ignore_index=True)\n",
    "    \n",
    "    tweetsDF['index'] = tweetsDF.index\n",
    "    \n",
    "    # Save the new set of tweets in the file.\n",
    "    tweetsDF.to_csv(fileName,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how doest he textual data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @Carol_D_Johnson: Thank you nurses for helping to keep us healthy  ❤ #COVID19 \\n#StayHomeSaveLives \\n#coronavirus https://t.co/HGv0HfuTgt'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF.Text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Data preprocessing\n",
    "As you can see from the above text, a tweet contains a lot of textual data which probably doesn't contain any useful informaiton for Topic Modeling. So, these tweets needs to be processed to extract only useful textual data for further analysis. We will perform the following data processing steps:\n",
    "\n",
    "* Tweet Preprocessing:\n",
    "> * Remove the leading **RT** - RT indicates that the user is re-posting someone else's tweet. We can remove this token.\n",
    "> * Remove the references to other accounts. The other accounts are usually referenced with '@' symbol.\n",
    "> * Remove urls mentioned in the tweets.\n",
    "\n",
    "* Generic text preprocessing:\n",
    "> * **Tokenization**: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "> * Remove words that have fewer than 3 characters.\n",
    "> * Remove all **stopwords**. [Stop words](https://en.wikipedia.org/wiki/Stop_words) usually do not contain any usual information. As such these words are generally removed from the text in the preprocessing stage. \n",
    "> * **Lemmatize** the words: words in third person are changed to first person and verbs in past and future tenses are changed into present.  \n",
    "> Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words. \n",
    ">> **WordnetLemmatizer**: uses lookup table from nltk wordnet corpus to lookup the lemma to return a valid language lemma.\n",
    "> * **Stem** the Words: words are reduced to their root form.  \n",
    "> Stemming is the process of reducing inflection in words to their root forms such as \n",
    "mapping a group of words to the same stem even if the stem itself is not a valid word \n",
    "in the Language.\n",
    ">> **PorterStemmer**: is known for simplicity and ease. The algorithm does not follow linguistics rather a set of rules that are applied in phases (step by step) to generate stems. This is the reason why PorterStemmer does not often generate stems that are actual English words.  \n",
    ">> **SnowballStemmer**: One can generate their own set of rules for any language. Python nltk introduced SnowballStemmers that are used to create non-English Stemmers!  \n",
    ">> **LancasterStemmer**: is simple, but heavy stemming due to iterations and over-stemming may occur. Over-stemming causes the stems to be not linguistic, or they may have no meaning.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data preprocessing for all tweets.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def tweet_cleanup(text):\n",
    "    # Remove the leading RT from the tweet\n",
    "    text = text.replace('RT','')\n",
    "    # Remove the references to the account names starting with '@'\n",
    "    text = re.sub(r'(@[a-zA-Z]*)','',text)\n",
    "    # Remove the urls in the tweet.\n",
    "    text = re.sub(r'((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)','',text)\n",
    "    \n",
    "    return text\n",
    "  \n",
    "# Clean up the tweets and then Tokenize and lemmatize\n",
    "def preprocess(text, stop_words=stop_words):\n",
    "    result=[]\n",
    "    text = tweet_cleanup(text)\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in stop_words and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: \n",
      "['RT', '@ALPublicHealth:', 'State', 'Health', 'Officer', 'Dr.', 'Scott', 'Harris', 'has', 'issued', 'a', 'stay', 'at', 'home', 'order', 'and', 'strict', 'quarantine', 'requirements.', 'Read', 'our', 'full…']\n",
      "\n",
      "\n",
      "Preprocessed tweet: \n",
      "['state', 'health', 'offic', 'scott', 'harri', 'issu', 'stay', 'home', 'order', 'strict', 'quarantin', 'requir', 'read', 'full']\n"
     ]
    }
   ],
   "source": [
    "# Test the preprocessing step on a sample tweet\n",
    "tweet_num = 0\n",
    "sampleTweet = tweetsDF[tweetsDF['index'] == tweet_num].Text.iloc[0]\n",
    "\n",
    "print(\"Original tweet: \")\n",
    "words = []\n",
    "for word in sampleTweet.split():\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nPreprocessed tweet: \")\n",
    "print(preprocess(sampleTweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [state, health, offic, scott, harri, issu, stay, home, order, strict, quarantin, requir, read, full]\n",
       "1                                                      [thank, nurs, help, keep, healthi, covid, coronavirus]\n",
       "2                                                                        [togetherapart, slow, spread, covid]\n",
       "3                    [smoker, greater, risk, contract, coronavirus, elli, cannon, say, equal, risk, contract]\n",
       "4                                      [video, model, scan, show, extent, covid, damag, lung, tissu, stayhom]\n",
       "5                      [leader, hous, parti, caucus, arizona, andi, bigg, think, spread, covid, much, possib]\n",
       "6                                                                                     [covid, test, administ]\n",
       "7           [keep, think, master, public, health, write, doctor, dissert, global, effort, tackl, aid, pandem]\n",
       "8    [ceylonblacktea, rich, theaflavin, help, increas, human, immun, covid, srilankatea, industri, successfu]\n",
       "9                                     [peopl, first, group, hospit, sick, place, infirmari, convert, convent]\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess all tweets and generate a new processed tweet text dataset.\n",
    "processed_tweets = tweetsDF['Text'].map(preprocess)\n",
    "processed_tweets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Text representation\n",
    "Computers don't understand natural language texts. Text is a mere sequence of letters for computers. While its still difficult for computer to understand what the sequence of letters mean, language is way more complicated than that. For an example, let us consider an idiom \"Kicked the bucket\". You know where I am going right? When I first heard that phrase as a kid I thought it meant someone was actually kicking a bucket. That's fun! But, when I realized that it meant someone died, it was no more fun! So, natural language is hard and computers don't understand it. \n",
    "\n",
    "Computers love numbers. At the core, computers perform their operations on numbers. So, it'd be good to represent the natural language text with numbers for computer algorithms to process easily. In the below section, we'll explore two different models for text representation namely Bag of words and TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1: Bag of words on the dataset\n",
    "Create a dictionary of words present in the preprocessed_tweets dataset. Gensim offers a great api for the same. This dictionary assigns a numerical id to each word so that you can work on the number representations of the word. This makes the data processing very easy than working on strings. \n",
    "\n",
    "Then create a corpus of Bag of words where words are represented by their numerical ids along with the frequency of occurence of that word in the tweet for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_tweets)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 full\n",
      "1 harri\n",
      "2 health\n",
      "3 home\n",
      "4 issu\n",
      "5 offic\n"
     ]
    }
   ],
   "source": [
    "# Check the id to word mapping from the dictionary created above\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the text corpus is very huge and sparse, we should try to minimize the amount of text being used for modeling. For this reason, let us remove very rare and very common words. Gensim dictionary object provides a good api to perform this operation.\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "\n",
    "Then convert it into bag of word corpus with very rare and very common wordsd filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_tweets]\n",
    "\n",
    "# Test the Bag of Words representation of the tweet --> (token_id, token_count)\n",
    "bow_corpus[tweet_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"health\") appears 1 time.\n",
      "Word 1 (\"home\") appears 1 time.\n",
      "Word 2 (\"order\") appears 1 time.\n",
      "Word 3 (\"state\") appears 1 time.\n",
      "Word 4 (\"stay\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Preview BOW for our sample preprocessed tweet\n",
    "bow_tweet_0 = bow_corpus[tweet_num]\n",
    "\n",
    "for i in range(len(bow_tweet_0)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_tweet_0[i][0], \n",
    "                                                     dictionary[bow_tweet_0[i][0]], \n",
    "                                                     bow_tweet_0[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2: TF-IDF on the data set\n",
    "TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. Summing the Tf-idf of all possible terms and documents recovers the mutual information between documents and term taking into account all the specificities of their joint distribution.\n",
    "\n",
    "TF (Term Frequency) - number of times a term occurs in a document.  \n",
    "IDF (Inverse Document Frequency) diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.40009295170061265),\n",
       " (1, 0.44472924895798494),\n",
       " (2, 0.45253501051552114),\n",
       " (3, 0.47433139975516847),\n",
       " (4, 0.4608289406979316)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tf-idf model object using models.TfidfModel\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# Apply transformation to the entire corpus\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "\n",
    "# Test the tf-idf representation of the sample tweet. Each word is represented by (token_id, tf-idf score).\n",
    "tfidf_corpus[tweet_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"health\") TF-IDF score: 0.40009295170061265.\n",
      "Word 1 (\"home\") TF-IDF score: 0.44472924895798494.\n",
      "Word 2 (\"order\") TF-IDF score: 0.45253501051552114.\n",
      "Word 3 (\"state\") TF-IDF score: 0.47433139975516847.\n",
      "Word 4 (\"stay\") TF-IDF score: 0.4608289406979316.\n"
     ]
    }
   ],
   "source": [
    "# Preview TF-IDF for our sample preprocessed tweet\n",
    "tfidf_tweet_0 = tfidf_corpus[tweet_num]\n",
    "\n",
    "for i in range(len(tfidf_tweet_0)):\n",
    "    print(\"Word {} (\\\"{}\\\") TF-IDF score: {}.\".format(tfidf_tweet_0[i][0], \n",
    "                                                     dictionary[tfidf_tweet_0[i][0]], \n",
    "                                                     tfidf_tweet_0[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Topic modeling using LDA\n",
    "Topic modeling is a statistical model to discover the abstract topics in a collection of documents. Probabilistic Latent Semantic Analysis(PLSA) is one of the ealiest models for topic modeling. Latent Dirichlet Allocation(LDA) is the most common topic model algorithm in use today which is a generalization of PLSA. \n",
    "\n",
    "LDA introduces sparse Dirichlet prior distributions over document-topic and topic-word distributions. This algorithm tries to model the intuition that each document has different abstract topics and that each topic is generalized by a small number of words.\n",
    "\n",
    "In this section we'll be building the topic models using LDA for both text representations developed above. \n",
    "\n",
    "\n",
    "## Step 4.1: Modeling using Bag of Words\n",
    "The LDA algorithm requires a few inputs to build the clusters. The main parameter it requires in the number of clusters we want the model to cluster the tweets into. But how do we identify the number of topics? The best way to identify that is by visualizing the clusters itself. \n",
    "\n",
    "Start with a high number of topics like 10 or 20. Then map the clusters into a vector space and see if the clusters have clear boundaries. If the clusters overlap, reduce the number of clusters, build the model and visualize again. Repeat the process until you are satisfied with the segregation of the clusters.\n",
    "\n",
    "### Step 4.1.1: Running LDA using bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the lda model using gensim.models.LdaMulticore on Bag of word corpus\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=3, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.043*\"work\" + 0.038*\"natur\" + 0.035*\"caus\" + 0.034*\"father\" + 0.034*\"murder\" + 0.033*\"speak\" + 0.033*\"trut\" + 0.033*\"poverti\" + 0.033*\"racism\" + 0.030*\"mask\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.070*\"coronavirus\" + 0.065*\"peopl\" + 0.050*\"test\" + 0.041*\"help\" + 0.029*\"affect\" + 0.025*\"everyon\" + 0.021*\"hospit\" + 0.020*\"fight\" + 0.020*\"poor\" + 0.019*\"either\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.044*\"health\" + 0.034*\"case\" + 0.034*\"keep\" + 0.033*\"pandem\" + 0.026*\"time\" + 0.025*\"death\" + 0.024*\"care\" + 0.023*\"human\" + 0.023*\"like\" + 0.021*\"trump\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the words occuring in that topic and its relative weight\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the top words in each topic, we can identify the generic topic in that cluster. In the above clustering, the topics could be around  \n",
    "Topic 0: Self quarantining     \n",
    "Topic 1: Impact of COVID-19 on work, racism, and poverty.   \n",
    "Topic 2: Testing and health concerns due to COVID 19.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Visualization using pyLDAVis for LDA with BOW\n",
    "In this section, we'll visualize the topics generated by the above LDA model using pyLDAvis library. pyLDAvis provides an amazing interactive visualization tool to see how different clusters are generated. It produces the intertopic distance map and shows top relevant terms for each topic amongst other features. \n",
    "\n",
    "As mentioned above, we'll visualize the intertopic distance map to see if there is a good segregation of clusters. If there is an overlap between multiple clusters, we'd reduce the number of topics and run the LDA model with reduced number of clusters and visualize again. Once we are satisfied with the cluster segregation in the intertopic distance map, we can start looking into the terms to see what each topic represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el6057351955275048881060128\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el6057351955275048881060128_data = {\"mdsDat\": {\"x\": [-0.01438560712429786, -0.14815786927904162, 0.16254347640333947], \"y\": [-0.15174191842060036, 0.08640953421015313, 0.06533238421044724], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [33.903133392333984, 33.809349060058594, 32.28751754760742]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [73.0, 96.0, 53.0, 53.0, 50.0, 37.0, 37.0, 38.0, 37.0, 39.0, 39.0, 30.0, 55.0, 44.0, 39.0, 42.0, 25.0, 30.0, 18.0, 20.0, 28.0, 22.0, 37.0, 19.0, 31.0, 23.0, 16.0, 16.0, 16.0, 49.0, 19.7803955078125, 14.691783905029297, 21.007009506225586, 17.25259017944336, 15.302350044250488, 23.52184295654297, 18.56552505493164, 14.04030990600586, 24.384780883789062, 34.199615478515625, 44.70894241333008, 25.98159408569336, 18.705251693725586, 12.899161338806152, 20.35712242126465, 12.868844032287598, 15.481350898742676, 16.45795440673828, 11.959931373596191, 10.991703033447266, 21.173677444458008, 24.956926345825195, 33.463130950927734, 18.397977828979492, 13.43064022064209, 12.719962120056152, 20.307113647460938, 17.359851837158203, 15.240833282470703, 22.742380142211914, 34.199764251708984, 18.03264808654785, 15.651979446411133, 12.962030410766602, 33.424129486083984, 33.349098205566406, 33.26233673095703, 20.620742797851562, 33.805511474609375, 35.16717529296875, 12.53483772277832, 12.510952949523926, 34.14375305175781, 12.337347984313965, 12.273006439208984, 26.065868377685547, 12.940823554992676, 37.9804801940918, 16.50950813293457, 13.766469955444336, 33.46805191040039, 29.816553115844727, 13.44481372833252, 18.905153274536133, 42.83572769165039, 15.07706069946289, 22.861440658569336, 10.539355278015137, 25.267377853393555, 19.623920440673828, 18.954471588134766, 9.88802433013916, 13.30263614654541, 9.974811553955078, 25.40578269958496, 14.44345760345459, 25.750349044799805, 13.241117477416992, 17.989116668701172, 15.961073875427246, 15.944502830505371, 15.921195030212402, 28.043045043945312, 18.218297958374023, 16.336997985839844, 48.212223052978516, 14.322379112243652, 17.834115982055664, 62.16093063354492, 16.82893180847168, 13.233564376831055, 14.980607986450195, 18.881343841552734, 39.05104064941406, 13.810419082641602, 15.234155654907227, 23.683086395263672, 67.6170883178711, 20.54167366027832, 17.4377384185791, 13.882986068725586, 11.698040008544922, 9.876630783081055, 12.295258522033691, 9.012115478515625, 10.259156227111816, 14.931906700134277, 10.8717679977417, 12.830439567565918, 19.46608543395996, 13.197744369506836, 16.24009895324707, 13.677947044372559], \"Term\": [\"peopl\", \"coronavirus\", \"test\", \"health\", \"help\", \"trut\", \"poverti\", \"murder\", \"racism\", \"father\", \"caus\", \"affect\", \"work\", \"natur\", \"keep\", \"speak\", \"human\", \"stay\", \"rich\", \"call\", \"care\", \"presid\", \"mask\", \"either\", \"time\", \"never\", \"religion\", \"basi\", \"cast\", \"pandem\", \"call\", \"support\", \"presid\", \"global\", \"crisi\", \"human\", \"confirm\", \"releas\", \"care\", \"keep\", \"health\", \"time\", \"china\", \"life\", \"think\", \"isol\", \"worker\", \"doctor\", \"great\", \"corona\", \"trump\", \"death\", \"pandem\", \"state\", \"must\", \"right\", \"order\", \"pleas\", \"govern\", \"like\", \"case\", \"fight\", \"take\", \"report\", \"trut\", \"poverti\", \"racism\", \"never\", \"murder\", \"caus\", \"propaganda\", \"narrat\", \"father\", \"gullibl\", \"ceas\", \"stay\", \"often\", \"natur\", \"nation\", \"week\", \"speak\", \"mask\", \"amaz\", \"say\", \"work\", \"thank\", \"public\", \"ask\", \"make\", \"need\", \"home\", \"respons\", \"spread\", \"posit\", \"case\", \"take\", \"coronavirus\", \"today\", \"rich\", \"religion\", \"basi\", \"cast\", \"affect\", \"either\", \"polic\", \"test\", \"look\", \"india\", \"peopl\", \"discrimin\", \"recommend\", \"cover\", \"poor\", \"help\", \"watch\", \"medic\", \"everyon\", \"coronavirus\", \"hospit\", \"face\", \"break\", \"even\", \"enough\", \"first\", \"mani\", \"total\", \"virus\", \"want\", \"protect\", \"fight\", \"like\", \"case\", \"pandem\"], \"Total\": [73.0, 96.0, 53.0, 53.0, 50.0, 37.0, 37.0, 38.0, 37.0, 39.0, 39.0, 30.0, 55.0, 44.0, 39.0, 42.0, 25.0, 30.0, 18.0, 20.0, 28.0, 22.0, 37.0, 19.0, 31.0, 23.0, 16.0, 16.0, 16.0, 49.0, 20.83652114868164, 15.83183765411377, 22.724502563476562, 18.896194458007812, 16.810932159423828, 25.878557205200195, 20.677953720092773, 15.909235954284668, 28.382877349853516, 39.81488800048828, 53.15202331542969, 31.425275802612305, 22.936187744140625, 15.98289966583252, 25.541004180908203, 17.376394271850586, 21.21098518371582, 22.707172393798828, 16.847776412963867, 15.673927307128906, 30.74739646911621, 37.19625473022461, 49.98316955566406, 28.6070556640625, 21.13714027404785, 20.63789176940918, 32.99870681762695, 28.70163345336914, 25.474573135375977, 38.12206268310547, 75.84564208984375, 42.952491760253906, 35.04336929321289, 26.735179901123047, 37.1957893371582, 37.20725631713867, 37.218360900878906, 23.118343353271484, 38.13527297973633, 39.9732666015625, 14.251039505004883, 14.256695747375488, 39.08543014526367, 14.291086196899414, 14.303888320922852, 30.393238067626953, 15.199784278869629, 44.662845611572266, 19.636117935180664, 17.118175506591797, 42.15979766845703, 37.77851867675781, 17.21070671081543, 24.280649185180664, 55.84939956665039, 21.890504837036133, 33.720115661621094, 16.58184242248535, 39.82192611694336, 34.28415298461914, 35.44499969482422, 18.880569458007812, 25.551963806152344, 19.964113235473633, 75.84564208984375, 35.04336929321289, 96.21814727783203, 33.56003952026367, 18.727619171142578, 16.6458740234375, 16.643178939819336, 16.639047622680664, 30.000444412231445, 19.639366149902344, 17.64510726928711, 53.576210021972656, 16.398221969604492, 20.521066665649414, 73.47427368164062, 20.658920288085938, 16.248367309570312, 18.470903396606445, 23.690935134887695, 50.550384521484375, 18.05510711669922, 20.47584342956543, 32.662376403808594, 96.21814727783203, 29.4990234375, 25.374771118164062, 22.470500946044922, 19.143543243408203, 16.531003952026367, 21.844322204589844, 16.898740768432617, 19.260055541992188, 29.187816619873047, 23.125911712646484, 27.373332977294922, 42.952491760253906, 38.12206268310547, 75.84564208984375, 49.98316955566406], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0296000242233276, 1.0068999528884888, 1.003100037574768, 0.9907000064849854, 0.9876000285148621, 0.9861999750137329, 0.9739000201225281, 0.9567000269889832, 0.9297999739646912, 0.9296000003814697, 0.9086999893188477, 0.8913999795913696, 0.8777999877929688, 0.8672999739646912, 0.8547999858856201, 0.7814000248908997, 0.7667999863624573, 0.7598000168800354, 0.7390000224113464, 0.7268000245094299, 0.7085999846458435, 0.6826000213623047, 0.680400013923645, 0.6402000188827515, 0.6281999945640564, 0.5976999998092651, 0.5961999893188477, 0.5788999795913696, 0.5680000185966492, 0.5651000142097473, 0.28519999980926514, 0.21379999816417694, 0.27570000290870667, 0.357699990272522, 0.9775000214576721, 0.9750000238418579, 0.972100019454956, 0.9700999855995178, 0.9639000296592712, 0.9563000202178955, 0.9560999870300293, 0.9538000226020813, 0.9492999911308289, 0.9373999834060669, 0.9312999844551086, 0.9308000206947327, 0.9235000014305115, 0.9223999977111816, 0.9110000133514404, 0.8665000200271606, 0.853600025177002, 0.8478000164031982, 0.8374999761581421, 0.8342000246047974, 0.819100022315979, 0.7116000056266785, 0.6958000063896179, 0.6312000155448914, 0.6294999718666077, 0.5264999866485596, 0.4584999978542328, 0.4375999867916107, 0.4316999912261963, 0.3905999958515167, -0.00930000003427267, 0.1981000006198883, -0.2337000072002411, 0.15440000593662262, 1.0902999639511108, 1.0885000228881836, 1.0875999927520752, 1.086400032043457, 1.062999963760376, 1.055400013923645, 1.0535000562667847, 1.024999976158142, 0.9951000213623047, 0.9901999831199646, 0.9632999897003174, 0.9254000186920166, 0.9253000020980835, 0.9210000038146973, 0.9035999774932861, 0.8723999857902527, 0.862500011920929, 0.8348000049591064, 0.8090000152587891, 0.7777000069618225, 0.7685999870300293, 0.7554000020027161, 0.6489999890327454, 0.6378999948501587, 0.6154000163078308, 0.5558000206947327, 0.501800000667572, 0.5005999803543091, 0.4602000117301941, 0.3756999969482422, 0.3727000057697296, 0.3391000032424927, 0.06970000267028809, -0.4106999933719635, -0.16539999842643738], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.9316000938415527, -4.229000091552734, -3.871500015258789, -4.068399906158447, -4.188300132751465, -3.7583999633789062, -3.994999885559082, -4.274400234222412, -3.722399950027466, -3.3840999603271484, -3.1161999702453613, -3.658900022506714, -3.987499952316284, -4.3592000007629395, -3.902899980545044, -4.361499786376953, -4.176700115203857, -4.115499973297119, -4.434800148010254, -4.519199848175049, -3.863600015640259, -3.699199914932251, -3.405900001525879, -4.0040998458862305, -4.31879997253418, -4.373199939727783, -3.905400037765503, -4.06220006942749, -4.192399978637695, -3.792099952697754, -3.3840999603271484, -4.024199962615967, -4.1656999588012695, -4.354300022125244, -3.4042999744415283, -3.4065001010894775, -3.40910005569458, -3.8873000144958496, -3.392899990081787, -3.3534998893737793, -4.3850998878479, -4.38700008392334, -3.382999897003174, -4.400899887084961, -4.406199932098389, -3.652899980545044, -4.3531999588012695, -3.2764999866485596, -4.109600067138672, -4.291299819946289, -3.4030001163482666, -3.5185000896453857, -4.315000057220459, -3.974100112915039, -3.1561999320983887, -4.200399875640869, -3.78410005569458, -4.5584001541137695, -3.6840999126434326, -3.936800003051758, -3.9714999198913574, -4.622200012207031, -4.3256001472473145, -4.613500118255615, -3.6786000728607178, -4.243299961090088, -3.66510009765625, -4.3302001953125, -3.9776999950408936, -4.097400188446045, -4.098400115966797, -4.099899768829346, -3.533799886703491, -3.965100049972534, -4.074100017547607, -2.9918999671936035, -4.205699920654297, -3.9863998889923096, -2.737799882888794, -4.044400215148926, -4.284800052642822, -4.160799980163574, -3.92930006980896, -3.2026000022888184, -4.242099761962891, -4.144000053405762, -3.702699899673462, -2.653599977493286, -3.845099925994873, -4.008900165557861, -4.236800193786621, -4.408100128173828, -4.577300071716309, -4.35830020904541, -4.668900012969971, -4.539299964904785, -4.164000034332275, -4.481299877166748, -4.315700054168701, -3.8987998962402344, -4.287499904632568, -4.079999923706055, -4.251699924468994]}, \"token.table\": {\"Topic\": [2, 3, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \"Freq\": [0.033332839608192444, 0.9333195090293884, 0.7553437352180481, 0.174310103058815, 0.1809207946062088, 0.6633762121200562, 0.1809207946062088, 0.9613547921180725, 0.35602232813835144, 0.04450279101729393, 0.6230390667915344, 0.9598531126976013, 0.047992657870054245, 0.8455802202224731, 0.10569752752780914, 0.035232510417699814, 0.44827887415885925, 0.32961681485176086, 0.21095477044582367, 0.961593508720398, 0.10006687790155411, 0.8755851984024048, 0.025016719475388527, 0.8389327526092529, 0.1398221254348755, 0.8283852934837341, 0.04359922558069229, 0.17439690232276917, 0.9188529849052429, 0.09672136604785919, 0.048360683023929596, 0.7018024325370789, 0.12760043144226074, 0.12760043144226074, 0.031179148703813553, 0.2702192962169647, 0.7067273855209351, 0.10827840864658356, 0.05413920432329178, 0.8120880722999573, 0.8922765254974365, 0.05948510020971298, 0.05948510020971298, 0.6721106767654419, 0.16130656003952026, 0.16130656003952026, 0.1452157199382782, 0.8228890895843506, 0.7046231627464294, 0.08807789534330368, 0.17615579068660736, 0.05091813951730728, 0.9165264964103699, 0.36295434832572937, 0.6049239635467529, 0.2611846625804901, 0.10447386652231216, 0.6268432140350342, 0.21431386470794678, 0.06123253330588341, 0.7347903847694397, 0.23645533621311188, 0.07881844788789749, 0.6699568033218384, 0.10233992338180542, 0.8698893785476685, 0.025584980845451355, 0.4190676510334015, 0.11640768498182297, 0.4423491954803467, 0.22889243066310883, 0.22889243066310883, 0.5493418574333191, 0.8996520638465881, 0.052920710295438766, 0.5888224244117737, 0.19627414643764496, 0.23552897572517395, 0.7122601866722107, 0.11871002614498138, 0.17806504666805267, 0.839684247970581, 0.13994736969470978, 0.8466281890869141, 0.09406979382038116, 0.05644187703728676, 0.21760468184947968, 0.01978224329650402, 0.7715075016021729, 0.31033995747566223, 0.5360417366027832, 0.1410636156797409, 0.20339655876159668, 0.06779885292053223, 0.7118879556655884, 0.927408754825592, 0.0772840604186058, 0.048730410635471344, 0.09746082127094269, 0.877147376537323, 0.7481414079666138, 0.23019735515117645, 0.05754933878779411, 0.8539519309997559, 0.02511623315513134, 0.10046493262052536, 0.8133693337440491, 0.18770061433315277, 0.6033251881599426, 0.05246305838227272, 0.34100988507270813, 0.060982219874858856, 0.12196443974971771, 0.8537511229515076, 0.10044717788696289, 0.6277948617935181, 0.27622973918914795, 0.2958800196647644, 0.1775280237197876, 0.5325840711593628, 0.15882042050361633, 0.7941020727157593, 0.052940137684345245, 0.19535215198993683, 0.04883803799748421, 0.7325705289840698, 0.10488977283239365, 0.8915630578994751, 0.6150311827659607, 0.28386053442955017, 0.09462017565965652, 0.911852240562439, 0.0701424777507782, 0.152779683470726, 0.8657515645027161, 0.050926562398672104, 0.0671699270606041, 0.8508190512657166, 0.0671699270606041, 0.3208479583263397, 0.5833598971366882, 0.11667197942733765, 0.04325569421052933, 0.9083696007728577, 0.08651138842105865, 0.06579040735960007, 0.8552752733230591, 0.13158081471920013, 0.6060843467712402, 0.24243374168872833, 0.15152108669281006, 0.6602222323417664, 0.06002020463347435, 0.28009429574012756, 0.040830619633197784, 0.1088816449046135, 0.843832790851593, 0.5923007726669312, 0.13936488330364227, 0.27872976660728455, 0.05667293444275856, 0.906766951084137, 0.1688409447669983, 0.04221023619174957, 0.8019945025444031, 0.05008987709879875, 0.5008987784385681, 0.45080891251564026, 0.08062943071126938, 0.8869237899780273, 0.026876477524638176, 0.9241126179695129, 0.044005364179611206, 0.044005364179611206, 0.9122141599655151, 0.0701703205704689, 0.25572332739830017, 0.2922552525997162, 0.4749147593975067, 0.26690301299095154, 0.6820854544639587, 0.02965588867664337, 0.08060538023710251, 0.8866591453552246, 0.026868458837270737, 0.061544645577669144, 0.12308929115533829, 0.8000803589820862, 0.8799919486045837, 0.06285656988620758, 0.961199164390564, 0.48625069856643677, 0.1496156007051468, 0.3740389943122864, 0.10592900961637497, 0.5296450257301331, 0.3707515299320221, 0.9611472487449646, 0.6299092769622803, 0.1453636884689331, 0.24227280914783478, 0.20592530071735382, 0.7825161218643188, 0.041185058653354645, 0.18975423276424408, 0.7827361822128296, 0.03913593664765358, 0.5087671279907227, 0.4304952919483185, 0.62921541929245, 0.13982564210891724, 0.20973846316337585, 0.032902054488658905, 0.8554534316062927, 0.09870616346597672, 0.9474579095840454, 0.0631638616323471, 0.4565770924091339, 0.39950495958328247, 0.1426803469657898, 0.03733000159263611, 0.07466000318527222, 0.8959200382232666, 0.2284095287322998, 0.6852285861968994, 0.0913638100028038, 0.7830545902252197, 0.11745818704366684, 0.07830546051263809, 0.8273594975471497, 0.0954645574092865, 0.06364303827285767, 0.23837874829769135, 0.387365460395813, 0.3575681149959564, 0.05192093178629875, 0.41536745429039, 0.5192093253135681, 0.6829846501350403, 0.260184645652771, 0.032523080706596375, 0.08065429329872131, 0.8871971964836121, 0.13704347610473633, 0.3426086902618408, 0.5139130353927612, 0.1297246217727661, 0.38917383551597595, 0.4756569266319275, 0.22154396772384644, 0.7754038572311401, 0.058417439460754395, 0.8178441524505615, 0.11683487892150879, 0.1969582438468933, 0.7699276804924011, 0.03581058979034424, 0.7071807384490967, 0.23572690784931183, 0.047145381569862366], \"Term\": [\"affect\", \"affect\", \"amaz\", \"amaz\", \"ask\", \"ask\", \"ask\", \"basi\", \"break\", \"break\", \"break\", \"call\", \"call\", \"care\", \"care\", \"care\", \"case\", \"case\", \"case\", \"cast\", \"caus\", \"caus\", \"caus\", \"ceas\", \"ceas\", \"china\", \"china\", \"china\", \"confirm\", \"confirm\", \"confirm\", \"corona\", \"corona\", \"corona\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"cover\", \"cover\", \"cover\", \"crisi\", \"crisi\", \"crisi\", \"death\", \"death\", \"death\", \"discrimin\", \"discrimin\", \"doctor\", \"doctor\", \"doctor\", \"either\", \"either\", \"enough\", \"enough\", \"even\", \"even\", \"even\", \"everyon\", \"everyon\", \"everyon\", \"face\", \"face\", \"face\", \"father\", \"father\", \"father\", \"fight\", \"fight\", \"fight\", \"first\", \"first\", \"first\", \"global\", \"global\", \"govern\", \"govern\", \"govern\", \"great\", \"great\", \"great\", \"gullibl\", \"gullibl\", \"health\", \"health\", \"health\", \"help\", \"help\", \"help\", \"home\", \"home\", \"home\", \"hospit\", \"hospit\", \"hospit\", \"human\", \"human\", \"india\", \"india\", \"india\", \"isol\", \"isol\", \"isol\", \"keep\", \"keep\", \"keep\", \"life\", \"life\", \"like\", \"like\", \"like\", \"look\", \"look\", \"look\", \"make\", \"make\", \"make\", \"mani\", \"mani\", \"mani\", \"mask\", \"mask\", \"mask\", \"medic\", \"medic\", \"medic\", \"murder\", \"murder\", \"must\", \"must\", \"must\", \"narrat\", \"narrat\", \"nation\", \"nation\", \"nation\", \"natur\", \"natur\", \"natur\", \"need\", \"need\", \"need\", \"never\", \"never\", \"never\", \"often\", \"often\", \"often\", \"order\", \"order\", \"order\", \"pandem\", \"pandem\", \"pandem\", \"peopl\", \"peopl\", \"peopl\", \"pleas\", \"pleas\", \"pleas\", \"polic\", \"polic\", \"poor\", \"poor\", \"poor\", \"posit\", \"posit\", \"posit\", \"poverti\", \"poverti\", \"poverti\", \"presid\", \"presid\", \"presid\", \"propaganda\", \"propaganda\", \"protect\", \"protect\", \"protect\", \"public\", \"public\", \"public\", \"racism\", \"racism\", \"racism\", \"recommend\", \"recommend\", \"recommend\", \"releas\", \"releas\", \"religion\", \"report\", \"report\", \"report\", \"respons\", \"respons\", \"respons\", \"rich\", \"right\", \"right\", \"right\", \"say\", \"say\", \"say\", \"speak\", \"speak\", \"spread\", \"spread\", \"spread\", \"state\", \"state\", \"state\", \"stay\", \"stay\", \"stay\", \"support\", \"support\", \"take\", \"take\", \"take\", \"test\", \"test\", \"test\", \"thank\", \"thank\", \"thank\", \"think\", \"think\", \"think\", \"time\", \"time\", \"time\", \"today\", \"today\", \"today\", \"total\", \"total\", \"total\", \"trump\", \"trump\", \"trump\", \"trut\", \"trut\", \"virus\", \"virus\", \"virus\", \"want\", \"want\", \"want\", \"watch\", \"watch\", \"week\", \"week\", \"week\", \"work\", \"work\", \"work\", \"worker\", \"worker\", \"worker\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el6057351955275048881060128\", ldavis_el6057351955275048881060128_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el6057351955275048881060128\", ldavis_el6057351955275048881060128_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el6057351955275048881060128\", ldavis_el6057351955275048881060128_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2     -0.014386 -0.151742       1        1  33.903133\n",
       "0     -0.148158  0.086410       2        1  33.809349\n",
       "1      0.162543  0.065332       3        1  32.287518, topic_info=    Category       Freq         Term      Total  loglift  logprob\n",
       "22   Default  73.000000        peopl  73.000000  30.0000  30.0000\n",
       "5    Default  96.000000  coronavirus  96.000000  29.0000  29.0000\n",
       "13   Default  53.000000         test  53.000000  28.0000  28.0000\n",
       "0    Default  53.000000       health  53.000000  27.0000  27.0000\n",
       "6    Default  50.000000         help  50.000000  26.0000  26.0000\n",
       "41   Default  37.000000         trut  37.000000  25.0000  25.0000\n",
       "38   Default  37.000000      poverti  37.000000  24.0000  24.0000\n",
       "36   Default  38.000000       murder  38.000000  23.0000  23.0000\n",
       "39   Default  37.000000       racism  37.000000  22.0000  22.0000\n",
       "35   Default  39.000000       father  39.000000  21.0000  21.0000\n",
       "34   Default  39.000000         caus  39.000000  20.0000  20.0000\n",
       "57   Default  30.000000       affect  30.000000  19.0000  19.0000\n",
       "42   Default  55.000000         work  55.000000  18.0000  18.0000\n",
       "37   Default  44.000000        natur  44.000000  17.0000  17.0000\n",
       "7    Default  39.000000         keep  39.000000  16.0000  16.0000\n",
       "40   Default  42.000000        speak  42.000000  15.0000  15.0000\n",
       "18   Default  25.000000        human  25.000000  14.0000  14.0000\n",
       "4    Default  30.000000         stay  30.000000  13.0000  13.0000\n",
       "19   Default  18.000000         rich  18.000000  12.0000  12.0000\n",
       "91   Default  20.000000         call  20.000000  11.0000  11.0000\n",
       "48   Default  28.000000         care  28.000000  10.0000  10.0000\n",
       "102  Default  22.000000       presid  22.000000   9.0000   9.0000\n",
       "24   Default  37.000000         mask  37.000000   8.0000   8.0000\n",
       "66   Default  19.000000       either  19.000000   7.0000   7.0000\n",
       "50   Default  31.000000         time  31.000000   6.0000   6.0000\n",
       "84   Default  23.000000        never  23.000000   5.0000   5.0000\n",
       "69   Default  16.000000     religion  16.000000   4.0000   4.0000\n",
       "63   Default  16.000000         basi  16.000000   3.0000   3.0000\n",
       "64   Default  16.000000         cast  16.000000   2.0000   2.0000\n",
       "16   Default  49.000000       pandem  49.000000   1.0000   1.0000\n",
       "91    Topic1  19.780396         call  20.836521   1.0296  -3.9316\n",
       "33    Topic1  14.691784      support  15.831838   1.0069  -4.2290\n",
       "102   Topic1  21.007010       presid  22.724503   1.0031  -3.8715\n",
       "15    Topic1  17.252590       global  18.896194   0.9907  -4.0684\n",
       "59    Topic1  15.302350        crisi  16.810932   0.9876  -4.1883\n",
       "18    Topic1  23.521843        human  25.878557   0.9862  -3.7584\n",
       "100   Topic1  18.565525      confirm  20.677954   0.9739  -3.9950\n",
       "108   Topic1  14.040310       releas  15.909236   0.9567  -4.2744\n",
       "48    Topic1  24.384781         care  28.382877   0.9298  -3.7224\n",
       "7     Topic1  34.199615         keep  39.814888   0.9296  -3.3841\n",
       "0     Topic1  44.708942       health  53.152023   0.9087  -3.1162\n",
       "50    Topic1  25.981594         time  31.425276   0.8914  -3.6589\n",
       "60    Topic1  18.705252        china  22.936188   0.8778  -3.9875\n",
       "58    Topic1  12.899161         life  15.982900   0.8673  -4.3592\n",
       "12    Topic1  20.357122        think  25.541004   0.8548  -3.9029\n",
       "45    Topic1  12.868844         isol  17.376394   0.7814  -4.3615\n",
       "75    Topic1  15.481351       worker  21.210985   0.7668  -4.1767\n",
       "14    Topic1  16.457954       doctor  22.707172   0.7598  -4.1155\n",
       "74    Topic1  11.959931        great  16.847776   0.7390  -4.4348\n",
       "88    Topic1  10.991703       corona  15.673927   0.7268  -4.5192\n",
       "106   Topic1  21.173677        trump  30.747396   0.7086  -3.8636\n",
       "46    Topic1  24.956926        death  37.196255   0.6826  -3.6992\n",
       "16    Topic1  33.463131       pandem  49.983170   0.6804  -3.4059\n",
       "3     Topic1  18.397978        state  28.607056   0.6402  -4.0041\n",
       "61    Topic1  13.430640         must  21.137140   0.6282  -4.3188\n",
       "105   Topic1  12.719962        right  20.637892   0.5977  -4.3732\n",
       "2     Topic1  20.307114        order  32.998707   0.5962  -3.9054\n",
       "87    Topic1  17.359852        pleas  28.701633   0.5789  -4.0622\n",
       "43    Topic1  15.240833       govern  25.474573   0.5680  -4.1924\n",
       "29    Topic1  22.742380         like  38.122063   0.5651  -3.7921\n",
       "53    Topic1  34.199764         case  75.845642   0.2852  -3.3841\n",
       "70    Topic1  18.032648        fight  42.952492   0.2138  -4.0242\n",
       "52    Topic1  15.651979         take  35.043369   0.2757  -4.1657\n",
       "47    Topic1  12.962030       report  26.735180   0.3577  -4.3543\n",
       "41    Topic2  33.424129         trut  37.195789   0.9775  -3.4043\n",
       "38    Topic2  33.349098      poverti  37.207256   0.9750  -3.4065\n",
       "39    Topic2  33.262337       racism  37.218361   0.9721  -3.4091\n",
       "84    Topic2  20.620743        never  23.118343   0.9701  -3.8873\n",
       "36    Topic2  33.805511       murder  38.135273   0.9639  -3.3929\n",
       "34    Topic2  35.167175         caus  39.973267   0.9563  -3.3535\n",
       "85    Topic2  12.534838   propaganda  14.251040   0.9561  -4.3851\n",
       "83    Topic2  12.510953       narrat  14.256696   0.9538  -4.3870\n",
       "35    Topic2  34.143753       father  39.085430   0.9493  -3.3830\n",
       "82    Topic2  12.337348      gullibl  14.291086   0.9374  -4.4009\n",
       "81    Topic2  12.273006         ceas  14.303888   0.9313  -4.4062\n",
       "4     Topic2  26.065868         stay  30.393238   0.9308  -3.6529\n",
       "23    Topic2  12.940824        often  15.199784   0.9235  -4.3532\n",
       "37    Topic2  37.980480        natur  44.662846   0.9224  -3.2765\n",
       "28    Topic2  16.509508       nation  19.636118   0.9110  -4.1096\n",
       "86    Topic2  13.766470         week  17.118176   0.8665  -4.2913\n",
       "40    Topic2  33.468052        speak  42.159798   0.8536  -3.4030\n",
       "24    Topic2  29.816553         mask  37.778519   0.8478  -3.5185\n",
       "80    Topic2  13.444814         amaz  17.210707   0.8375  -4.3150\n",
       "11    Topic2  18.905153          say  24.280649   0.8342  -3.9741\n",
       "42    Topic2  42.835728         work  55.849400   0.8191  -3.1562\n",
       "9     Topic2  15.077061        thank  21.890505   0.7116  -4.2004\n",
       "17    Topic2  22.861441       public  33.720116   0.6958  -3.7841\n",
       "90    Topic2  10.539355          ask  16.581842   0.6312  -4.5584\n",
       "72    Topic2  25.267378         make  39.821926   0.6295  -3.6841\n",
       "78    Topic2  19.623920         need  34.284153   0.5265  -3.9368\n",
       "1     Topic2  18.954472         home  35.445000   0.4585  -3.9715\n",
       "62    Topic2   9.888024      respons  18.880569   0.4376  -4.6222\n",
       "10    Topic2  13.302636       spread  25.551964   0.4317  -4.3256\n",
       "104   Topic2   9.974812        posit  19.964113   0.3906  -4.6135\n",
       "53    Topic2  25.405783         case  75.845642  -0.0093  -3.6786\n",
       "52    Topic2  14.443458         take  35.043369   0.1981  -4.2433\n",
       "5     Topic2  25.750349  coronavirus  96.218147  -0.2337  -3.6651\n",
       "30    Topic2  13.241117        today  33.560040   0.1544  -4.3302\n",
       "19    Topic3  17.989117         rich  18.727619   1.0903  -3.9777\n",
       "69    Topic3  15.961074     religion  16.645874   1.0885  -4.0974\n",
       "63    Topic3  15.944503         basi  16.643179   1.0876  -4.0984\n",
       "64    Topic3  15.921195         cast  16.639048   1.0864  -4.0999\n",
       "57    Topic3  28.043045       affect  30.000444   1.0630  -3.5338\n",
       "66    Topic3  18.218298       either  19.639366   1.0554  -3.9651\n",
       "97    Topic3  16.336998        polic  17.645107   1.0535  -4.0741\n",
       "13    Topic3  48.212223         test  53.576210   1.0250  -2.9919\n",
       "79    Topic3  14.322379         look  16.398222   0.9951  -4.2057\n",
       "71    Topic3  17.834116        india  20.521067   0.9902  -3.9864\n",
       "22    Topic3  62.160931        peopl  73.474274   0.9633  -2.7378\n",
       "65    Topic3  16.828932    discrimin  20.658920   0.9254  -4.0444\n",
       "98    Topic3  13.233564    recommend  16.248367   0.9253  -4.2848\n",
       "107   Topic3  14.980608        cover  18.470903   0.9210  -4.1608\n",
       "68    Topic3  18.881344         poor  23.690935   0.9036  -3.9293\n",
       "6     Topic3  39.051041         help  50.550385   0.8724  -3.2026\n",
       "103   Topic3  13.810419        watch  18.055107   0.8625  -4.2421\n",
       "31    Topic3  15.234156        medic  20.475843   0.8348  -4.1440\n",
       "67    Topic3  23.683086      everyon  32.662376   0.8090  -3.7027\n",
       "5     Topic3  67.617088  coronavirus  96.218147   0.7777  -2.6536\n",
       "21    Topic3  20.541674       hospit  29.499023   0.7686  -3.8451\n",
       "101   Topic3  17.437738         face  25.374771   0.7554  -4.0089\n",
       "92    Topic3  13.882986        break  22.470501   0.6490  -4.2368\n",
       "44    Topic3  11.698040         even  19.143543   0.6379  -4.4081\n",
       "55    Topic3   9.876631       enough  16.531004   0.6154  -4.5773\n",
       "20    Topic3  12.295259        first  21.844322   0.5558  -4.3583\n",
       "95    Topic3   9.012115         mani  16.898741   0.5018  -4.6689\n",
       "93    Topic3  10.259156        total  19.260056   0.5006  -4.5393\n",
       "73    Topic3  14.931907        virus  29.187817   0.4602  -4.1640\n",
       "32    Topic3  10.871768         want  23.125912   0.3757  -4.4813\n",
       "96    Topic3  12.830440      protect  27.373333   0.3727  -4.3157\n",
       "70    Topic3  19.466085        fight  42.952492   0.3391  -3.8988\n",
       "29    Topic3  13.197744         like  38.122063   0.0697  -4.2875\n",
       "53    Topic3  16.240099         case  75.845642  -0.4107  -4.0800\n",
       "16    Topic3  13.677947       pandem  49.983170  -0.1654  -4.2517, token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "57        2  0.033333       affect\n",
       "57        3  0.933320       affect\n",
       "80        2  0.755344         amaz\n",
       "80        3  0.174310         amaz\n",
       "90        1  0.180921          ask\n",
       "90        2  0.663376          ask\n",
       "90        3  0.180921          ask\n",
       "63        3  0.961355         basi\n",
       "92        1  0.356022        break\n",
       "92        2  0.044503        break\n",
       "92        3  0.623039        break\n",
       "91        1  0.959853         call\n",
       "91        3  0.047993         call\n",
       "48        1  0.845580         care\n",
       "48        2  0.105698         care\n",
       "48        3  0.035233         care\n",
       "53        1  0.448279         case\n",
       "53        2  0.329617         case\n",
       "53        3  0.210955         case\n",
       "64        3  0.961594         cast\n",
       "34        1  0.100067         caus\n",
       "34        2  0.875585         caus\n",
       "34        3  0.025017         caus\n",
       "81        2  0.838933         ceas\n",
       "81        3  0.139822         ceas\n",
       "60        1  0.828385        china\n",
       "60        2  0.043599        china\n",
       "60        3  0.174397        china\n",
       "100       1  0.918853      confirm\n",
       "100       2  0.096721      confirm\n",
       "100       3  0.048361      confirm\n",
       "88        1  0.701802       corona\n",
       "88        2  0.127600       corona\n",
       "88        3  0.127600       corona\n",
       "5         1  0.031179  coronavirus\n",
       "5         2  0.270219  coronavirus\n",
       "5         3  0.706727  coronavirus\n",
       "107       1  0.108278        cover\n",
       "107       2  0.054139        cover\n",
       "107       3  0.812088        cover\n",
       "59        1  0.892277        crisi\n",
       "59        2  0.059485        crisi\n",
       "59        3  0.059485        crisi\n",
       "46        1  0.672111        death\n",
       "46        2  0.161307        death\n",
       "46        3  0.161307        death\n",
       "65        1  0.145216    discrimin\n",
       "65        3  0.822889    discrimin\n",
       "14        1  0.704623       doctor\n",
       "14        2  0.088078       doctor\n",
       "14        3  0.176156       doctor\n",
       "66        2  0.050918       either\n",
       "66        3  0.916526       either\n",
       "55        2  0.362954       enough\n",
       "55        3  0.604924       enough\n",
       "44        1  0.261185         even\n",
       "44        2  0.104474         even\n",
       "44        3  0.626843         even\n",
       "67        1  0.214314      everyon\n",
       "67        2  0.061233      everyon\n",
       "67        3  0.734790      everyon\n",
       "101       1  0.236455         face\n",
       "101       2  0.078818         face\n",
       "101       3  0.669957         face\n",
       "35        1  0.102340       father\n",
       "35        2  0.869889       father\n",
       "35        3  0.025585       father\n",
       "70        1  0.419068        fight\n",
       "70        2  0.116408        fight\n",
       "70        3  0.442349        fight\n",
       "20        1  0.228892        first\n",
       "20        2  0.228892        first\n",
       "20        3  0.549342        first\n",
       "15        1  0.899652       global\n",
       "15        3  0.052921       global\n",
       "43        1  0.588822       govern\n",
       "43        2  0.196274       govern\n",
       "43        3  0.235529       govern\n",
       "74        1  0.712260        great\n",
       "74        2  0.118710        great\n",
       "74        3  0.178065        great\n",
       "82        2  0.839684      gullibl\n",
       "82        3  0.139947      gullibl\n",
       "0         1  0.846628       health\n",
       "0         2  0.094070       health\n",
       "0         3  0.056442       health\n",
       "6         1  0.217605         help\n",
       "6         2  0.019782         help\n",
       "6         3  0.771508         help\n",
       "1         1  0.310340         home\n",
       "1         2  0.536042         home\n",
       "1         3  0.141064         home\n",
       "21        1  0.203397       hospit\n",
       "21        2  0.067799       hospit\n",
       "21        3  0.711888       hospit\n",
       "18        1  0.927409        human\n",
       "18        3  0.077284        human\n",
       "71        1  0.048730        india\n",
       "71        2  0.097461        india\n",
       "71        3  0.877147        india\n",
       "45        1  0.748141         isol\n",
       "45        2  0.230197         isol\n",
       "45        3  0.057549         isol\n",
       "7         1  0.853952         keep\n",
       "7         2  0.025116         keep\n",
       "7         3  0.100465         keep\n",
       "58        1  0.813369         life\n",
       "58        3  0.187701         life\n",
       "29        1  0.603325         like\n",
       "29        2  0.052463         like\n",
       "29        3  0.341010         like\n",
       "79        1  0.060982         look\n",
       "79        2  0.121964         look\n",
       "79        3  0.853751         look\n",
       "72        1  0.100447         make\n",
       "72        2  0.627795         make\n",
       "72        3  0.276230         make\n",
       "95        1  0.295880         mani\n",
       "95        2  0.177528         mani\n",
       "95        3  0.532584         mani\n",
       "24        1  0.158820         mask\n",
       "24        2  0.794102         mask\n",
       "24        3  0.052940         mask\n",
       "31        1  0.195352        medic\n",
       "31        2  0.048838        medic\n",
       "31        3  0.732571        medic\n",
       "36        1  0.104890       murder\n",
       "36        2  0.891563       murder\n",
       "61        1  0.615031         must\n",
       "61        2  0.283861         must\n",
       "61        3  0.094620         must\n",
       "83        2  0.911852       narrat\n",
       "83        3  0.070142       narrat\n",
       "28        1  0.152780       nation\n",
       "28        2  0.865752       nation\n",
       "28        3  0.050927       nation\n",
       "37        1  0.067170        natur\n",
       "37        2  0.850819        natur\n",
       "37        3  0.067170        natur\n",
       "78        1  0.320848         need\n",
       "78        2  0.583360         need\n",
       "78        3  0.116672         need\n",
       "84        1  0.043256        never\n",
       "84        2  0.908370        never\n",
       "84        3  0.086511        never\n",
       "23        1  0.065790        often\n",
       "23        2  0.855275        often\n",
       "23        3  0.131581        often\n",
       "2         1  0.606084        order\n",
       "2         2  0.242434        order\n",
       "2         3  0.151521        order\n",
       "16        1  0.660222       pandem\n",
       "16        2  0.060020       pandem\n",
       "16        3  0.280094       pandem\n",
       "22        1  0.040831        peopl\n",
       "22        2  0.108882        peopl\n",
       "22        3  0.843833        peopl\n",
       "87        1  0.592301        pleas\n",
       "87        2  0.139365        pleas\n",
       "87        3  0.278730        pleas\n",
       "97        1  0.056673        polic\n",
       "97        3  0.906767        polic\n",
       "68        1  0.168841         poor\n",
       "68        2  0.042210         poor\n",
       "68        3  0.801995         poor\n",
       "104       1  0.050090        posit\n",
       "104       2  0.500899        posit\n",
       "104       3  0.450809        posit\n",
       "38        1  0.080629      poverti\n",
       "38        2  0.886924      poverti\n",
       "38        3  0.026876      poverti\n",
       "102       1  0.924113       presid\n",
       "102       2  0.044005       presid\n",
       "102       3  0.044005       presid\n",
       "85        2  0.912214   propaganda\n",
       "85        3  0.070170   propaganda\n",
       "96        1  0.255723      protect\n",
       "96        2  0.292255      protect\n",
       "96        3  0.474915      protect\n",
       "17        1  0.266903       public\n",
       "17        2  0.682085       public\n",
       "17        3  0.029656       public\n",
       "39        1  0.080605       racism\n",
       "39        2  0.886659       racism\n",
       "39        3  0.026868       racism\n",
       "98        1  0.061545    recommend\n",
       "98        2  0.123089    recommend\n",
       "98        3  0.800080    recommend\n",
       "108       1  0.879992       releas\n",
       "108       3  0.062857       releas\n",
       "69        3  0.961199     religion\n",
       "47        1  0.486251       report\n",
       "47        2  0.149616       report\n",
       "47        3  0.374039       report\n",
       "62        1  0.105929      respons\n",
       "62        2  0.529645      respons\n",
       "62        3  0.370752      respons\n",
       "19        3  0.961147         rich\n",
       "105       1  0.629909        right\n",
       "105       2  0.145364        right\n",
       "105       3  0.242273        right\n",
       "11        1  0.205925          say\n",
       "11        2  0.782516          say\n",
       "11        3  0.041185          say\n",
       "40        1  0.189754        speak\n",
       "40        2  0.782736        speak\n",
       "10        1  0.039136       spread\n",
       "10        2  0.508767       spread\n",
       "10        3  0.430495       spread\n",
       "3         1  0.629215        state\n",
       "3         2  0.139826        state\n",
       "3         3  0.209738        state\n",
       "4         1  0.032902         stay\n",
       "4         2  0.855453         stay\n",
       "4         3  0.098706         stay\n",
       "33        1  0.947458      support\n",
       "33        2  0.063164      support\n",
       "52        1  0.456577         take\n",
       "52        2  0.399505         take\n",
       "52        3  0.142680         take\n",
       "13        1  0.037330         test\n",
       "13        2  0.074660         test\n",
       "13        3  0.895920         test\n",
       "9         1  0.228410        thank\n",
       "9         2  0.685229        thank\n",
       "9         3  0.091364        thank\n",
       "12        1  0.783055        think\n",
       "12        2  0.117458        think\n",
       "12        3  0.078305        think\n",
       "50        1  0.827359         time\n",
       "50        2  0.095465         time\n",
       "50        3  0.063643         time\n",
       "30        1  0.238379        today\n",
       "30        2  0.387365        today\n",
       "30        3  0.357568        today\n",
       "93        1  0.051921        total\n",
       "93        2  0.415367        total\n",
       "93        3  0.519209        total\n",
       "106       1  0.682985        trump\n",
       "106       2  0.260185        trump\n",
       "106       3  0.032523        trump\n",
       "41        1  0.080654         trut\n",
       "41        2  0.887197         trut\n",
       "73        1  0.137043        virus\n",
       "73        2  0.342609        virus\n",
       "73        3  0.513913        virus\n",
       "32        1  0.129725         want\n",
       "32        2  0.389174         want\n",
       "32        3  0.475657         want\n",
       "103       2  0.221544        watch\n",
       "103       3  0.775404        watch\n",
       "86        1  0.058417         week\n",
       "86        2  0.817844         week\n",
       "86        3  0.116835         week\n",
       "42        1  0.196958         work\n",
       "42        2  0.769928         work\n",
       "42        3  0.035811         work\n",
       "75        1  0.707181       worker\n",
       "75        2  0.235727       worker\n",
       "75        3  0.047145       worker, R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 1, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the visualizations, it'd be best to create 3 clusters instead of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2: Modeling using TF-IDF\n",
    "TF-IDF intends to reflect on the importance of each word in the tweet amongst other tweets. Thus it tries to create a better model instead of using mere Term Frequency as in Bag of words model. However, for TF-IDF to work it needs to have a good size of text in each document. However, tweet is usually very small in size. Thus, most of the times each word ends up being mentioned only once. Thus, TF-IDF doesn't work better for short texts. However, let's train the model and evaluate the performance and see how does it perform.\n",
    "\n",
    "### 4.2.1. Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train lda model using corpus_tfidf\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(tfidf_corpus, \n",
    "                                             num_topics=3, \n",
    "                                             id2word = dictionary, \n",
    "                                             passes = 2, \n",
    "                                             workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.058*\"peopl\" + 0.035*\"fight\" + 0.033*\"help\" + 0.028*\"medic\" + 0.025*\"live\" + 0.023*\"crisi\" + 0.022*\"equip\" + 0.021*\"virus\" + 0.021*\"corona\" + 0.020*\"first\"\n",
      "\n",
      "\n",
      "Topic: 1 Word: 0.038*\"case\" + 0.037*\"make\" + 0.036*\"work\" + 0.031*\"health\" + 0.027*\"natur\" + 0.026*\"caus\" + 0.025*\"murder\" + 0.025*\"speak\" + 0.025*\"father\" + 0.025*\"racism\"\n",
      "\n",
      "\n",
      "Topic: 2 Word: 0.046*\"coronavirus\" + 0.032*\"take\" + 0.032*\"test\" + 0.027*\"death\" + 0.027*\"time\" + 0.026*\"pandem\" + 0.026*\"state\" + 0.024*\"stay\" + 0.023*\"need\" + 0.023*\"home\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the words occuring in that topic and its relative weight\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is my attempt at generalizing the topics from their corresponding relevant terms.  \n",
    "Topic 0: President Trump's announcements around COVID19.   \n",
    "Topic 1: Quarantining and stopping the spread of the disease.  \n",
    "Topic 2: Impact of COVID on people\n",
    "\n",
    "### 4.2.2. Visualization using pyLDAVis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el6057351959156005476006804\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el6057351959156005476006804_data = {\"mdsDat\": {\"x\": [0.125581164284176, -0.1411646081023001, 0.015583443818124064], \"y\": [-0.0715933155653101, -0.05024050644526521, 0.12183382201057537], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [36.171600341796875, 33.46841049194336, 30.35999298095703]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [36.0, 19.0, 14.0, 22.0, 22.0, 13.0, 13.0, 13.0, 13.0, 24.0, 14.0, 14.0, 15.0, 16.0, 12.0, 12.0, 12.0, 16.0, 10.0, 23.0, 12.0, 17.0, 14.0, 12.0, 17.0, 24.0, 12.0, 23.0, 10.0, 9.0, 17.659582138061523, 5.933657646179199, 11.212878227233887, 10.365225791931152, 4.977749824523926, 4.973165035247803, 4.956141471862793, 14.094548225402832, 6.334322452545166, 11.427966117858887, 14.5298490524292, 14.672398567199707, 6.8358869552612305, 6.968005657196045, 12.989102363586426, 8.486095428466797, 6.870057582855225, 8.52428150177002, 17.588773727416992, 11.248610496520996, 6.82330846786499, 12.103111267089844, 5.599060535430908, 11.436920166015625, 7.290961742401123, 12.444029808044434, 7.380437850952148, 12.547183990478516, 6.387869834899902, 5.826513290405273, 25.298416137695312, 14.427181243896484, 10.13642692565918, 11.067523002624512, 7.7670063972473145, 7.111123085021973, 8.252737045288086, 6.690846920013428, 12.552398681640625, 12.82898235321045, 12.452969551086426, 12.425315856933594, 11.172868728637695, 11.202670097351074, 12.71386432647705, 12.976616859436035, 10.653278350830078, 12.803122520446777, 13.445651054382324, 8.739794731140137, 18.194887161254883, 11.056292533874512, 18.537151336669922, 9.786014556884766, 7.636042594909668, 11.464557647705078, 8.582125663757324, 15.92562198638916, 10.410787582397461, 8.390475273132324, 19.276201248168945, 9.892618179321289, 7.140252590179443, 4.873742580413818, 7.027219295501709, 9.372483253479004, 6.5546135902404785, 6.657851696014404, 7.989250183105469, 8.179936408996582, 6.8342814445495605, 9.659955024719238, 9.436441421508789, 12.923364639282227, 10.685988426208496, 8.924057006835938, 8.115084648132324, 5.2999091148376465, 5.280897617340088, 5.273726940155029, 5.259282112121582, 10.131210327148438, 5.638590335845947, 11.385725975036621, 15.907127380371094, 26.450929641723633, 9.259025573730469, 5.342757225036621, 9.655165672302246, 15.067983627319336, 6.970176696777344, 7.999842166900635, 6.732932090759277, 8.34106731414795, 6.817903995513916, 6.796658039093018, 6.903217315673828, 9.092952728271484, 8.286643981933594, 8.123536109924316, 5.6568756103515625, 5.291884899139404, 8.092572212219238, 6.443216800689697, 7.539333343505859, 6.864372730255127, 5.949557304382324, 6.1528520584106445, 5.909519672393799, 6.092334747314453], \"Term\": [\"peopl\", \"take\", \"medic\", \"fight\", \"work\", \"murder\", \"racism\", \"trut\", \"poverti\", \"make\", \"caus\", \"father\", \"speak\", \"natur\", \"crisi\", \"april\", \"worker\", \"state\", \"corona\", \"health\", \"equip\", \"time\", \"live\", \"lockdown\", \"death\", \"test\", \"everyon\", \"help\", \"polic\", \"support\", \"take\", \"rich\", \"everyon\", \"human\", \"cast\", \"religion\", \"basi\", \"state\", \"either\", \"die\", \"time\", \"death\", \"cover\", \"poor\", \"stay\", \"face\", \"watch\", \"respons\", \"test\", \"thank\", \"releas\", \"keep\", \"recommend\", \"trump\", \"countri\", \"home\", \"right\", \"need\", \"governor\", \"global\", \"coronavirus\", \"pandem\", \"patient\", \"mask\", \"want\", \"world\", \"peopl\", \"order\", \"racism\", \"murder\", \"trut\", \"poverti\", \"april\", \"worker\", \"father\", \"caus\", \"lockdown\", \"speak\", \"natur\", \"look\", \"work\", \"send\", \"make\", \"think\", \"mani\", \"say\", \"china\", \"health\", \"govern\", \"break\", \"case\", \"spread\", \"doctor\", \"isol\", \"protect\", \"like\", \"report\", \"care\", \"today\", \"mask\", \"public\", \"coronavirus\", \"corona\", \"medic\", \"crisi\", \"polic\", \"support\", \"narrat\", \"gullibl\", \"propaganda\", \"ceas\", \"equip\", \"often\", \"live\", \"fight\", \"peopl\", \"first\", \"amaz\", \"virus\", \"help\", \"india\", \"nurs\", \"great\", \"call\", \"never\", \"affect\", \"week\", \"order\", \"hospit\", \"public\", \"must\", \"ask\", \"today\", \"need\", \"coronavirus\", \"case\", \"patient\", \"health\", \"pleas\", \"pandem\"], \"Total\": [36.0, 19.0, 14.0, 22.0, 22.0, 13.0, 13.0, 13.0, 13.0, 24.0, 14.0, 14.0, 15.0, 16.0, 12.0, 12.0, 12.0, 16.0, 10.0, 23.0, 12.0, 17.0, 14.0, 12.0, 17.0, 24.0, 12.0, 23.0, 10.0, 9.0, 19.208038330078125, 6.652559280395508, 12.605786323547363, 11.825359344482422, 5.691977500915527, 5.692140579223633, 5.692017078399658, 16.338031768798828, 7.355520725250244, 13.435561180114746, 17.45566749572754, 17.954208374023438, 8.404796600341797, 8.63464069366455, 16.87898826599121, 11.068268775939941, 9.011629104614258, 11.580560684204102, 24.58159637451172, 15.72407341003418, 9.572279930114746, 17.309911727905273, 8.105371475219727, 17.323793411254883, 11.059163093566895, 18.91062355041504, 11.68410587310791, 20.152862548828125, 10.410135269165039, 9.729103088378906, 42.497703552246094, 24.902013778686523, 17.16083526611328, 21.104475021362305, 13.630584716796875, 12.648938179016113, 36.83692169189453, 16.76314353942871, 13.507222175598145, 13.874109268188477, 13.512648582458496, 13.513633728027344, 12.435266494750977, 12.491803169250488, 14.33846664428711, 14.69499683380127, 12.271270751953125, 15.135187149047852, 16.107210159301758, 10.646060943603516, 22.412683486938477, 13.829117774963379, 24.366823196411133, 12.885740280151367, 10.798439025878906, 16.751968383789062, 12.676894187927246, 23.524871826171875, 15.95789623260498, 13.34332275390625, 31.09610939025879, 16.971195220947266, 12.32107925415039, 9.651171684265137, 14.090086936950684, 19.016250610351562, 14.126409530639648, 14.623332977294922, 17.987096786499023, 21.104475021362305, 15.525460243225098, 42.497703552246094, 10.777017593383789, 14.759588241577148, 12.493765830993652, 10.47789192199707, 9.703176498413086, 6.455967903137207, 6.453492641448975, 6.452975273132324, 6.451362133026123, 12.436580657958984, 7.088575839996338, 14.928815841674805, 22.06653594970703, 36.83692169189453, 13.079504013061523, 7.565462589263916, 14.469100952148438, 23.040428161621094, 10.707643508911133, 12.666068077087402, 10.693893432617188, 13.372030258178711, 11.103296279907227, 11.559944152832031, 12.416561126708984, 16.76314353942871, 15.787956237792969, 15.525460243225098, 11.47004508972168, 11.301708221435547, 17.987096786499023, 20.152862548828125, 42.497703552246094, 31.09610939025879, 17.16083526611328, 23.524871826171875, 16.59719467163086, 24.902013778686523], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9327999949455261, 0.9024999737739563, 0.8998000025749207, 0.8851000070571899, 0.8827999830245972, 0.8819000124931335, 0.8784999847412109, 0.8691999912261963, 0.8673999905586243, 0.8550999760627747, 0.8334000110626221, 0.8149999976158142, 0.8102999925613403, 0.8023999929428101, 0.7548999786376953, 0.7512000203132629, 0.7455999851226807, 0.7105000019073486, 0.682200014591217, 0.6819000244140625, 0.6783999800682068, 0.6590999960899353, 0.6470000147819519, 0.6017000079154968, 0.6003000140190125, 0.5983999967575073, 0.5575000047683716, 0.5429999828338623, 0.5285000205039978, 0.5041999816894531, 0.498199999332428, 0.47110000252723694, 0.4903999865055084, 0.37139999866485596, 0.4544999897480011, 0.4410000145435333, -0.47909998893737793, 0.09849999845027924, 1.021299958229065, 1.0162999629974365, 1.0128999948501587, 1.010599970817566, 0.987500011920929, 0.9855999946594238, 0.9743000268936157, 0.9702000021934509, 0.9531999826431274, 0.9272000193595886, 0.9139999747276306, 0.8973000049591064, 0.8860999941825867, 0.8708000183105469, 0.8210999965667725, 0.8194000124931335, 0.7480000257492065, 0.7153000235557556, 0.7045000195503235, 0.7044000029563904, 0.6675000190734863, 0.6305999755859375, 0.6164000034332275, 0.5547999739646912, 0.5490000247955322, 0.4113999903202057, 0.39890000224113464, 0.3871000111103058, 0.32670000195503235, 0.3077000081539154, 0.28299999237060547, 0.1467999964952469, 0.27399998903274536, -0.38690000772476196, 1.0592000484466553, 1.0592000484466553, 1.0356999635696411, 1.031499981880188, 1.0132999420166016, 0.994700014591217, 0.9915000200271606, 0.9901999831199646, 0.9876999855041504, 0.9869999885559082, 0.9631999731063843, 0.9211000204086304, 0.8647000193595886, 0.86080002784729, 0.8465999960899353, 0.8442000150680542, 0.7875000238418579, 0.7674000263214111, 0.7627000212669373, 0.7325000166893005, 0.7293999791145325, 0.7200999855995178, 0.7044000029563904, 0.6608999967575073, 0.6050000190734863, 0.5803999900817871, 0.5473999977111816, 0.5443000197410583, 0.4851999878883362, 0.4332999885082245, 0.39329999685287476, 0.05169999971985817, -0.5372999906539917, -0.31869998574256897, 0.13269999623298645, -0.14910000562667847, 0.15940000116825104, -0.2159000039100647], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.432800054550171, -4.523399829864502, -3.88700008392334, -3.96560001373291, -4.699100017547607, -4.699999809265137, -4.703400135040283, -3.6582999229431152, -4.458099842071533, -3.868000030517578, -3.6277999877929688, -3.6180999279022217, -4.381899833679199, -4.36269998550415, -3.7399001121520996, -4.165599822998047, -4.3769001960754395, -4.161099910736084, -3.436800003051758, -3.8838000297546387, -4.383699893951416, -3.8106000423431396, -4.581399917602539, -3.8671998977661133, -4.317399978637695, -3.7827999591827393, -4.305200099945068, -3.7744998931884766, -4.4496002197265625, -4.541600227355957, -3.0732998847961426, -3.6349000930786133, -3.9879000186920166, -3.9000000953674316, -4.254199981689453, -4.342400074005127, -4.19350004196167, -4.403299808502197, -3.696500062942505, -3.6747000217437744, -3.704400062561035, -3.7065999507904053, -3.8129000663757324, -3.8101999759674072, -3.6837000846862793, -3.6631999015808105, -3.8605000972747803, -3.6767001152038574, -3.627700090408325, -4.058499813079834, -3.325200080871582, -3.8234000205993652, -3.3066000938415527, -3.9453999996185303, -4.19350004196167, -3.787100076675415, -4.076700210571289, -3.458400011062622, -3.883500099182129, -4.099299907684326, -3.2674999237060547, -3.9346001148223877, -4.2606000900268555, -4.642499923706055, -4.276599884033203, -3.988600015640259, -4.346199989318848, -4.330599784851074, -4.1483001708984375, -4.12470006942749, -4.3043999671936035, -3.958400011062622, -3.8842999935150146, -3.5699000358581543, -3.759999990463257, -3.9400999546051025, -4.035200119018555, -4.46120023727417, -4.464799880981445, -4.46619987487793, -4.468900203704834, -3.8132998943328857, -4.3993000984191895, -3.696500062942505, -3.3620998859405518, -2.853600025177002, -3.9033000469207764, -4.453199863433838, -3.8613998889923096, -3.416300058364868, -4.187300205230713, -4.049499988555908, -4.22189998626709, -4.007699966430664, -4.2093000411987305, -4.212500095367432, -4.196899890899658, -3.9214000701904297, -4.0142998695373535, -4.03410005569458, -4.395999908447266, -4.462699890136719, -4.037899971008301, -4.265900135040283, -4.108799934387207, -4.202600002288818, -4.345600128173828, -4.311999797821045, -4.35230016708374, -4.321899890899658]}, \"token.table\": {\"Topic\": [1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \"Freq\": [0.34602242708206177, 0.6055392622947693, 0.1321796178817749, 0.1321796178817749, 0.6608981490135193, 0.08041644841432571, 0.8845809698104858, 0.08041644841432571, 0.08848220109939575, 0.44241100549697876, 0.44241100549697876, 0.8784232139587402, 0.22483155131340027, 0.5995507836341858, 0.14988769590854645, 0.22434887290000916, 0.1495659202337265, 0.598263680934906, 0.20515158772468567, 0.478687047958374, 0.34191930294036865, 0.16079181432724, 0.6110089421272278, 0.2251085489988327, 0.8784293532371521, 0.06805036962032318, 0.8846548199653625, 0.06805036962032318, 0.1550060212612152, 0.7750301361083984, 0.31553471088409424, 0.7099530696868896, 0.07888367772102356, 0.09279005229473114, 0.8351104259490967, 0.5882670879364014, 0.23530682921409607, 0.18824546039104462, 0.6329593062400818, 0.2712682783603668, 0.09042275696992874, 0.8328577280044556, 0.11897967755794525, 0.11897967755794525, 0.08003991842269897, 0.08003991842269897, 0.8804391026496887, 0.8354586958885193, 0.1113944947719574, 0.0556972473859787, 0.8187227845191956, 0.07442934066057205, 0.07442934066057205, 0.08116172254085541, 0.5681320428848267, 0.40580859780311584, 0.8157138228416443, 0.13595230877399445, 0.08040795475244522, 0.08040795475244522, 0.804079532623291, 0.8726151585578918, 0.07932864874601364, 0.7227869033813477, 0.09034836292266846, 0.09034836292266846, 0.06974246352910995, 0.90665203332901, 0.06974246352910995, 0.04531748965382576, 0.2265874445438385, 0.7250798344612122, 0.2293664962053299, 0.0764554962515831, 0.6880995035171509, 0.616706371307373, 0.3083531856536865, 0.10278439521789551, 0.12532980740070343, 0.626649022102356, 0.25065961480140686, 0.576361358165741, 0.38424092531204224, 0.1870226263999939, 0.1870226263999939, 0.6545791625976562, 0.15495485067367554, 0.7747742533683777, 0.04250820353627205, 0.6801312565803528, 0.2550491988658905, 0.21700985729694366, 0.1302059143781662, 0.6510295867919922, 0.6345639228820801, 0.21152131259441376, 0.15864098072052002, 0.38003653287887573, 0.12667883932590485, 0.5067153573036194, 0.845640242099762, 0.08456402271986008, 0.08456402271986008, 0.09339123219251633, 0.2801736891269684, 0.6537386178970337, 0.41445744037628174, 0.5180718302726746, 0.10361436009407043, 0.693244457244873, 0.28885185718536377, 0.057770371437072754, 0.3155196011066437, 0.4732794165611267, 0.15775980055332184, 0.06698454916477203, 0.13396909832954407, 0.7368300557136536, 0.0814911499619484, 0.8964026570320129, 0.0814911499619484, 0.09393145889043808, 0.8453831076622009, 0.09393145889043808, 0.08207881450653076, 0.7797487378120422, 0.16415762901306152, 0.7408478260040283, 0.2778179347515106, 0.5212164521217346, 0.379066526889801, 0.09476663172245026, 0.06775256991386414, 0.06775256991386414, 0.8807833790779114, 0.0720767006278038, 0.93699711561203, 0.43591806292533875, 0.08718361705541611, 0.5231016874313354, 0.15489543974399567, 0.7744771838188171, 0.12416799366474152, 0.8070919513702393, 0.06208399683237076, 0.6450696587562561, 0.049620743840932846, 0.2977244555950165, 0.18012668192386627, 0.18012668192386627, 0.6304433941841125, 0.15790219604969025, 0.23685330152511597, 0.631608784198761, 0.14107206463813782, 0.8464323878288269, 0.41758278012275696, 0.059654682874679565, 0.5368921160697937, 0.5622035264968872, 0.16062958538532257, 0.24094437062740326, 0.5827222466468811, 0.05827222019433975, 0.3496333360671997, 0.21717341244220734, 0.054293353110551834, 0.7058135867118835, 0.3615068793296814, 0.2410045862197876, 0.3615068793296814, 0.09543904662132263, 0.09543904662132263, 0.8589513897895813, 0.8106880187988281, 0.11581257730722427, 0.07399934530258179, 0.8879920840263367, 0.07399934530258179, 0.1549672782421112, 0.7748363614082336, 0.3548594117164612, 0.4968031942844391, 0.1419437676668167, 0.06441032886505127, 0.4508723020553589, 0.5152826309204102, 0.9624480605125427, 0.7402498722076416, 0.24674995243549347, 0.12337497621774673, 0.7312782406806946, 0.2089366465806961, 0.10446832329034805, 0.8784041404724121, 0.1415787935256958, 0.4955257773399353, 0.3539469838142395, 0.7771644592285156, 0.17270320653915405, 0.08635160326957703, 0.9019085168838501, 0.5991044640541077, 0.3423454165458679, 0.11938895285129547, 0.6566392779350281, 0.23877790570259094, 0.07231119275093079, 0.7954230904579163, 0.14462238550186157, 0.06607120484113693, 0.8589256405830383, 0.13214240968227386, 0.11784673482179642, 0.5892336964607239, 0.29461684823036194, 0.8568963408470154, 0.0612068846821785, 0.122413769364357, 0.7701883316040039, 0.17773577570915222, 0.05924525856971741, 0.10305903106927872, 0.8244722485542297, 0.9371076822280884, 0.0520615354180336, 0.0520615354180336, 0.7322551012039185, 0.040680840611457825, 0.24408504366874695, 0.6995642781257629, 0.12719351053237915, 0.12719351053237915, 0.23281548917293549, 0.7760516405105591, 0.07760516554117203, 0.8593197464942932, 0.11457596719264984, 0.05728798359632492, 0.11119081825017929, 0.44476327300071716, 0.44476327300071716, 0.6349648833274841, 0.23089630901813507, 0.11544815450906754, 0.07400473952293396, 0.8880568146705627, 0.07400473952293396, 0.06911279261112213, 0.2764511704444885, 0.6911279559135437, 0.5869153738021851, 0.29345768690109253, 0.14672884345054626, 0.7767741084098816, 0.1109677255153656, 0.1109677255153656, 0.24161279201507568, 0.1610751897096634, 0.5637632012367249, 0.08923518657684326, 0.8031166791915894, 0.08923518657684326, 0.08005249500274658, 0.8805774450302124, 0.08005249500274658, 0.5534061193466187, 0.07905802130699158, 0.3162320852279663], \"Term\": [\"affect\", \"affect\", \"amaz\", \"amaz\", \"amaz\", \"april\", \"april\", \"april\", \"ask\", \"ask\", \"ask\", \"basi\", \"break\", \"break\", \"break\", \"call\", \"call\", \"call\", \"care\", \"care\", \"care\", \"case\", \"case\", \"case\", \"cast\", \"caus\", \"caus\", \"caus\", \"ceas\", \"ceas\", \"china\", \"china\", \"china\", \"corona\", \"corona\", \"coronavirus\", \"coronavirus\", \"coronavirus\", \"countri\", \"countri\", \"countri\", \"cover\", \"cover\", \"cover\", \"crisi\", \"crisi\", \"crisi\", \"death\", \"death\", \"death\", \"die\", \"die\", \"die\", \"doctor\", \"doctor\", \"doctor\", \"either\", \"either\", \"equip\", \"equip\", \"equip\", \"everyon\", \"everyon\", \"face\", \"face\", \"face\", \"father\", \"father\", \"father\", \"fight\", \"fight\", \"fight\", \"first\", \"first\", \"first\", \"global\", \"global\", \"global\", \"govern\", \"govern\", \"govern\", \"governor\", \"governor\", \"great\", \"great\", \"great\", \"gullibl\", \"gullibl\", \"health\", \"health\", \"health\", \"help\", \"help\", \"help\", \"home\", \"home\", \"home\", \"hospit\", \"hospit\", \"hospit\", \"human\", \"human\", \"human\", \"india\", \"india\", \"india\", \"isol\", \"isol\", \"isol\", \"keep\", \"keep\", \"keep\", \"like\", \"like\", \"like\", \"live\", \"live\", \"live\", \"lockdown\", \"lockdown\", \"lockdown\", \"look\", \"look\", \"look\", \"make\", \"make\", \"make\", \"mani\", \"mani\", \"mask\", \"mask\", \"mask\", \"medic\", \"medic\", \"medic\", \"murder\", \"murder\", \"must\", \"must\", \"must\", \"narrat\", \"narrat\", \"natur\", \"natur\", \"natur\", \"need\", \"need\", \"need\", \"never\", \"never\", \"never\", \"nurs\", \"nurs\", \"nurs\", \"often\", \"often\", \"order\", \"order\", \"order\", \"pandem\", \"pandem\", \"pandem\", \"patient\", \"patient\", \"patient\", \"peopl\", \"peopl\", \"peopl\", \"pleas\", \"pleas\", \"pleas\", \"polic\", \"polic\", \"polic\", \"poor\", \"poor\", \"poverti\", \"poverti\", \"poverti\", \"propaganda\", \"propaganda\", \"protect\", \"protect\", \"protect\", \"public\", \"public\", \"public\", \"racism\", \"recommend\", \"recommend\", \"recommend\", \"releas\", \"releas\", \"releas\", \"religion\", \"report\", \"report\", \"report\", \"respons\", \"respons\", \"respons\", \"rich\", \"right\", \"right\", \"say\", \"say\", \"say\", \"send\", \"send\", \"send\", \"speak\", \"speak\", \"speak\", \"spread\", \"spread\", \"spread\", \"state\", \"state\", \"state\", \"stay\", \"stay\", \"stay\", \"support\", \"support\", \"take\", \"take\", \"take\", \"test\", \"test\", \"test\", \"thank\", \"thank\", \"thank\", \"think\", \"think\", \"think\", \"time\", \"time\", \"time\", \"today\", \"today\", \"today\", \"trump\", \"trump\", \"trump\", \"trut\", \"trut\", \"trut\", \"virus\", \"virus\", \"virus\", \"want\", \"want\", \"want\", \"watch\", \"watch\", \"watch\", \"week\", \"week\", \"week\", \"work\", \"work\", \"work\", \"worker\", \"worker\", \"worker\", \"world\", \"world\", \"world\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el6057351959156005476006804\", ldavis_el6057351959156005476006804_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el6057351959156005476006804\", ldavis_el6057351959156005476006804_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el6057351959156005476006804\", ldavis_el6057351959156005476006804_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.125581 -0.071593       1        1  36.171600\n",
       "1     -0.141165 -0.050241       2        1  33.468410\n",
       "0      0.015583  0.121834       3        1  30.359993, topic_info=    Category       Freq         Term      Total  loglift  logprob\n",
       "22   Default  36.000000        peopl  36.000000  30.0000  30.0000\n",
       "52   Default  19.000000         take  19.000000  29.0000  29.0000\n",
       "31   Default  14.000000        medic  14.000000  28.0000  28.0000\n",
       "70   Default  22.000000        fight  22.000000  27.0000  27.0000\n",
       "42   Default  22.000000         work  22.000000  26.0000  26.0000\n",
       "36   Default  13.000000       murder  13.000000  25.0000  25.0000\n",
       "39   Default  13.000000       racism  13.000000  24.0000  24.0000\n",
       "41   Default  13.000000         trut  13.000000  23.0000  23.0000\n",
       "38   Default  13.000000      poverti  13.000000  22.0000  22.0000\n",
       "72   Default  24.000000         make  24.000000  21.0000  21.0000\n",
       "34   Default  14.000000         caus  14.000000  20.0000  20.0000\n",
       "35   Default  14.000000       father  14.000000  19.0000  19.0000\n",
       "40   Default  15.000000        speak  15.000000  18.0000  18.0000\n",
       "37   Default  16.000000        natur  16.000000  17.0000  17.0000\n",
       "59   Default  12.000000        crisi  12.000000  16.0000  16.0000\n",
       "26   Default  12.000000        april  12.000000  15.0000  15.0000\n",
       "75   Default  12.000000       worker  12.000000  14.0000  14.0000\n",
       "3    Default  16.000000        state  16.000000  13.0000  13.0000\n",
       "88   Default  10.000000       corona  10.000000  12.0000  12.0000\n",
       "0    Default  23.000000       health  23.000000  11.0000  11.0000\n",
       "77   Default  12.000000        equip  12.000000  10.0000  10.0000\n",
       "50   Default  17.000000         time  17.000000   9.0000   9.0000\n",
       "76   Default  14.000000         live  14.000000   8.0000   8.0000\n",
       "27   Default  12.000000     lockdown  12.000000   7.0000   7.0000\n",
       "46   Default  17.000000        death  17.000000   6.0000   6.0000\n",
       "13   Default  24.000000         test  24.000000   5.0000   5.0000\n",
       "67   Default  12.000000      everyon  12.000000   4.0000   4.0000\n",
       "6    Default  23.000000         help  23.000000   3.0000   3.0000\n",
       "97   Default  10.000000        polic  10.000000   2.0000   2.0000\n",
       "33   Default   9.000000      support   9.000000   1.0000   1.0000\n",
       "52    Topic1  17.659582         take  19.208038   0.9328  -3.4328\n",
       "19    Topic1   5.933658         rich   6.652559   0.9025  -4.5234\n",
       "67    Topic1  11.212878      everyon  12.605786   0.8998  -3.8870\n",
       "18    Topic1  10.365226        human  11.825359   0.8851  -3.9656\n",
       "64    Topic1   4.977750         cast   5.691978   0.8828  -4.6991\n",
       "69    Topic1   4.973165     religion   5.692141   0.8819  -4.7000\n",
       "63    Topic1   4.956141         basi   5.692017   0.8785  -4.7034\n",
       "3     Topic1  14.094548        state  16.338032   0.8692  -3.6583\n",
       "66    Topic1   6.334322       either   7.355521   0.8674  -4.4581\n",
       "89    Topic1  11.427966          die  13.435561   0.8551  -3.8680\n",
       "50    Topic1  14.529849         time  17.455667   0.8334  -3.6278\n",
       "46    Topic1  14.672399        death  17.954208   0.8150  -3.6181\n",
       "107   Topic1   6.835887        cover   8.404797   0.8103  -4.3819\n",
       "68    Topic1   6.968006         poor   8.634641   0.8024  -4.3627\n",
       "4     Topic1  12.989102         stay  16.878988   0.7549  -3.7399\n",
       "101   Topic1   8.486095         face  11.068269   0.7512  -4.1656\n",
       "103   Topic1   6.870058        watch   9.011629   0.7456  -4.3769\n",
       "62    Topic1   8.524282      respons  11.580561   0.7105  -4.1611\n",
       "13    Topic1  17.588774         test  24.581596   0.6822  -3.4368\n",
       "9     Topic1  11.248610        thank  15.724073   0.6819  -3.8838\n",
       "108   Topic1   6.823308       releas   9.572280   0.6784  -4.3837\n",
       "7     Topic1  12.103111         keep  17.309912   0.6591  -3.8106\n",
       "98    Topic1   5.599061    recommend   8.105371   0.6470  -4.5814\n",
       "106   Topic1  11.436920        trump  17.323793   0.6017  -3.8672\n",
       "54    Topic1   7.290962      countri  11.059163   0.6003  -4.3174\n",
       "1     Topic1  12.444030         home  18.910624   0.5984  -3.7828\n",
       "105   Topic1   7.380438        right  11.684106   0.5575  -4.3052\n",
       "78    Topic1  12.547184         need  20.152863   0.5430  -3.7745\n",
       "99    Topic1   6.387870     governor  10.410135   0.5285  -4.4496\n",
       "15    Topic1   5.826513       global   9.729103   0.5042  -4.5416\n",
       "5     Topic1  25.298416  coronavirus  42.497704   0.4982  -3.0733\n",
       "16    Topic1  14.427181       pandem  24.902014   0.4711  -3.6349\n",
       "49    Topic1  10.136427      patient  17.160835   0.4904  -3.9879\n",
       "24    Topic1  11.067523         mask  21.104475   0.3714  -3.9000\n",
       "32    Topic1   7.767006         want  13.630585   0.4545  -4.2542\n",
       "56    Topic1   7.111123        world  12.648938   0.4410  -4.3424\n",
       "22    Topic1   8.252737        peopl  36.836922  -0.4791  -4.1935\n",
       "2     Topic1   6.690847        order  16.763144   0.0985  -4.4033\n",
       "39    Topic2  12.552399       racism  13.507222   1.0213  -3.6965\n",
       "36    Topic2  12.828982       murder  13.874109   1.0163  -3.6747\n",
       "41    Topic2  12.452970         trut  13.512649   1.0129  -3.7044\n",
       "38    Topic2  12.425316      poverti  13.513634   1.0106  -3.7066\n",
       "26    Topic2  11.172869        april  12.435266   0.9875  -3.8129\n",
       "75    Topic2  11.202670       worker  12.491803   0.9856  -3.8102\n",
       "35    Topic2  12.713864       father  14.338467   0.9743  -3.6837\n",
       "34    Topic2  12.976617         caus  14.694997   0.9702  -3.6632\n",
       "27    Topic2  10.653278     lockdown  12.271271   0.9532  -3.8605\n",
       "40    Topic2  12.803123        speak  15.135187   0.9272  -3.6767\n",
       "37    Topic2  13.445651        natur  16.107210   0.9140  -3.6277\n",
       "79    Topic2   8.739795         look  10.646061   0.8973  -4.0585\n",
       "42    Topic2  18.194887         work  22.412683   0.8861  -3.3252\n",
       "25    Topic2  11.056293         send  13.829118   0.8708  -3.8234\n",
       "72    Topic2  18.537151         make  24.366823   0.8211  -3.3066\n",
       "12    Topic2   9.786015        think  12.885740   0.8194  -3.9454\n",
       "95    Topic2   7.636043         mani  10.798439   0.7480  -4.1935\n",
       "11    Topic2  11.464558          say  16.751968   0.7153  -3.7871\n",
       "60    Topic2   8.582126        china  12.676894   0.7045  -4.0767\n",
       "0     Topic2  15.925622       health  23.524872   0.7044  -3.4584\n",
       "43    Topic2  10.410788       govern  15.957896   0.6675  -3.8835\n",
       "92    Topic2   8.390475        break  13.343323   0.6306  -4.0993\n",
       "53    Topic2  19.276201         case  31.096109   0.6164  -3.2675\n",
       "10    Topic2   9.892618       spread  16.971195   0.5548  -3.9346\n",
       "14    Topic2   7.140253       doctor  12.321079   0.5490  -4.2606\n",
       "45    Topic2   4.873743         isol   9.651172   0.4114  -4.6425\n",
       "96    Topic2   7.027219      protect  14.090087   0.3989  -4.2766\n",
       "29    Topic2   9.372483         like  19.016251   0.3871  -3.9886\n",
       "47    Topic2   6.554614       report  14.126410   0.3267  -4.3462\n",
       "48    Topic2   6.657852         care  14.623333   0.3077  -4.3306\n",
       "30    Topic2   7.989250        today  17.987097   0.2830  -4.1483\n",
       "24    Topic2   8.179936         mask  21.104475   0.1468  -4.1247\n",
       "17    Topic2   6.834281       public  15.525460   0.2740  -4.3044\n",
       "5     Topic2   9.659955  coronavirus  42.497704  -0.3869  -3.9584\n",
       "88    Topic3   9.436441       corona  10.777018   1.0592  -3.8843\n",
       "31    Topic3  12.923365        medic  14.759588   1.0592  -3.5699\n",
       "59    Topic3  10.685988        crisi  12.493766   1.0357  -3.7600\n",
       "97    Topic3   8.924057        polic  10.477892   1.0315  -3.9401\n",
       "33    Topic3   8.115085      support   9.703176   1.0133  -4.0352\n",
       "83    Topic3   5.299909       narrat   6.455968   0.9947  -4.4612\n",
       "82    Topic3   5.280898      gullibl   6.453493   0.9915  -4.4648\n",
       "85    Topic3   5.273727   propaganda   6.452975   0.9902  -4.4662\n",
       "81    Topic3   5.259282         ceas   6.451362   0.9877  -4.4689\n",
       "77    Topic3  10.131210        equip  12.436581   0.9870  -3.8133\n",
       "23    Topic3   5.638590        often   7.088576   0.9632  -4.3993\n",
       "76    Topic3  11.385726         live  14.928816   0.9211  -3.6965\n",
       "70    Topic3  15.907127        fight  22.066536   0.8647  -3.3621\n",
       "22    Topic3  26.450930        peopl  36.836922   0.8608  -2.8536\n",
       "20    Topic3   9.259026        first  13.079504   0.8466  -3.9033\n",
       "80    Topic3   5.342757         amaz   7.565463   0.8442  -4.4532\n",
       "73    Topic3   9.655166        virus  14.469101   0.7875  -3.8614\n",
       "6     Topic3  15.067984         help  23.040428   0.7674  -3.4163\n",
       "71    Topic3   6.970177        india  10.707644   0.7627  -4.1873\n",
       "8     Topic3   7.999842         nurs  12.666068   0.7325  -4.0495\n",
       "74    Topic3   6.732932        great  10.693893   0.7294  -4.2219\n",
       "91    Topic3   8.341067         call  13.372030   0.7201  -4.0077\n",
       "84    Topic3   6.817904        never  11.103296   0.7044  -4.2093\n",
       "57    Topic3   6.796658       affect  11.559944   0.6609  -4.2125\n",
       "86    Topic3   6.903217         week  12.416561   0.6050  -4.1969\n",
       "2     Topic3   9.092953        order  16.763144   0.5804  -3.9214\n",
       "21    Topic3   8.286644       hospit  15.787956   0.5474  -4.0143\n",
       "17    Topic3   8.123536       public  15.525460   0.5443  -4.0341\n",
       "61    Topic3   5.656876         must  11.470045   0.4852  -4.3960\n",
       "90    Topic3   5.291885          ask  11.301708   0.4333  -4.4627\n",
       "30    Topic3   8.092572        today  17.987097   0.3933  -4.0379\n",
       "78    Topic3   6.443217         need  20.152863   0.0517  -4.2659\n",
       "5     Topic3   7.539333  coronavirus  42.497704  -0.5373  -4.1088\n",
       "53    Topic3   6.864373         case  31.096109  -0.3187  -4.2026\n",
       "49    Topic3   5.949557      patient  17.160835   0.1327  -4.3456\n",
       "0     Topic3   6.152852       health  23.524872  -0.1491  -4.3120\n",
       "87    Topic3   5.909520        pleas  16.597195   0.1594  -4.3523\n",
       "16    Topic3   6.092335       pandem  24.902014  -0.2159  -4.3219, token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "57        1  0.346022       affect\n",
       "57        3  0.605539       affect\n",
       "80        1  0.132180         amaz\n",
       "80        2  0.132180         amaz\n",
       "80        3  0.660898         amaz\n",
       "26        1  0.080416        april\n",
       "26        2  0.884581        april\n",
       "26        3  0.080416        april\n",
       "90        1  0.088482          ask\n",
       "90        2  0.442411          ask\n",
       "90        3  0.442411          ask\n",
       "63        1  0.878423         basi\n",
       "92        1  0.224832        break\n",
       "92        2  0.599551        break\n",
       "92        3  0.149888        break\n",
       "91        1  0.224349         call\n",
       "91        2  0.149566         call\n",
       "91        3  0.598264         call\n",
       "48        1  0.205152         care\n",
       "48        2  0.478687         care\n",
       "48        3  0.341919         care\n",
       "53        1  0.160792         case\n",
       "53        2  0.611009         case\n",
       "53        3  0.225109         case\n",
       "64        1  0.878429         cast\n",
       "34        1  0.068050         caus\n",
       "34        2  0.884655         caus\n",
       "34        3  0.068050         caus\n",
       "81        2  0.155006         ceas\n",
       "81        3  0.775030         ceas\n",
       "60        1  0.315535        china\n",
       "60        2  0.709953        china\n",
       "60        3  0.078884        china\n",
       "88        2  0.092790       corona\n",
       "88        3  0.835110       corona\n",
       "5         1  0.588267  coronavirus\n",
       "5         2  0.235307  coronavirus\n",
       "5         3  0.188245  coronavirus\n",
       "54        1  0.632959      countri\n",
       "54        2  0.271268      countri\n",
       "54        3  0.090423      countri\n",
       "107       1  0.832858        cover\n",
       "107       2  0.118980        cover\n",
       "107       3  0.118980        cover\n",
       "59        1  0.080040        crisi\n",
       "59        2  0.080040        crisi\n",
       "59        3  0.880439        crisi\n",
       "46        1  0.835459        death\n",
       "46        2  0.111394        death\n",
       "46        3  0.055697        death\n",
       "89        1  0.818723          die\n",
       "89        2  0.074429          die\n",
       "89        3  0.074429          die\n",
       "14        1  0.081162       doctor\n",
       "14        2  0.568132       doctor\n",
       "14        3  0.405809       doctor\n",
       "66        1  0.815714       either\n",
       "66        3  0.135952       either\n",
       "77        1  0.080408        equip\n",
       "77        2  0.080408        equip\n",
       "77        3  0.804080        equip\n",
       "67        1  0.872615      everyon\n",
       "67        3  0.079329      everyon\n",
       "101       1  0.722787         face\n",
       "101       2  0.090348         face\n",
       "101       3  0.090348         face\n",
       "35        1  0.069742       father\n",
       "35        2  0.906652       father\n",
       "35        3  0.069742       father\n",
       "70        1  0.045317        fight\n",
       "70        2  0.226587        fight\n",
       "70        3  0.725080        fight\n",
       "20        1  0.229366        first\n",
       "20        2  0.076455        first\n",
       "20        3  0.688100        first\n",
       "15        1  0.616706       global\n",
       "15        2  0.308353       global\n",
       "15        3  0.102784       global\n",
       "43        1  0.125330       govern\n",
       "43        2  0.626649       govern\n",
       "43        3  0.250660       govern\n",
       "99        1  0.576361     governor\n",
       "99        3  0.384241     governor\n",
       "74        1  0.187023        great\n",
       "74        2  0.187023        great\n",
       "74        3  0.654579        great\n",
       "82        2  0.154955      gullibl\n",
       "82        3  0.774774      gullibl\n",
       "0         1  0.042508       health\n",
       "0         2  0.680131       health\n",
       "0         3  0.255049       health\n",
       "6         1  0.217010         help\n",
       "6         2  0.130206         help\n",
       "6         3  0.651030         help\n",
       "1         1  0.634564         home\n",
       "1         2  0.211521         home\n",
       "1         3  0.158641         home\n",
       "21        1  0.380037       hospit\n",
       "21        2  0.126679       hospit\n",
       "21        3  0.506715       hospit\n",
       "18        1  0.845640        human\n",
       "18        2  0.084564        human\n",
       "18        3  0.084564        human\n",
       "71        1  0.093391        india\n",
       "71        2  0.280174        india\n",
       "71        3  0.653739        india\n",
       "45        1  0.414457         isol\n",
       "45        2  0.518072         isol\n",
       "45        3  0.103614         isol\n",
       "7         1  0.693244         keep\n",
       "7         2  0.288852         keep\n",
       "7         3  0.057770         keep\n",
       "29        1  0.315520         like\n",
       "29        2  0.473279         like\n",
       "29        3  0.157760         like\n",
       "76        1  0.066985         live\n",
       "76        2  0.133969         live\n",
       "76        3  0.736830         live\n",
       "27        1  0.081491     lockdown\n",
       "27        2  0.896403     lockdown\n",
       "27        3  0.081491     lockdown\n",
       "79        1  0.093931         look\n",
       "79        2  0.845383         look\n",
       "79        3  0.093931         look\n",
       "72        1  0.082079         make\n",
       "72        2  0.779749         make\n",
       "72        3  0.164158         make\n",
       "95        2  0.740848         mani\n",
       "95        3  0.277818         mani\n",
       "24        1  0.521216         mask\n",
       "24        2  0.379067         mask\n",
       "24        3  0.094767         mask\n",
       "31        1  0.067753        medic\n",
       "31        2  0.067753        medic\n",
       "31        3  0.880783        medic\n",
       "36        1  0.072077       murder\n",
       "36        2  0.936997       murder\n",
       "61        1  0.435918         must\n",
       "61        2  0.087184         must\n",
       "61        3  0.523102         must\n",
       "83        2  0.154895       narrat\n",
       "83        3  0.774477       narrat\n",
       "37        1  0.124168        natur\n",
       "37        2  0.807092        natur\n",
       "37        3  0.062084        natur\n",
       "78        1  0.645070         need\n",
       "78        2  0.049621         need\n",
       "78        3  0.297724         need\n",
       "84        1  0.180127        never\n",
       "84        2  0.180127        never\n",
       "84        3  0.630443        never\n",
       "8         1  0.157902         nurs\n",
       "8         2  0.236853         nurs\n",
       "8         3  0.631609         nurs\n",
       "23        2  0.141072        often\n",
       "23        3  0.846432        often\n",
       "2         1  0.417583        order\n",
       "2         2  0.059655        order\n",
       "2         3  0.536892        order\n",
       "16        1  0.562204       pandem\n",
       "16        2  0.160630       pandem\n",
       "16        3  0.240944       pandem\n",
       "49        1  0.582722      patient\n",
       "49        2  0.058272      patient\n",
       "49        3  0.349633      patient\n",
       "22        1  0.217173        peopl\n",
       "22        2  0.054293        peopl\n",
       "22        3  0.705814        peopl\n",
       "87        1  0.361507        pleas\n",
       "87        2  0.241005        pleas\n",
       "87        3  0.361507        pleas\n",
       "97        1  0.095439        polic\n",
       "97        2  0.095439        polic\n",
       "97        3  0.858951        polic\n",
       "68        1  0.810688         poor\n",
       "68        2  0.115813         poor\n",
       "38        1  0.073999      poverti\n",
       "38        2  0.887992      poverti\n",
       "38        3  0.073999      poverti\n",
       "85        2  0.154967   propaganda\n",
       "85        3  0.774836   propaganda\n",
       "96        1  0.354859      protect\n",
       "96        2  0.496803      protect\n",
       "96        3  0.141944      protect\n",
       "17        1  0.064410       public\n",
       "17        2  0.450872       public\n",
       "17        3  0.515283       public\n",
       "39        2  0.962448       racism\n",
       "98        1  0.740250    recommend\n",
       "98        2  0.246750    recommend\n",
       "98        3  0.123375    recommend\n",
       "108       1  0.731278       releas\n",
       "108       2  0.208937       releas\n",
       "108       3  0.104468       releas\n",
       "69        1  0.878404     religion\n",
       "47        1  0.141579       report\n",
       "47        2  0.495526       report\n",
       "47        3  0.353947       report\n",
       "62        1  0.777164      respons\n",
       "62        2  0.172703      respons\n",
       "62        3  0.086352      respons\n",
       "19        1  0.901909         rich\n",
       "105       1  0.599104        right\n",
       "105       3  0.342345        right\n",
       "11        1  0.119389          say\n",
       "11        2  0.656639          say\n",
       "11        3  0.238778          say\n",
       "25        1  0.072311         send\n",
       "25        2  0.795423         send\n",
       "25        3  0.144622         send\n",
       "40        1  0.066071        speak\n",
       "40        2  0.858926        speak\n",
       "40        3  0.132142        speak\n",
       "10        1  0.117847       spread\n",
       "10        2  0.589234       spread\n",
       "10        3  0.294617       spread\n",
       "3         1  0.856896        state\n",
       "3         2  0.061207        state\n",
       "3         3  0.122414        state\n",
       "4         1  0.770188         stay\n",
       "4         2  0.177736         stay\n",
       "4         3  0.059245         stay\n",
       "33        2  0.103059      support\n",
       "33        3  0.824472      support\n",
       "52        1  0.937108         take\n",
       "52        2  0.052062         take\n",
       "52        3  0.052062         take\n",
       "13        1  0.732255         test\n",
       "13        2  0.040681         test\n",
       "13        3  0.244085         test\n",
       "9         1  0.699564        thank\n",
       "9         2  0.127194        thank\n",
       "9         3  0.127194        thank\n",
       "12        1  0.232815        think\n",
       "12        2  0.776052        think\n",
       "12        3  0.077605        think\n",
       "50        1  0.859320         time\n",
       "50        2  0.114576         time\n",
       "50        3  0.057288         time\n",
       "30        1  0.111191        today\n",
       "30        2  0.444763        today\n",
       "30        3  0.444763        today\n",
       "106       1  0.634965        trump\n",
       "106       2  0.230896        trump\n",
       "106       3  0.115448        trump\n",
       "41        1  0.074005         trut\n",
       "41        2  0.888057         trut\n",
       "41        3  0.074005         trut\n",
       "73        1  0.069113        virus\n",
       "73        2  0.276451        virus\n",
       "73        3  0.691128        virus\n",
       "32        1  0.586915         want\n",
       "32        2  0.293458         want\n",
       "32        3  0.146729         want\n",
       "103       1  0.776774        watch\n",
       "103       2  0.110968        watch\n",
       "103       3  0.110968        watch\n",
       "86        1  0.241613         week\n",
       "86        2  0.161075         week\n",
       "86        3  0.563763         week\n",
       "42        1  0.089235         work\n",
       "42        2  0.803117         work\n",
       "42        3  0.089235         work\n",
       "75        1  0.080052       worker\n",
       "75        2  0.880577       worker\n",
       "75        3  0.080052       worker\n",
       "56        1  0.553406        world\n",
       "56        2  0.079058        world\n",
       "56        3  0.316232        world, R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_tfidf, tfidf_corpus, dictionary=lda_model_tfidf.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Model evaluation\n",
    "Topic cluster is a statistical tool that imposes probability distribution over words. We can use different coherence measures to quantify the quality of the clusters and the probability distribution of the words. However, before we dwelve into the coherence measures, lets try to approach the evaluation with a more intuitive way i.e. human validation. \n",
    "\n",
    "## 5.1. Evaluation by human validation\n",
    "In this section, we'll cluster a sample tweet and see how well does this clustering matches the overall topics generated above.\n",
    "\n",
    "### 5.1.1: Human validation for Bag of words model\n",
    "Classify a sample tweet into the topics and then evaluate if the general topic matches with the tweet better than other topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our test tweet is: 0: ['health', 'home', 'order', 'state', 'stay']\n",
      "\n",
      "Score: 0.5977810025215149\t \n",
      "Topic: 2 : 0.044*\"health\" + 0.034*\"case\" + 0.034*\"keep\" + 0.033*\"pandem\" + 0.026*\"time\" + 0.025*\"death\" + 0.024*\"care\" + 0.023*\"human\" + 0.023*\"like\" + 0.021*\"trump\"\n",
      "\n",
      "Score: 0.34373247623443604\t \n",
      "Topic: 0 : 0.043*\"work\" + 0.038*\"natur\" + 0.035*\"caus\" + 0.034*\"father\" + 0.034*\"murder\" + 0.033*\"speak\" + 0.033*\"trut\" + 0.033*\"poverti\" + 0.033*\"racism\" + 0.030*\"mask\"\n",
      "\n",
      "Score: 0.05848647654056549\t \n",
      "Topic: 1 : 0.070*\"coronavirus\" + 0.065*\"peopl\" + 0.050*\"test\" + 0.041*\"help\" + 0.029*\"affect\" + 0.025*\"everyon\" + 0.021*\"hospit\" + 0.020*\"fight\" + 0.020*\"poor\" + 0.019*\"either\"\n"
     ]
    }
   ],
   "source": [
    "# Our test tweet is \n",
    "print('Our test tweet is: {}: {}'.format(tweet_num, [dictionary[word[0]] for word in bow_corpus[tweet_num]]))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_corpus[tweet_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {} : {}\".format(score, index, lda_model.print_topic(index, 10))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample tweet is classified to Topic 0 with 88% score. Topic 0 in Bag of word model was centered around self quarantining. The sample tweet matches with this topic. We could try evaluating more tweets manually. Seems like the BOW based LDA model worked well.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Human validation for TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our test tweet is: 0: ['health', 'home', 'order', 'state', 'stay']\n",
      "\n",
      "Score: 0.6827797293663025\t \n",
      "Topic: 2 : 0.044*\"health\" + 0.034*\"case\" + 0.034*\"keep\" + 0.033*\"pandem\" + 0.026*\"time\" + 0.025*\"death\" + 0.024*\"care\" + 0.023*\"human\" + 0.023*\"like\" + 0.021*\"trump\"\n",
      "\n",
      "Score: 0.1886352002620697\t \n",
      "Topic: 1 : 0.070*\"coronavirus\" + 0.065*\"peopl\" + 0.050*\"test\" + 0.041*\"help\" + 0.029*\"affect\" + 0.025*\"everyon\" + 0.021*\"hospit\" + 0.020*\"fight\" + 0.020*\"poor\" + 0.019*\"either\"\n",
      "\n",
      "Score: 0.128585085272789\t \n",
      "Topic: 0 : 0.043*\"work\" + 0.038*\"natur\" + 0.035*\"caus\" + 0.034*\"father\" + 0.034*\"murder\" + 0.033*\"speak\" + 0.033*\"trut\" + 0.033*\"poverti\" + 0.033*\"racism\" + 0.030*\"mask\"\n"
     ]
    }
   ],
   "source": [
    "# Our test tweet is \n",
    "print('Our test tweet is: {}: {}'.format(tweet_num, [dictionary[word[0]] for word in tfidf_corpus[tweet_num]]))\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[tfidf_corpus[tweet_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {} : {}\".format(score, index, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the sample tweet is split between topics 0 with 49% score and topic 1 with 38% score. As we saw in the section 4.2.1, Topic 0 was centered around President Trump's announcement around COVID 19 and Topic 1 around Quarantine and fight the spread of COVID-19. \n",
    "\n",
    "The sample tweet matches well with Topic 1. However, the confidence of this clustering is low compared to 88% confidence we saw for Bag of Words model. \n",
    "\n",
    "The TF-IDF modeling didn't have good confidence in the classification. This was expected as TF-IDF doesn't work good for short text documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Coherence measures\n",
    "The topic coherence measures scores a single topic by computing the semantic similarity between the top words in that topic. We can then average the scores of each topic to get the overall coherence measure for the model.\n",
    "\n",
    "There are different coherence measures. In the below evaluation we'll be using the C_v measure. C_v measure is based on a sliding window, a one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosinus similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score: 0.4304417242985787\n",
      "Per Topic: [0.4493676045869341, 0.33174836101253, 0.510209207296272]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score for BOW based LDA model\n",
    "cv_model_bow = CoherenceModel(model=lda_model, texts=processed_tweets, dictionary=dictionary, coherence='c_v')\n",
    "cv_bow_overall = cv_model_bow.get_coherence()\n",
    "cv_bow_pertopic = cv_model_bow.get_coherence_per_topic()\n",
    "print('\\nCoherence Score: {0}\\nPer Topic: {1}'.format(cv_bow_overall , cv_bow_pertopic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score: 0.5277570239510211\n",
      "Per Topic: [0.5811252711244042, 0.46330507155149236, 0.5388407291771666]\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score for TF-IDF based LDA model\n",
    "cv_model_tfidf = CoherenceModel(model=lda_model_tfidf, texts=processed_tweets, dictionary=lda_model_tfidf.id2word, coherence='c_v')\n",
    "cv_tfidf_overall = cv_model_tfidf.get_coherence()\n",
    "cv_tfidf_pertopic = cv_model_tfidf.get_coherence_per_topic()\n",
    "print('\\nCoherence Score: {0}\\nPer Topic: {1}'.format(cv_tfidf_overall, cv_tfidf_pertopic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the coherence measures, it seems that the tf-idf model seemed to have gathered the topics based on better semantic similarity between the words. This was contradictory to what we saw in the previous section on human validation of the topics. However, the previous measure was done on a few sample tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Hyper parameter tuning \n",
    "Now that we have established the modeling and performance evaluation methods, let's try to tune the Number of Topics hyper parameter. We'll define a range for the number of topics. Then we'll train a model for each value for the hyper parameter. We'll also compute the coherence score and jot it down. At the end we'll plot the scores to see what's the best for the Number of Topics hyper parameter.\n",
    "\n",
    "We'll repeat the same for both BOW and TF-IDF models and compare the scores to choose the best model.\n",
    "\n",
    "The LDA model has more hyper-parameters namely alpha and beta values. We could tune the alpha and beta hyperparameters also in the similar fashion. \n",
    "\n",
    "For parameter tuning, sklearn exposes a GridSearchCV api that can be configured easily with ranges for different hyper parameters. For this, we can use the LDA_Transform model exposed by gensim. However, that approach uses the default log_likelihood score for tuning the hyper parameters. Since we decided to use coherence scores, we'll tune the hyper parameters in a loop instead of using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(corpus, dictionary, num_topics):\n",
    "    lda_model = gensim.models.LdaMulticore(corpus, \n",
    "                                       num_topics=num_topics, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=2)\n",
    "        \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_tweets, dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    return lda_model, coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 1/18 [00:01<00:25,  1.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 2/18 [00:02<00:23,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 3/18 [00:04<00:20,  1.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 4/18 [00:05<00:18,  1.33s/it]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 5/18 [00:06<00:16,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 6/18 [00:07<00:15,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 7/18 [00:09<00:14,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 8/18 [00:10<00:12,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 9/18 [00:11<00:11,  1.30s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 10/18 [00:13<00:10,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 11/18 [00:14<00:09,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 12/18 [00:15<00:07,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 13/18 [00:16<00:06,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 14/18 [00:18<00:05,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 15/18 [00:19<00:03,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 16/18 [00:20<00:02,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 17/18 [00:21<00:01,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 18/18 [00:23<00:00,  1.29s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Models\n",
    "model_sets = [bow_corpus, tfidf_corpus]\n",
    "model_title = ['BOW', 'TF-IDF']\n",
    "\n",
    "model_results = pd.DataFrame(columns=['Model','Num_Topics','Coherence'])\n",
    "\n",
    "pbar = tqdm.tqdm(total=len(model_sets)*len(topics_range))\n",
    "    \n",
    "if 1 == 1:\n",
    "  # iterate through validation corpuses\n",
    "  for i in range(len(corpus_sets)):\n",
    "    # iterate through number of topics\n",
    "    for k in topics_range:\n",
    "        # get the coherence score for the given parameters\n",
    "        model, cv = compute_coherence_values(corpus=model_sets[i], dictionary=dictionary, num_topics=k)\n",
    "        \n",
    "        # Save the model results\n",
    "        results = {'Model':model_title[i]\n",
    "                   ,'Num_Topics':k\n",
    "                   ,'Coherence':cv}\n",
    "        \n",
    "        model_results = model_results.append(results , ignore_index=True)\n",
    "        pbar.update(1)\n",
    "\n",
    "  model_results.to_csv('lda_tuning_results.csv', index=False)\n",
    "  pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverlabel": {
          "namelength": 0
         },
         "hovertemplate": "Model=BOW<br>Num_Topics=%{x}<br>Coherence=%{y}",
         "legendgroup": "BOW",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "BOW",
         "showlegend": true,
         "type": "scatter",
         "x": [
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
         ],
         "xaxis": "x",
         "y": [
          0.4957159423978963,
          0.42670833564931554,
          0.3921510251260488,
          0.4306887266479943,
          0.43642704895820233,
          0.41861840686266333,
          0.44824196131865546,
          0.4542230914642606,
          0.43144075308547175
         ],
         "yaxis": "y"
        },
        {
         "hoverlabel": {
          "namelength": 0
         },
         "hovertemplate": "Model=TF-IDF<br>Num_Topics=%{x}<br>Coherence=%{y}",
         "legendgroup": "TF-IDF",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "TF-IDF",
         "showlegend": true,
         "type": "scatter",
         "x": [
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
         ],
         "xaxis": "x",
         "y": [
          0.5367665234387993,
          0.5014556292406364,
          0.49640089984296154,
          0.49338161491613386,
          0.46607438957522246,
          0.49011888696452505,
          0.4844073731044565,
          0.4774603085716225,
          0.49019754259853504
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "Model"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Num_Topics"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Coherence"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"85934080-5220-4517-8fd6-ffc97cf0561a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"85934080-5220-4517-8fd6-ffc97cf0561a\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '85934080-5220-4517-8fd6-ffc97cf0561a',\n",
       "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Model=BOW<br>Num_Topics=%{x}<br>Coherence=%{y}\", \"legendgroup\": \"BOW\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"BOW\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [2, 3, 4, 5, 6, 7, 8, 9, 10], \"xaxis\": \"x\", \"y\": [0.4957159423978963, 0.42670833564931554, 0.3921510251260488, 0.4306887266479943, 0.43642704895820233, 0.41861840686266333, 0.44824196131865546, 0.4542230914642606, 0.43144075308547175], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Model=TF-IDF<br>Num_Topics=%{x}<br>Coherence=%{y}\", \"legendgroup\": \"TF-IDF\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"TF-IDF\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [2, 3, 4, 5, 6, 7, 8, 9, 10], \"xaxis\": \"x\", \"y\": [0.5367665234387993, 0.5014556292406364, 0.49640089984296154, 0.49338161491613386, 0.46607438957522246, 0.49011888696452505, 0.4844073731044565, 0.4774603085716225, 0.49019754259853504], \"yaxis\": \"y\"}],\n",
       "                        {\"legend\": {\"title\": {\"text\": \"Model\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Num_Topics\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Coherence\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('85934080-5220-4517-8fd6-ffc97cf0561a');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = px.line(model_results, x='Num_Topics', y='Coherence',color='Model')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above models, seems like both BOW and TF-IDF models with 2 topics performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "EarlyIndicatorsFromNews",
  "notebookId": 3907296281459545
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
